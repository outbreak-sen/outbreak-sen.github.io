<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>This is a 部落格 of outbreak_sen</title>
  
  <subtitle>[object Object]</subtitle>
  <link href="http://outbreak-sen.github.io/atom.xml" rel="self"/>
  
  <link href="http://outbreak-sen.github.io/"/>
  <updated>2025-12-03T06:11:20.417Z</updated>
  <id>http://outbreak-sen.github.io/</id>
  
  <author>
    <name>outbreak_sen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读_MoGe 2</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MoGe%202/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MoGe%202/</id>
    <published>2025-12-03T06:04:18.000Z</published>
    <updated>2025-12-03T06:11:20.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left">Ruicheng Wang1∗ Sicheng Xu2 Yue Dong2 Yu Deng2 Jianfeng Xiang3∗ Zelong Lv1∗ Guangzhong Sun1 Xin Tong2 Jiaolong Yang</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">USTC  Microsoft Research  Tsinghua University</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2025</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>将MoGe扩展为公制几何预测，同时不损害仿射不变点表示提供的相对几何精度。</td></tr><tr><td><strong>所属领域</strong></td><td>视觉 Transformer</td></tr></tbody></table><h2 id="背景：">背景：</h2><ol><li>单目深度估计（MDE）和单目几何估计（MGE）的最新进展得益于在大规模数据集上训练的基础模型。相较于深度估计，MGE方法通常还预测相机内参，从而将像素提升到三维空间，因此支持更广泛的应用。但没有实现公制尺度重建</li><li>对于公制几何估计，一种直接的解决方案是直接预测公制空间中的绝对点图。然而，由于焦距-距离模糊问题，这种方法并非最优。为此，我们探索了两种简单、直观但有效的替代方案。第一种采用平移不变点图表示，直接将公制尺度集成到点图预测中。第二种保留仿射不变表示，但以解耦方式额外预测全局尺度因子。两种策略均缓解了焦距-距离模糊问题，但后者产生了更精确的结果，这可能归因于其规范化点图空间更好地保留了相对几何。</li><li>在后一方面，我们提出了一种实用的数据精炼方法，为真实世界训练数据生成锐利深度标签。真实数据标签通常存在噪声和不完整，尤其在物体边界处，这阻碍了精细几何细节的学习。此前工作如Depth Anything V2选择仅使用合成数据标签，尽管在二维可视化时显得锐利，但牺牲了几何精度。类似地，Depth Pro在其两阶段中的第二阶段也仅使用合成数据。相比之下，我们在整个训练过程中采用真实数据，以确保高几何精度——这是我们方法的关键目标。我们的流程首先过滤真实数据中不匹配或错误的深度值（主要出现在物体边界），然后通过边缘保持深度补全，利用在合成数据上训练的模型填充缺失区域。该方法显著提升了细节精细度，同时几何精度与使用未处理完整真实数据训练的模型相当。</li></ol><h3 id="创新点">创新点</h3><ol><li>提出一种新的MGE方法，我们的方法建立在近期MoGe方法基础上，该方法从单张图像预测仿射不变点图，并达到最先进的几何精度。MoGe的核心是其优化的训练方案，包括鲁棒且最优的点云对齐求解器以及增强局部几何精度的多尺度监督方法。我们的工作通过引入公制几何预测能力并提升几何粒度以捕捉复杂细节，对MoGe进行了扩展。<img src="%E9%99%84%E4%BB%B6/Pasted%20image%2020250907115341.png" alt=""><img src="%E9%99%84%E4%BB%B6/Pasted%20image%2020250907115728.png" alt=""></li><li>此外发现真实数据中的噪声和误差会降低预测几何的细节粒度。为此开发了一种统一的数据精炼方法，利用清晰的合成标签对不同来源的真实数据进行过滤和补全(数据集)</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_变分自编码器VAE</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_%E5%8F%98%E5%88%86%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8VAE/</id>
    <published>2025-12-03T06:01:57.000Z</published>
    <updated>2025-12-03T06:11:20.415Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Auto-Encoding Variational Bayes</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left">Diederik P. Kingma 和 Max Welling</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left">2013</td></tr><tr><td style="text-align:left"><strong>论文链接</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>别名</strong></td><td style="text-align:left">Variational Autoencoder，变分自编码器</td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td>输入</td><td>单张 RGB 图像</td></tr><tr><td>输出</td><td>分类、分割</td></tr><tr><td>所属领域</td><td>视觉 Transformer</td></tr></tbody></table><h3 id="背景">背景</h3><p>传统自编码器 (Autoencoder) 的局限，比如PCA：</p><ul><li>传统AE由编码器（Encoder）和解码器（Decoder）组成。</li><li>编码器将输入数据 <code>x</code> 压缩成一个低维的潜在向量 <code>z</code>。</li><li>解码器尝试从 <code>z</code> 重构回原始数据 <code>x</code>。</li><li>PCA：x本身是一个矩阵，通过一个变换W变成了一个低维矩阵c，因为这一过程是线性的，所以再通过一个WT变换就能还原出一个x ^ \widehat{x} x ,现在我们要找到一种变换W，使得矩阵x与x ^ \widehat{x} 能够尽可能地一致，在PCA中找这个变换W用到的方法是SVD（奇异值分解）算法</li><li>学习到的潜在空间 <code>z</code> 可能是不连续、不规则的。例如，两个在潜在空间中很接近的点 <code>z1</code> 和 <code>z2</code>，解码后可能产生完全无关的输出。这使得<strong>直接从潜在空间采样生成新数据变得困难且不可控</strong>。</li></ul><h3 id="创新点">创新点</h3><ol><li><ul><li>VAE 不再让编码器输出一个确定的点 <code>z</code>，而是输出一个<strong>概率分布</strong>（通常是高斯分布）的参数（均值 <code>μ</code> 和方差 <code>σ²</code>）。它强制这个潜在空间 <code>z</code> 遵循一个<strong>简单的先验分布</strong>（通常是标准正态分布 <code>N(0, I)</code>）。通过优化一个下界（ELBO），VAE 学习一个既能良好重构数据，又能使潜在空间平滑、连续的模型。这样，你就可以从标准正态分布中采样 <code>z</code>，然后用解码器生成新的、合理的数据。</li></ul></li></ol><h3 id="怎么使用vae">怎么使用VAE</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VAE</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim=<span class="number">784</span>, hidden_dim=<span class="number">400</span>, latent_dim=<span class="number">20</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VAE, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.input_dim = input_dim</span><br><span class="line">        <span class="variable language_">self</span>.hidden_dim = hidden_dim</span><br><span class="line">        <span class="variable language_">self</span>.latent_dim = latent_dim</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 编码器 (Encoder)</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_dim, hidden_dim)</span><br><span class="line">        <span class="variable language_">self</span>.fc21 = nn.Linear(hidden_dim, latent_dim)  <span class="comment"># 均值 μ</span></span><br><span class="line">        <span class="variable language_">self</span>.fc22 = nn.Linear(hidden_dim, latent_dim)  <span class="comment"># 对数方差 log(σ²)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 解码器 (Decoder)</span></span><br><span class="line">        <span class="variable language_">self</span>.fc3 = nn.Linear(latent_dim, hidden_dim)</span><br><span class="line">        <span class="variable language_">self</span>.fc4 = nn.Linear(hidden_dim, input_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, x</span>):</span><br><span class="line">        h1 = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fc21(h1), <span class="variable language_">self</span>.fc22(h1)  <span class="comment"># μ, log(σ²)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span><br><span class="line">        std = torch.exp(<span class="number">0.5</span> * logvar)</span><br><span class="line">        eps = torch.randn_like(std)  <span class="comment"># ε ~ N(0, I)</span></span><br><span class="line">        <span class="keyword">return</span> mu + eps * std  <span class="comment"># z = μ + ε * σ</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, z</span>):</span><br><span class="line">        h3 = F.relu(<span class="variable language_">self</span>.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> torch.sigmoid(<span class="variable language_">self</span>.fc4(h3))  <span class="comment"># 假设输入是[0,1]归一化的图像</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        mu, logvar = <span class="variable language_">self</span>.encode(x.view(-<span class="number">1</span>, <span class="variable language_">self</span>.input_dim))  <span class="comment"># 展平图像</span></span><br><span class="line">        z = <span class="variable language_">self</span>.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decode(z), mu, logvar</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型</span></span><br><span class="line">model = VAE(input_dim=<span class="number">784</span>, hidden_dim=<span class="number">400</span>, latent_dim=<span class="number">20</span>)  <span class="comment"># 例如，用于28x28的MNIST图像</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># VAE的损失函数由两部分组成：</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vae_loss</span>(<span class="params">recon_x, x, mu, logvar</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    recon_x: 重构后的数据</span></span><br><span class="line"><span class="string">    x: 原始输入数据</span></span><br><span class="line"><span class="string">    mu, logvar: 编码器输出的均值和对数方差</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1. 重构损失: 使用二值交叉熵 (适用于[0,1]归一化的图像)</span></span><br><span class="line">    <span class="comment"># 注意: sum(-log(p(x|z)))，这里用BCELoss</span></span><br><span class="line">    BCE = F.binary_cross_entropy(recon_x, x.view(-<span class="number">1</span>, <span class="number">784</span>), reduction=<span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. KL散度损失: D_KL(q(z|x) || p(z)) = -0.5 * sum(1 + log(σ²) - μ² - σ²)</span></span><br><span class="line">    <span class="comment"># 其中 σ² = exp(logvar)</span></span><br><span class="line">    KLD = -<span class="number">0.5</span> * torch.<span class="built_in">sum</span>(<span class="number">1</span> + logvar - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - logvar.exp())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> BCE + KLD  <span class="comment"># 最小化这个总损失</span></span><br></pre></td></tr></table></figure><h1>自编码器历史</h1><ol><li>Autoencoder</li><li>Denoising Autoencoder：<strong>在输入数据中引入噪声</strong>，然后训练网络恢复原始未受扰动的数据。这个过程迫使网络学习更为鲁棒的数据表示，忽略随机噪声，从而提高模型对输入数据中噪声或缺失值的容忍度。</li><li>Sparse Autoencoder：在隐藏层上应用“稀疏”约束以防止过拟合并增强模型的鲁棒性。该模型通过限制隐藏层中同时激活的神经元数量，<strong>强制使大部分神经元大多数时间处于非激活状态。</strong></li><li>K-Sparse Autoencoder：由Makhzani和Frey于2013年提出</li><li>Contractive Autoencoder：收缩自编码器在损失函数中加入额外的项来鼓励模型在被压缩的空间中学习更稳健的表示。</li></ol><p>之后向VAE（Variational Autoencoder）转变：不是将输入直接映射到一个固定的向量，而是将输入映射到一个概率分布上。<br>6. Conditional VAE：通过引入额外的条件变量来控制生成过程。<br>7. Beta-VAE：2017年提出。其核心目标是发现解耦或分解的潜在因子。<br>8. VQ-VAE：由van den Oord等人于2017年提出，潜在空间的离散化，这种离散化使得模型能够有效地处理和生成高度结构化的数据。<br>9. VQ-VAE-2：由Ali Razavi等人于2019年提出，是VQ-VAE的升级版。它引入了一个层次化的结构<br>10. 时间序列数据的TD-VAE</p><h2 id="vae">VAE</h2><p>这里有一个非常好的解释为什么要做变分的自编码器：<a href="https://blog.csdn.net/a312863063/article/details/87953517">【VAE学习笔记】全面通透地理解VAE(Variational Auto Encoder)_vae架构-CSDN博客</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>Multi View Stereo</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/Multi%20View%20Stereo/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/Multi%20View%20Stereo/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<h1>Multi View Stereo</h1><p>[TOC]</p><h2 id="传统方法">传统方法</h2><p>SSD、NCC、SAD+三角测量重建</p><h2 id="开源工具">开源工具</h2><p>四种经典的三维重建技术Pipeline。分别为：<strong>1）传统方法（COLMAP）2）深度学习方法（COLMAP + MVSNet）3）传统方法（COLMAP + OpenMVS）4）深度学习方法（COLMAP + R-MVSNet）</strong></p><h2 id="论文部分">论文部分</h2><p><a href="https://zhuanlan.zhihu.com/p/148569782">有监督深度学习三维重建</a></p><p><a href="https://zhuanlan.zhihu.com/p/439210991">无监督深度学习三维重建</a></p><p>综述：Deep Learning for Multi-view Stereo via Plane Sweep: A Survey</p><p>MVSNet(2018年ECCV),RMVSNet(CVPR2019),PointMVSNet(ICCV2019),</p><p>P-MVSNet(ICCV2019),MVSCRF(ICCV2019),Cascade(CVPR2020),CVP-MVSNet(CVPR2020),</p><p>Fast-MVSNet(CVPR2020),UCSNet(CVPR2020),CIDER(AAAI2020),PVAMVSNet(ECCV2020),</p><p>*D*2HC-RMVSNet(ECCV2020),Vis-MVSNet(BMVC 2020),AA-RMVSNet(ICCV2021),</p><p>EPP-MVSNet(ICCV2021)</p><h2 id="mvsnet">MVSNet</h2><p>香港科技大学权龙yoyo和深圳Altizure公司的研究团队</p><p>被ECCV2018选为Oral</p><ol><li><p>通过<strong>可微单应矩阵（Differentiable Homography</strong>）将相机几何嵌入到网络，以帮助实现<strong>端到端</strong>的深度神经网络。</p></li><li><p>设计了<strong>基于方差的多视匹配代价计算准则</strong>，将任意数量的像素特征向量映射为一个匹配代价向量，以帮助网络适用于不定数量的图像输入</p></li><li><p>输入一张r<strong>eference image（为主）</strong> 和几张<strong>source images（辅助）</strong>；</p></li><li><p>分别用网络提取出下采样四分之一的32通道的特征图；</p></li><li><p>采用立体匹配（即双目深度估计）里提出的cost volume的概念，将几张source images的特征利用<strong>单应性变换( homography warping)<strong>转换到reference image，在转换的过程中，<strong>类似极线搜索</strong>，引入了深度信息。构建</strong>cost volume</strong>可以说是<strong>MVSNet的关键</strong></p></li><li><p>可微单应性变换（<a href="https://zhuanlan.zhihu.com/p/363830541%EF%BC%89">https://zhuanlan.zhihu.com/p/363830541）</a></p></li><li><p><strong>具体costvolume上一个点是所有图片在这个点和深度值上特征的方差，方差越小，说明在该深度上置信度越高。</strong></p></li><li><p>利用3D卷积操作cost volume，先输出每个深度的概率，然后求深度的加权平均得到预测的深度信息，用L1或smoothL1回归深度信息，是一个回归模型。</p></li><li><p>利用多张图片之间的重建约束（<em>photometric</em> and <em>geometric</em> consistencies）来选择预测正确的深度信息，重建成三维点云。</p></li></ol><p>过于耗费内存而难以应用到大尺度场景</p><p>Xiaoyang Guo 同学把原来MVSNet的tensorflow代码改成了pytorch，效果提升了很多</p><h2 id="r-mvsnet-recurrent循环-mvsnet">R-MVSNet（Recurrent循环 MVSNet）</h2><p>香港科技大学权龙团队yoyo和深圳Altizure公司的研究团队</p><p>被CVPR2019接收</p><ol><li><p>引入循环神经网络架构，可依序地在深度方向通过GRU单元正则化2D的代价图，较大程度地缓解了内存消耗</p></li><li><p>输入的多视影像首先经由2D的特征提取层提取特征，经由可微的单应矩阵变换到参考影像的相机视锥体的正面平行面上，然后在不同深度计算代价，并经由卷积的GRU单元进行正则化，使在深度方向获取几何和单向的语义信息成为可能。该网络将问题视为分类问题，以交叉熵作为损失函数。</p></li><li><p>3D卷积换成GRU的时序网络来降低模型大小，然后loss也改成了多分类的交叉熵损失，其他都一样，还是在四分之一的图上预测深度。模型变小了，但是其实精度也小有降低。</p></li><li><p>和MVSNet代码合到一起了</p></li></ol><h2 id="pointmvsnet">PointMVSNet</h2><p>ICCV2019</p><ol><li>测出深度depth信息然后和图片构成三维点云，再用3D点云的算法去优化depth的回归。</li></ol><h2 id="p-mvsnet">P-MVSNet</h2><p>ICCV2019</p><p>采用传统三维重建算法中Patch-wise</p><h2 id="mvscrf-learning-multi-view-stereo-with-conditional-random-fields">MVSCRF(Learning Multi-view Stereo with Conditional Random Fields )</h2><p>ICCV2019</p><p>接入了一个CRF模块</p><h2 id="cascade-mvsnet">cascade MVSNet</h2><p>CVPR2020</p><p>阿里，GitHub链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/alibaba/cascade-stereo">https://github.com/alibaba/cascade-stereo</a></p><p>把MVSNet的模型改成了层级的，先预测下采样四分之一的深度，然后用来缩小下采样二分之一的深度，再用其缩小原始图片大小的深度，这样层级的方式，可以采用大的深度间隔和少的深度区间，从而可以一次训练更多数据。先预测出深度信息然后用来缩小更大的图片的深度</p><p>cascade MVSNet也把cost volume用在了双目立体匹配中</p><h2 id="cvp-mvsnet">CVP-MVSNet</h2><p>CVPR2020</p><p>澳大利亚国立和英伟达，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/JiayuYANG/CVP-MVSNet">https://github.com/JiayuYANG/CVP-MVSNet</a></p><p>先预测出深度信息然后用来缩小更大的图片的深度，CVP-MVSNet相比cascade MVSNet也缩小了cost volume的范围。</p><h2 id="fast-mvsnet">Fast-MVSNet</h2><p>上海科技大学，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/svip-lab/FastMVSNet">https://github.com/svip-lab/FastMVSNet</a></p><p>采用稀疏的cost volume以及Gauss-Newton layer，目的是提高MVSNet<strong>的速度</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Multi View Stereo&lt;/h1&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;传统方法&quot;&gt;传统方法&lt;/h2&gt;
&lt;p&gt;SSD、NCC、SAD+三角测量重建&lt;/p&gt;
&lt;h2 id=&quot;开源工具&quot;&gt;开源工具&lt;/h2&gt;
&lt;p&gt;四种经典的三维重建技术Pipeline。分别为</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记_SFM</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SFM/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SFM/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.418Z</updated>
    
    <content type="html"><![CDATA[<h1>SFM</h1><h2 id="sfm是什么？和mvs的对比？">SFM是什么？和MVS的对比？</h2><p>运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题</p><ul><li><p>SFM相当于获得由二维点获得三维点输出稀疏点云还有每张图片对应的相机参数</p></li><li><p>MVS相当于给三维点贴图输出稠密点云</p></li></ul><p>3D点阵可以通过MeshLab来重建稀疏的Mesh。也可以通过PMVS（Patch-based Multi-view Stereo）来重建Dense的Mesh</p><h2 id="sfm的阶段">SFM的阶段</h2><p>SfM 涉及三个主要阶段：</p><ol><li>提取图像中的特征（例如，兴趣点、线条等）并在图像之间匹配这些特征<ol><li>SIFT，SURF来提取并匹配，也可以用最新的AKAZE（SIFT的改进版，2010）来匹配。</li><li>误匹配会造成较大的Error，所以要对匹配进行筛选，目前流行的方法是RANSAC（Random Sample Consensus）。2D的误匹配点可以应用3D的Geometric特征来进行排除。</li></ol></li><li>相机运动估计–外参（使用提取的特征）</li><li>使用估计的外参特征恢复 3D 结构（通过最小化所谓的重投影误差）</li></ol><p>该技术旨在优化称为<strong>total reprojection error的成本函数</strong></p><h2 id="sfm分类">SfM分类</h2><ul><li><p>增量式（incremental/sequential）：</p><ul><li>优–对错误的匹配点有较强鲁棒性，总体精度更高</li><li>劣运行时间长，<strong>drift: error随着camera registration(相机校准)逐步积累</strong></li></ul></li><li><p>全局式（global）：</p><ul><li>优–避免了drift的问题（更反映了图像的全局性), 速度快（只需解决两个global synchronization(global SfM算法中的全局同步操作)+一次BA<br>(光束平差法)）</li><li>劣–对错误的匹配点鲁棒性较差，且错误难以修正 (error会沿着pipeline累积）</li></ul></li><li><p>混合式（hybrid）：</p><ul><li>全局估计摄像机旋转矩阵，增量估计摄像机中心</li></ul></li><li><p>层次式（hierarchical）：</p><ul><li>执行顺序上采用了层次式的聚类策略（clustering）。其先生成一棵聚类二叉树（binary cluster tree），然后算法自底向上进行处理:<strong>算法的每次迭代合并具有最小距离的两个clusters，每个cluster可以是一张图片，也可以是一个合并之后的cluster</strong>。</li></ul></li><li><p>基于语义的(Semantic)</p></li></ul><h2 id="2006-年-sfm-的-sequential-pipeline增量式">2006 年 SfM 的 sequential  pipeline增量式</h2><ol><li>测每个图像中的关键点。使用SIFT 描述符来比较跨图像的这些关键点</li><li>应用随机采样和一致性（RANSAC），以稳健地估计<strong>图像对之间的基本矩阵</strong>（用于相机相对运动）并丢弃异常匹配</li><li>从找到<strong>内部匹配数量最多的一对图像开始</strong>，然后一次贪婪地添加一个图像，反复求解束调整</li></ol><h2 id="cvpr-2016structure-from-motion-revisited提出colmap">CVPR 2016Structure-from-Motion Revisited提出colmap</h2><ul><li><a href="https://ieeexplore.ieee.org/document/7780814">https://ieeexplore.ieee.org/document/7780814</a></li><li>(论文“Structure-from-Motion Revisited” 对ISFM改进的理解)[<a href="https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235%5Ev38%5Epc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235^v38^pc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4</a>]</li><li>在实验部分对比的两个incremetal SfM框架：<a href="http://phototour.cs.washington.edu/Photo_Tourism.pdf">Bundler</a>、<a href="https://ieeexplore.ieee.org/document/6599068">VisualSFM</a>；两个global SfM 框架：<a href="https://ieeexplore.ieee.org/document/5995626">DISCO</a>、<a href="http://theia-sfm.org/">Theia</a>提出了一个当时“近乎理想”的incremental SfM并开源了代码[COLMAP]</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;SFM&lt;/h1&gt;
&lt;h2 id=&quot;sfm是什么？和mvs的对比？&quot;&gt;SFM是什么？和MVS的对比？&lt;/h2&gt;
&lt;p&gt;运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SFM相当于获得由二维点获得三</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记_位置编码</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.419Z</updated>
    
    <content type="html"><![CDATA[<p><strong>给输入序列注入“位置信息”</strong>，让模型知道“每个元素在什么位置”。<br>位置编码的两大类型</p><table><thead><tr><th>类型</th><th>特点</th><th>代表</th></tr></thead><tbody><tr><td>**固定位置编码 **（Fixed）</td><td>位置编码是预定义的，不可学习</td><td>原始 Transformer 的正弦编码</td></tr><tr><td>**可学习位置编码 **（Learned）</td><td>位置编码是可训练的参数，<strong>就是字典啦，tokenizer把文本变成数字编码之后做的</strong></td><td>BERT、ViT 的 <code>position embedding</code></td></tr></tbody></table><h2 id="正弦位置编码-sinusoidal-positional-encoding">正弦位置编码（Sinusoidal Positional Encoding）</h2><p>这是 <strong>原始 Transformer 论文</strong>（“Attention is All You Need”, 2017）中提出的方法。<br>核心思想：使用<strong>正弦和余弦函数</strong>生成位置编码。编码是<strong>确定性的、固定的</strong>，不参与训练。可以表示<strong>任意长度的位置</strong>，外推性好</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SinusoidalPositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, max_len, d_model)</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (B, seq_len, d_model)</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>编码公式</strong></p><p><img src="%E9%99%84%E4%BB%B6/Pasted%20image%2020250829234857.png" alt=""></p><h2 id="可学习位置编码-learned-positional-embedding">可学习位置编码（Learned Positional Embedding）</h2><p>这是 <strong>BERT、ViT、GPT 等模型</strong> 采用的方法。位置编码是<strong>可学习的参数矩阵</strong>。每个位置对应一个向量（类似词嵌入）。在大多数任务上优于正弦编码，超出训练长度时性能急剧下降</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LearnedPositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.pos_embedding = nn.Embedding(max_len, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (B, seq_len, d_model)</span></span><br><span class="line">        positions = torch.arange(x.size(<span class="number">1</span>), device=x.device).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pos_embedding(positions)</span><br></pre></td></tr></table></figure><h3 id="旋转位置编码-rotary-position-embedding-rope">旋转位置编码 **（Rotary Position Embedding, RoPE）</h3><p>将位置信息编码为<strong>旋转矩阵</strong>，通过旋转向量来体现位置差异。代表：<strong>LLaMA、ChatGLM、PaLM</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;给输入序列注入“位置信息”&lt;/strong&gt;，让模型知道“每个元素在什么位置”。&lt;br&gt;
位置编码的两大类型&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;th&gt;代表&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记_文生图串讲</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%96%87%E7%94%9F%E5%9B%BE%E4%B8%B2%E8%AE%B2/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%96%87%E7%94%9F%E5%9B%BE%E4%B8%B2%E8%AE%B2/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<p>目前我的理解有两个阵营</p><ol><li>Stable Diffusion=DiffusionUNet+VAE+CLIP</li><li>Dalle2=CLIP+像素级别DiffusionUNet</li><li>其他还有其他的文生图的网络，也用了diffusion，<ol><li>Imagen=T5+DiffusionTransformer不开源</li><li>Midjourney不开源</li></ol></li></ol><table><thead><tr><th>特性</th><th>DALL·E 2</th><th>Stable Diffusion</th></tr></thead><tbody><tr><td><strong>开发者</strong></td><td>OpenAI</td><td>Stability AI, CompVis, LMU</td></tr><tr><td><strong>开源状态</strong></td><td>❌ <strong>闭源</strong>，通过 API 或 Web 界面使用</td><td>✅ <strong>完全开源</strong>，可本地部署</td></tr><tr><td><strong>架构</strong></td><td>基于 CLIP + 级联扩散模型，也是一个diffusion的</td><td>基于潜在空间的扩散模型 (Latent Diffusion)</td></tr><tr><td><strong>访问方式</strong></td><td>订阅制（按生成次数付费）</td><td>免费开源，可自行运行</td></tr><tr><td>社区与定制</td><td>有限，无法微调模型</td><td>拥有庞大的社区，可训练 LoRA、Dreambooth 模型等</td></tr><tr><td>成风格</td><td>通常更“安全”、更偏向摄影/插画</td><td>风格极其多样，取决于微调模型</td></tr><tr><td><strong>生去噪空间</strong></td><td>扩散过程主要在<strong>像素空间</strong>或<strong>CLIP 的嵌入空间</strong>进行，而不是像 SD 那样在独立的 VAE 潜在空间。</td><td>核心创新——“<strong>潜在扩散</strong>”（Latent Diffusion），极大提升了效率。</td></tr></tbody></table><h2 id="从diffusion到stablediffusion">从Diffusion到StableDiffusion</h2><p>扩散模型（Diffusion Model）是一种<strong>生成式深度学习模型</strong><br>它的工作原理分为两个主要阶段：</p><ol><li>前向扩散过程 (Forward Diffusion Process)<ul><li>从一张真实的图像开始。</li><li>在训练过程中，模型会<strong>逐步、有规律地向图像中添加高斯噪声</strong>。</li><li>经过很多步（比如 1000 步）后，原始图像被完全破坏，变成了一幅纯噪声图像。</li><li>这个过程是固定的、可预测的。</li></ul></li><li>反向去噪过程 (Reverse Denoising Process)<ul><li>这是生成新图像的关键。</li><li>模型的目标是学习如何<strong>从纯噪声开始，一步步地去除噪声，最终还原出一张清晰的图像</strong>。</li><li>在训练时，模型学习预测每一步被添加的噪声是什么。</li><li>在生成时（推理阶段），模型从一个随机噪声开始，利用学到的知识，一步一步地“去噪”，最终生成一张新的、逼真的图像。<br>Stable Diffusion 是由 <strong>Stability AI</strong> 等机构在 2022 年发布的一个<strong>具体的、开源的文本生成图像模型</strong>。核心创新：引入了 <strong>VAE（变分自编码器）在潜在空间（Latent Space）中进行扩散</strong><br>它的工作原理分为三个主要阶段：</li></ul></li><li><strong>编码器</strong>：先将原始图像压缩到一个低维的<strong>潜在空间</strong>（例如 64x64x4）。</li><li><strong>扩散过程</strong>：扩散模型（U-Net）在这个<strong>低维的潜在空间</strong>中进行去噪和生成。这大大降低了计算复杂度，提升了速度。</li><li><strong>解码器</strong>：生成完成后，再用 VAE 的<strong>解码器</strong>将低维的潜在表示“解压”回高维的像素图像。</li></ol><p>Stable Diffusion (通常指 <strong>v1.x</strong> 系列) 和 <strong>Stable Diffusion v2</strong> 是由 Stability AI 发布的同一模型系列的两个主要版本，Stable Diffusion v2的数据集更好，另外提供 <strong>upscaling diffusion model</strong>，可将低分辨率图像提高分辨率，还能利用 MiDaS 模型估计深度。</p><ol><li>Stable Diffusion v1.x 系列：使用 OpenAI 的 CLIP。Stable Diffusion v2.0：切换为 OpenCLIP</li></ol><h2 id="vae-variational-autoencoder-变分自编码器-在stablediffusion中的作用">VAE (Variational Autoencoder - 变分自编码器)在StableDiffusion中的作用</h2><p>VAE 在文生图流程中主要负责<strong>图像的编码与解码</strong>，其作用是连接像素空间和潜在空间（latent space）。</p><ul><li><strong>编码器 (Encoder) 的作用：</strong><ul><li>在训练阶段，VAE 的编码器将真实图像从高维的<strong>像素空间</strong>压缩到一个低维的、结构化的<strong>潜在空间</strong>（latent space）。</li><li>这个潜在空间的向量（通常称为 latent code 或 latent representation）包含了原始图像的关键视觉信息，但维度远低于原始像素，从而大大降低了后续生成模型（如扩散模型）的计算复杂度。</li><li>例如，在 Stable Diffusion 中，一张 512x512x3 的图像被编码成一个 64x64x4 的潜在向量。</li></ul></li><li><strong>解码器 (Decoder) 的作用：</strong><ul><li>在生成阶段，生成模型（如扩散模型）在潜在空间中逐步“去噪”或构建出一个代表目标图像的潜在向量。</li><li>VAE 的解码器则负责将这个最终的潜在向量<strong>解码回像素空间</strong>，生成我们最终看到的、可视化的图像。</li></ul></li></ul><h2 id="clip-contrastive-language-image-pre-training-对比语言-图像预训练模型-在stablediffusion中的作用">CLIP (Contrastive Language–Image Pre-training - 对比语言-图像预训练模型)在StableDiffusion中的作用</h2><p>CLIP 的核心作用是<strong>理解文本语义并将其与视觉概念对齐</strong>，充当文本和图像之间的“翻译器”或“桥梁”。</p><ul><li><strong>文本编码器 (Text Encoder)：</strong><ul><li>将输入的文本描述（如“一只戴着墨镜的猫在太空漫步”）转换成一个高维的<strong>文本嵌入向量</strong>（text embedding）。这个向量捕捉了文本的语义信息。</li></ul></li><li><strong>图像编码器 (Image Encoder)：</strong><ul><li>将图像（或图像的潜在表示）也转换成一个<strong>图像嵌入向量</strong>。</li></ul></li><li><strong>对齐机制：</strong><ul><li>CLIP 是在海量的“图像-文本对”数据上进行对比学习训练的。它的目标是让匹配的图像和文本的嵌入向量在向量空间中尽可能接近，而不匹配的则尽可能远离。</li><li>这种训练方式使得 CLIP 能够深刻理解不同概念之间的关联，例如知道“猫”这个词对应的视觉特征是什么。</li></ul></li><li><strong>在文生图中的应用：</strong><ul><li><strong>文本条件输入：</strong> 在生成过程中，CLIP 的文本编码器将用户的文本提示（prompt）编码成一个文本嵌入向量。这个向量作为<strong>条件信息</strong>，指导生成模型（如扩散模型中的 U-Net）去生成符合该描述的图像。</li><li><strong>引导生成：</strong> 生成模型利用这个文本嵌入向量来调整其在潜在空间中的生成过程，确保生成的潜在表示与文本描述在语义上保持一致。</li><li><strong>评估与排序（可选）：</strong> 有时 CLIP 也可以用来评估生成图像与文本描述的匹配程度。</li></ul></li></ul><h2 id="dall-e-2">DALL·E 2</h2><ul><li><strong>输入</strong>：一段文字描述，例如：“一只戴着贝雷帽、在月球上用萨克斯演奏爵士乐的柴犬，赛博朋克风格，8K 分辨率”。</li><li><strong>输出</strong>：生成一张或多张与该描述高度匹配的、高质量的图像。<br>工作原理简述有两个阶段</li></ul><ol><li>文本理解：首先，DALL·E 2 利用了 OpenAI 自家的 <strong>CLIP 模型</strong>。将输入的文本描述编码成一个高维的语义向量。</li><li>图像生成：一个级联的扩散模型开始工作：<ul><li><strong>第一阶段（低分辨率生成）</strong>：一个扩散模型从纯噪声开始，根据文本语义向量的指导，逐步去噪，生成一张低分辨率（如 256x256）的图像。</li><li><strong>第二阶段（超分辨率）</strong>：另一组扩散模型（超分辨率模型）将低分辨率图像作为输入，逐步添加细节，提升到更高的分辨率（如 1024x1024），使图像更清晰、细节更丰富。</li><li>还具备强大的编辑功能：局部编辑（Inpainting），图像到图像（Image-to-Image），外绘（Outpainting）</li></ul></li></ol><h2 id="为什么用unet做diffusion？">为什么用UNet做diffusion？</h2><p>既要理解全局语义，又要保留精确的空间位置信息。Unet的跳跃连接刚好能做到<br><strong>核心去噪网络都是基于 U-Net 或其变体</strong>（如 DiT）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;目前我的理解有两个阵营&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stable Diffusion=DiffusionUNet+VAE+CLIP&lt;/li&gt;
&lt;li&gt;Dalle2=CLIP+像素级别DiffusionUNet&lt;/li&gt;
&lt;li&gt;其他还有其他的文生图的网络，也用了diffusi</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记_深度学习入门几个常见的概念</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%87%A0%E4%B8%AA%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%87%A0%E4%B8%AA%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E5%BF%B5/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<h1>无监督和有监督学习</h1><h2 id="监督学习-supervised-learning">监督学习（supervised learning）</h2><p>擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个<em>样本</em>（example）。</p><p>回归问题：（regression）平方误差损失函数。</p><p>分类问题：（classification）<em>分类</em>问题希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class）），当有两个以上的类别时，我们把这个问题称为<em>多项分类</em>（multiclass classification）问题。 常见的例子包括手写字符识别 。 与解决回归问题不同，分类问题的常见损失函数被称为<em>交叉熵</em>（cross-entropy）</p><p>标记问题：学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification），一个样本点有多个标签，标记问题输出样本点上的所有标签，分类只有一个标签</p><p>搜索问题：搜索一个标签，输出多个结果，需要对输出多个结果进行相关度排序。</p><p>序列学习问题：输入是连续的，模型可能就需要拥有“记忆”功能</p><h3 id="无监督学习">无监督学习</h3><p>聚类（clustering）问题：没有标签的情况下，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片</p><p>主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。</p><p>因果关系（causality）和<em>概率图模型</em>（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</p><p>生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。<br>1.聚类是最常见的无监督学习方法之一。聚类的方法包括将未标记的数据组织成类似的组。此处的主要目标是发现数据点中的相似性，并将相似的数据点分组到一个聚类中。</p><p>2.异常检测是识别与大多数数据显著不同的特殊项、事件或观测值的方法。通常在数据中寻找异常或异常值的原因在于它们是可疑的。异常检测常用于银行欺诈和医疗差错检测。</p><h3 id="强化学习">强化学习</h3><p><em>离线学习</em>：（offline learning）预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的。但是有些策略与环境互动，而不是离线的，比如强化学习</p><p>机器学习开发与环境交互并采取行动感兴趣，那么最终可能会专注于<em>强化学习</em>（reinforcement learning）</p><p>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得<em>奖励</em>（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。强化学习的目标是产生一个好的<em>策略</em>（policy）。</p><h2 id="前馈神经网络-全连接神经网络-feedforward-neural-network-fnn-：">前馈神经网络/全连接神经网络(Feedforward Neural Network,FNN)：</h2><p>线性函数和激活函数叠好几个层最简单</p><h2 id="循环神经网络-recurrent-neural-network">循环神经网络(Recurrent Neural Network):</h2><p><a href="https://blog.csdn.net/bestrivern/article/details/90723524">RNN详解</a></p><p><a href="https://zhuanlan.zhihu.com/p/123211148">史上最详细循环神经网络讲解（RNN/LSTM/GRU）</a></p><p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p><p>层与层之间是全连接的，每层之间的节点也是连接的，每一步的参数共享</p><p>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息</p><h2 id="长短期记忆递归神经网络-long-short-term-memory-lstm">长短期记忆递归神经网络（Long-Short Term Memory，LSTM):</h2><p>LSTM是RNN的一种变体</p><p>LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；</p><p>引入了很多内容，导致参数变多，也使得训练难度加大了很多。</p><p>因此往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p><p>三个门：</p><ol><li>Input Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory Cell。</li><li>Output Gate：中文是输出门，每一时刻是否有信息从Memory Cell输出取决于这一道门。</li><li>Forget Gate：中文是遗忘门，每一时刻Memory Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory Cell里的值清除，也就是遗忘掉。</li></ol><h2 id="门控循环单元-gate-recurrent-unit-gru-2014提出循环神经网络的一种">门控循环单元（Gate Recurrent Unit，GRU）:2014提出循环神经网络的一种</h2><p>GRU模型中有两个门，重置门和更新门</p><p><strong>重置门决定了如何将新的输入信息与前面的记忆相结合</strong></p><p>更新门用于控制前一时刻的状态信息被带入到当前状态中的程度</p><h2 id="递归神经网络-recursive-neural-network-rnn">递归神经网络 (Recursive Neural Network, RNN)</h2><p>尽管<strong>递归神经网络</strong>具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，<strong>递归神经网络</strong>的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用<strong>循环神经网络</strong>处理句子，那么我们可以直接把句子作为输入。然而，如果我们用<strong>递归神经网络</strong>处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于<strong>递归神经网络</strong>能够带来的性能提升，这个投入是不太划算的。</p><h2 id="深度神经网络dnn：">深度神经网络DNN：</h2><p>有很多隐藏层的神经网络</p><h2 id="卷积神经网络cnn">卷积神经网络CNN</h2><h2 id="生成对抗网络gan">生成对抗网络GAN</h2><h2 id="自然语言处理nlp">自然语言处理NLP</h2><p>分类问题的常见损失函数：交叉熵</p><p>回归问题的常见损失函数：均方差</p><p>激活函数：<a href="https://zhuanlan.zhihu.com/p/364620596">https://zhuanlan.zhihu.com/p/364620596</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;无监督和有监督学习&lt;/h1&gt;
&lt;h2 id=&quot;监督学习-supervised-learning&quot;&gt;监督学习（supervised learning）&lt;/h2&gt;
&lt;p&gt;擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个&lt;em&gt;样本&lt;/em&gt;（examp</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>实践日记_AlphaPose微调</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_AlphaPose%E5%BE%AE%E8%B0%83/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_AlphaPose%E5%BE%AE%E8%B0%83/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.417Z</updated>
    
    <content type="html"><![CDATA[<h1>AlphaPose_finetune和自定义关键点</h1><p>我有以下几个目标</p><ul><li>我创建了一个只有运动员的数据集，我需要在Alphapose上做微调训练或者完全重新训练</li><li>我的运动员穿着服装非常单一，我需要修改yolo检测部分让他能够只识别运动员</li><li>我的运动员数据集相对于coco数据集17个关键点，多了手背脚尖腰部5个关键点，现在我需要针对这个骨架重新编写一个data并进行训练，使得我的Alphapose能够输出22个坐标点</li></ul><h2 id="数据集准备">数据集准备</h2><h3 id="mscoco">MSCOCO</h3><p>我这里准备的是coco数据集，这里的coco数据集需要注意</p><ul><li>一定要用数字做图像名称</li><li>要添加url</li><li>train和val目录不能出现有相同名称的图像</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">|-- json</span><br><span class="line">|-- exp</span><br><span class="line">|-- alphapose</span><br><span class="line">|-- configs</span><br><span class="line">|-- test</span><br><span class="line">|-- data</span><br><span class="line">`-- |-- coco</span><br><span class="line">    `-- |-- annotations</span><br><span class="line">        |   |-- person_keypoints_train2017.json</span><br><span class="line">        |   `-- person_keypoints_val2017.json</span><br><span class="line">        |-- train2017</span><br><span class="line">        |   |-- 000000000009.jpg</span><br><span class="line">        |   |-- 000000000025.jpg</span><br><span class="line">        |   |-- 000000000030.jpg</span><br><span class="line">        |   |-- ... </span><br><span class="line">        `-- val2017</span><br><span class="line">            |-- 000000000139.jpg</span><br><span class="line">            |-- 000000000285.jpg</span><br><span class="line">            |-- 000000000632.jpg</span><br><span class="line">            |-- ... </span><br></pre></td></tr></table></figure><h2 id="微调训练train-py"><a href="http://xn--train-bm3jx01pwuvdwa.py">微调训练train.py</a></h2><p>这个train.py发现是自带微调的，但是是全权重微调，没有只微调一部分的。需要额外训练YOLO检测，这里分为yolov3和yoloX，但是yolov3的这个代码来自一个不知名地方，所以后期还是得改成用yolox，这里有一个如何训练yolox的链接。</p><ul><li><p>注意一点，其中检测头的yolo权重不会被微调，但是在微调过程中是使用数据集中的bbox中的图像进行关键点检测的，所以你可以在训练完alphapose之后之后再训练yolo，但是等到你需要部署的时候肯定得训练yolo让你的alphapose能先找到目标任务然后进行特征点提取</p></li><li><p>注意第二点是，模型下载到.cache/torch/hub/checkpoint中的backbone，比如resnet50、resnet152也同样在微调过程中不会被训练，但是这部分也没必要训练。hrnet的版本不需要下载权重，比较喜欢，因为有些项目不能上网。</p></li></ul><p>下面讲解如何快速微调：</p><p>其实就是修改config文件，必须修改configs/coco/resnet/a256x192_res152_lr1e-3_1x-duc.yaml</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"># 重点修改这里使的dataset的位置正确</span><br><span class="line">DATASET:</span><br><span class="line">  TRAIN:</span><br><span class="line">    TYPE: &#x27;Mscoco&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;train2017&#x27;</span><br><span class="line">    ANN: &#x27;annotations/person_keypoints_train2017.json&#x27;</span><br><span class="line">    AUG:</span><br><span class="line">      FLIP: true</span><br><span class="line">      ROT_FACTOR: 40</span><br><span class="line">      SCALE_FACTOR: 0.3</span><br><span class="line">      NUM_JOINTS_HALF_BODY: 8</span><br><span class="line">      PROB_HALF_BODY: -1</span><br><span class="line">  VAL:</span><br><span class="line">    TYPE: &#x27;Mscoco&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;val2017&#x27;</span><br><span class="line">    ANN: &#x27;annotations/person_keypoints_val2017.json&#x27;</span><br><span class="line">  TEST:</span><br><span class="line">    TYPE: &#x27;Mscoco_det&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;val2017&#x27;</span><br><span class="line">    DET_FILE: &#x27;./exp/json/test_det_yolo.json&#x27;</span><br><span class="line">    ANN: &#x27;annotations/person_keypoints_val2017.json&#x27;</span><br><span class="line">-------------------省略号---------------</span><br><span class="line">MODEL:</span><br><span class="line">  TYPE: &#x27;FastPose_DUC&#x27;</span><br><span class="line">  BACKBONE: &#x27;se-resnet&#x27;</span><br><span class="line"># 最重点的是修改这里的预训练权重</span><br><span class="line"># 如果设置了PRETRAINED预训练权重，那么会在预训练权重上微调，这个是支持的，最好这么做</span><br><span class="line"># 如果没有设置PRETRAINED，那么就会从零训练一个网络，但是如果你的数据集很单一，就是一个一种服装的运动员，那么你的训练结果会非常的过拟合，换个角度就难以识别了</span><br><span class="line">  PRETRAINED: &#x27;pretrained_models/fast_421_res152_256x192.pth&#x27;</span><br><span class="line"># 如果你想要训练一个和coco2017关键点数目不一样的网络，但是又想预训练，那么可以在这里TRY_LOAD加载一个权重，代码会根据权重名称和大小，把大小和名称都对应的部分加载了，但是不对应的比如最后的输出层会不加载，刚好适用于微调一个新的模型  </span><br><span class="line">  TRY_LOAD: &#x27;&#x27;</span><br><span class="line">  NUM_DECONV_FILTERS:</span><br><span class="line">  - 256</span><br><span class="line">  - 256</span><br><span class="line">  - 256</span><br><span class="line">  NUM_LAYERS: 152</span><br><span class="line">  FINAL_CONV_KERNEL: 1</span><br><span class="line">  STAGE1:</span><br><span class="line">    NUM_CONV: 4</span><br><span class="line">  STAGE2:</span><br><span class="line">    NUM_CONV: 2</span><br><span class="line">  STAGE3:</span><br><span class="line">    NUM_CONV: 1</span><br><span class="line">LOSS:</span><br><span class="line">  TYPE: &#x27;MSELoss&#x27;</span><br><span class="line">#这里是默认设置为yolov3</span><br><span class="line">  NAME: &#x27;yolo&#x27;</span><br><span class="line">  CONFIG: &#x27;detector/yolo/cfg/yolov3-spp.cfg&#x27;</span><br><span class="line">  WEIGHTS: &#x27;detector/yolo/data/yolov3-spp.weights&#x27;</span><br><span class="line">  NMS_THRES: 0.6</span><br><span class="line">  CONFIDENCE: 0.05</span><br><span class="line">TRAIN:</span><br><span class="line">  WORLD_SIZE: 4</span><br><span class="line">  BATCH_SIZE: 32</span><br><span class="line">  BEGIN_EPOCH: 0</span><br><span class="line">  # 修改这里</span><br><span class="line">  END_EPOCH: 200</span><br><span class="line">  OPTIMIZER: &#x27;adam&#x27;</span><br><span class="line">  LR: 0.001</span><br><span class="line">  LR_FACTOR: 0.1</span><br><span class="line">  LR_STEP:</span><br><span class="line">  - 90</span><br><span class="line">  - 120</span><br><span class="line">  DPG_MILESTONE: 140</span><br><span class="line">  DPG_STEP:</span><br><span class="line">  - 160</span><br><span class="line">  - 190</span><br></pre></td></tr></table></figure><p>最后开始训练，代码很简单</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 展示训练res50</span><br><span class="line">CUDA_VISIBLE_DEVICES=1 \ #这里我会设置以下用哪些卡进行训练</span><br><span class="line">./scripts/train.sh \</span><br><span class="line">./configs/coco/resnet/256x192_res50_lr1e-3_1x.yaml \</span><br><span class="line">250420Finetune #给这个训练过程起一个名字，然后接可以在exp/250420Finetune找到对应的model</span><br><span class="line"></span><br><span class="line"># 展示训练hrnet的结构，一般来说hrnet会非常准，但是实验测试res152更准，毕竟网络变深了</span><br><span class="line">CUDA_VISIBLE_DEVICES=1 \</span><br><span class="line">./scripts/train.sh \</span><br><span class="line">./configs/coco/hrnet/b256x192_w32_lr1e-3.yaml \</span><br><span class="line">0516Hrnetcoco1Pretrined</span><br></pre></td></tr></table></figure><h2 id="修改yolo检测部分让他能够只识别某一个人">修改yolo检测部分让他能够只识别某一个人</h2><p><strong>首先需要微调训一个只识别一种类别“person”的yolo网络，这里训练的版本为yolox_s，因为他小，训练的快，但是yolo_l性能不错</strong></p><p>，然后在detector/yolox/exps/default/yolox_s.py，修改对应网络架构为只识别一个类别</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Exp(MyExp):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Exp, self).__init__()</span><br><span class="line">        self.depth = 0.33</span><br><span class="line">        self.width = 0.50</span><br><span class="line">        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(&quot;.&quot;)[0]</span><br><span class="line">        # 后添加的</span><br><span class="line">        self.num_classes = 1</span><br></pre></td></tr></table></figure><p>然后训练demo就可以了</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">UDA_VISIBLE_DEVICES=4 \</span><br><span class="line">python demo_inference.py \ </span><br><span class="line">--cfg configs/coco/resnet/a256x192_res152_lr1e-3_1x-duc.yaml  \</span><br><span class="line">--detector yolox-s  --checkpoint exp/0516res152coco1FullPretrained-a256x192_res152_lr1e-3_1x-duc.yaml/model_199.pth \</span><br><span class="line">--video example2024012900025/202401290002_Miqus_5_28519.avi \</span><br><span class="line">--outdir example2024012900025/0517FulltrainedRes152coco1Pretrainedyoloxstrain0517num80epoth20 \</span><br><span class="line">--pose_track \</span><br><span class="line">--save_video \</span><br><span class="line">--vis_fast</span><br></pre></td></tr></table></figure><h1>自定义关节点个数，微调训练一个新网络</h1><ol><li>首先按照alphapose/datasets/coco_det.py和alphapose/datasets/mscoco.py创建了具有22个点的alphapose/datasets/coco22_det.py和alphapose/datasets/coco22.py</li><li>然后注意在alphapose/datasets/<strong>init</strong>.py注册这个新建立的数据集格式coco22</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from .coco_det import Mscoco_det</span><br><span class="line">from .concat_dataset import ConcatDataset</span><br><span class="line">from .custom import CustomDataset</span><br><span class="line">from .mscoco import Mscoco</span><br><span class="line">from .mpii import Mpii</span><br><span class="line">from .coco_wholebody import coco_wholebody</span><br><span class="line">from .coco_wholebody_det import coco_wholebody_det</span><br><span class="line">from .halpe_26 import Halpe_26</span><br><span class="line">from .halpe_136 import Halpe_136</span><br><span class="line">from .halpe_136_det import  Halpe_136_det</span><br><span class="line">from .halpe_26_det import  Halpe_26_det</span><br><span class="line">from .halpe_coco_wholebody_26 import Halpe_coco_wholebody_26</span><br><span class="line">from .halpe_coco_wholebody_26_det import Halpe_coco_wholebody_26_det</span><br><span class="line">from .halpe_coco_wholebody_136 import Halpe_coco_wholebody_136</span><br><span class="line">from .halpe_coco_wholebody_136_det import Halpe_coco_wholebody_136_det</span><br><span class="line">from .halpe_68_noface import Halpe_68_noface</span><br><span class="line">from .halpe_68_noface_det import Halpe_68_noface_det</span><br><span class="line">from .single_hand import SingleHand</span><br><span class="line">from .single_hand_det import SingleHand_det</span><br><span class="line">from .coco22 import Coco22</span><br><span class="line">from .coco22_det import Coco22_det</span><br><span class="line">__all__ = [&#x27;CustomDataset&#x27;, &#x27;ConcatDataset&#x27;, &#x27;Mpii&#x27;, &#x27;Mscoco&#x27;, &#x27;Mscoco_det&#x27;, \</span><br><span class="line">           &#x27;Halpe_26&#x27;, &#x27;Halpe_26_det&#x27;, &#x27;Halpe_136&#x27;, &#x27;Halpe_136_det&#x27;, \</span><br><span class="line">           &#x27;Halpe_coco_wholebody_26&#x27;, &#x27;Halpe_coco_wholebody_26_det&#x27;, \</span><br><span class="line">           &#x27;Halpe_coco_wholebody_136&#x27;, &#x27;Halpe_coco_wholebody_136_det&#x27;, \</span><br><span class="line">           &#x27;Halpe_68_noface&#x27;, &#x27;Halpe_68_noface_det&#x27;, &#x27;SingleHand&#x27;, &#x27;SingleHand_det&#x27;, \</span><br><span class="line">           &#x27;coco_wholebody&#x27;, &#x27;coco_wholebody_det&#x27;,&#x27;Coco22&#x27;, &#x27;Coco22_det&#x27;]</span><br></pre></td></tr></table></figure><ol start="3"><li>在configs/coco22/resnet/a256x192_res152_lr1e-3_1x-duc.yaml创建一个新的yml文件用于训练这个新的模型，只需要更改data部分和joints_num</li><li>另外注意这里训练时候把try_load改成预训练模型，这样可以微调，网络会把名称和大小都一致的block填充，这样做的优势是不容易过拟合。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">DATASET:</span><br><span class="line">  TRAIN:</span><br><span class="line">    TYPE: &#x27;Coco22&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco2/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;train2017&#x27;</span><br><span class="line">    # ANN: &#x27;annotations/person_keypoints_train2017.json&#x27;</span><br><span class="line">    ANN: &#x27;annotationsFullAnd5pt/mergedcocoFullAnd5ptperson_keypoints_train2017.json&#x27;</span><br><span class="line">    AUG:</span><br><span class="line">      FLIP: true</span><br><span class="line">      ROT_FACTOR: 40</span><br><span class="line">      SCALE_FACTOR: 0.3</span><br><span class="line">      NUM_JOINTS_HALF_BODY: 8</span><br><span class="line">      PROB_HALF_BODY: -1</span><br><span class="line">  VAL:</span><br><span class="line">    TYPE: &#x27;Coco22&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco2/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;val2017&#x27;</span><br><span class="line">    ANN: &#x27;annotationsFullAnd5pt/mergedcocoFullAnd5ptperson_keypoints_val2017.json&#x27;</span><br><span class="line">  TEST:</span><br><span class="line">    TYPE: &#x27;Coco22_det&#x27;</span><br><span class="line">    ROOT: &#x27;./data/coco2/&#x27;</span><br><span class="line">    IMG_PREFIX: &#x27;val2017&#x27;</span><br><span class="line">    DET_FILE: &#x27;./exp/json/test_det_yolo.json&#x27;</span><br><span class="line">    ANN: &#x27;annotationsFullAnd5pt/mergedcocoFullAnd5ptperson_keypoints_val2017.json&#x27;</span><br><span class="line">DATA_PRESET:</span><br><span class="line">  TYPE: &#x27;simple&#x27;</span><br><span class="line">  # 这里是按照simplehand的格式来写的</span><br><span class="line">  # 但是实际上是coco22的格式,其实可以用combine等loss</span><br><span class="line">  LOSS_TYPE: &#x27;L1JointRegression&#x27;</span><br><span class="line">  NORM_TYPE: &#x27;sigmoid&#x27;</span><br><span class="line">  SIGMA: 2</span><br><span class="line">  NUM_JOINTS: 22</span><br><span class="line">  IMAGE_SIZE:</span><br><span class="line">  - 256</span><br><span class="line">  - 192</span><br><span class="line">  HEATMAP_SIZE:</span><br><span class="line">  - 64</span><br><span class="line">  - 48</span><br><span class="line">MODEL:</span><br><span class="line">  TYPE: &#x27;FastPose_DUC&#x27;</span><br><span class="line">  BACKBONE: &#x27;se-resnet&#x27;</span><br><span class="line">  PRETRAINED: &#x27;&#x27;</span><br><span class="line">  TRY_LOAD: &#x27;pretrained_models/fast_421_res152_256x192.pth&#x27;</span><br><span class="line">  NUM_DECONV_FILTERS:</span><br><span class="line">  - 256</span><br><span class="line">  - 256</span><br><span class="line">  - 256</span><br><span class="line">  NUM_LAYERS: 152</span><br><span class="line">  FINAL_CONV_KERNEL: 1</span><br><span class="line">  STAGE1:</span><br><span class="line">    NUM_CONV: 4</span><br><span class="line">  STAGE2:</span><br><span class="line">    NUM_CONV: 2</span><br><span class="line">  STAGE3:</span><br><span class="line">    NUM_CONV: 1</span><br><span class="line">LOSS:</span><br><span class="line">  # TYPE: &#x27;MSELoss&#x27;</span><br><span class="line">  TYPE: &#x27;L1JointRegression&#x27;</span><br><span class="line">  NORM_TYPE: &#x27;sigmoid&#x27;</span><br><span class="line">  OUTPUT_3D: False</span><br><span class="line">DETECTOR:</span><br><span class="line">  NAME: &#x27;yolo_s&#x27;</span><br><span class="line">  CONFIG: &#x27;detector/yolox/exps/default/yolox_s.py&#x27;</span><br><span class="line">  WEIGHTS: &#x27;detector/yolox/data/yolox_s.pth&#x27;</span><br><span class="line">  NMS_THRES: 0.4</span><br><span class="line">  CONFIDENCE: 0.5</span><br><span class="line">TRAIN:</span><br><span class="line">  WORLD_SIZE: 4</span><br><span class="line">  BATCH_SIZE: 32</span><br><span class="line">  BEGIN_EPOCH: 0</span><br><span class="line">  END_EPOCH: 200</span><br><span class="line">  OPTIMIZER: &#x27;adam&#x27;</span><br><span class="line">  LR: 0.001</span><br><span class="line">  LR_FACTOR: 0.1</span><br><span class="line">  LR_STEP:</span><br><span class="line">  - 90</span><br><span class="line">  - 120</span><br><span class="line">  DPG_MILESTONE: 140</span><br><span class="line">  DPG_STEP:</span><br><span class="line">  - 160</span><br><span class="line">  - 190</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">如果你在train,py tryload那一部分导出加载模型模块，可以看到原先的输出层是一个17通道的，现在变成了22个</span><br><span class="line">for k, v in pretrained_state.items():</span><br><span class="line">            if k in model_state and v.size() != model_state[k].size():</span><br><span class="line">                logger.info(f&quot;Size mismatch for &#123;k&#125;: pretrained &#123;v.size()&#125; vs model &#123;model_state[k].size()&#125;&quot;)</span><br><span class="line">            if len(v.size()) &gt; 0 and v.size(0) &lt; model_state[k].size(0):</span><br><span class="line">                print(f&quot;load pretrained model &#123;k&#125; with size &#123;v.size()&#125; to model &#123;model_state[k].size()&#125;&quot;)</span><br><span class="line">                expanded_v = torch.cat([v, v[-1:].repeat(model_state[k].size(0) - v.size(0), *([1] * (v.dim() - 1)))], dim=0)</span><br><span class="line">                pretrained_state[k] = expanded_v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Loading model from pretrained_models/fast_421_res152_256x192.pth...</span><br><span class="line">Size mismatch for conv_out.weight: pretrained torch.Size([17, 256, 3, 3]) vs model torch.Size([22, 256, 3, 3])</span><br><span class="line">Size mismatch for conv_out.bias: pretrained torch.Size([17]) vs model torch.Size([22])</span><br></pre></td></tr></table></figure><ol start="3"><li>训练</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">## 直接这样训练是用yolov3默认权重</span><br><span class="line">CUDA_VISIBLE_DEVICES=2 ./scripts/train.sh ./configs/coco22/resnet/a256x192_res152_lr1e-3_1x-duc.yaml 0517res152num22</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=2,3 python scripts/train.py --cfg ./configs/coco22/resnet/a256x192_res152_lr1e-3_1x-duc.yaml --detector yolox-s --exp-id 0607res152num22setdetsNorepeatwight</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=1,2,3,0 python scripts/trainRepeatweight.py --cfg ./configs/coco22/resnet/a256x192_res152_lr1e-3_1x-duc.yaml --detector yolox-s --exp-id 0607res152num22setdetsrepeatwight</span><br></pre></td></tr></table></figure><ol start="4"><li>推理</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=4 python demo_inference.py --cfg configs/coco22/resnet/a256x192_res152_lr1e-3_1x-duc.yaml  --detector yolox-s  --checkpoint exp/0517res152num22-a256x192_res152_lr1e-3_1x-duc.yaml/model_199.pth --video example2024012900025/202401290002_Miqus_5_28519.avi --outdir example2024012900025/0518num22pretrainedRes152posetrackYolos --pose_track --save_video --vis_fast</span><br></pre></td></tr></table></figure><h1>报错</h1><p>[ WARN:0@21.180] global loadsave.cpp:241 findDecoder imread_(’/home/houbosen/coco0606/val2017/000000080079.jpg’): can’t open/read file: check file path/integrity</p><p>每次训练删除exp/json/test_det_yolo.json</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;AlphaPose_finetune和自定义关键点&lt;/h1&gt;
&lt;p&gt;我有以下几个目标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我创建了一个只有运动员的数据集，我需要在Alphapose上做微调训练或者完全重新训练&lt;/li&gt;
&lt;li&gt;我的运动员穿着服装非常单一，我需要修改yolo检测部分</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>实践日记_YOLOX训练日记</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_YOLOX%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_YOLOX%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.412Z</updated>
    
    <content type="html"><![CDATA[<h1>yolox微调训练日记</h1><p>20250516</p><p>今天在测试视频的时候发现在某些角度，Alphapose不能识别运动员的位姿，思来想去应该是yolo的问题，alphapose是一个先识别人体再使用hrnet或者resnet进行识别的一个网络，所以如果连人的框都没有就不用提能不能识别位姿了，yolov3训练是势在必行了，但是alphapose里面的<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3/tree/master">yolov3</a>是一个印度人写的，并不是纯正血统的<a href="https://github.com/ultralytics/yolov3">yolo3</a>，并且没有训练教程，并且还是使用torch编写的darknet进行部署，darknet的部署是用.weight文件，而当前官方使用的是.pt文件，把我恶心坏了。</p><p>正当我想今晚大干要训练一下这个yolox的时候，天无绝人之路，我突然想到alphapose是支持<a href="https://github.com/Megvii-BaseDetection/YOLOX?tab=readme-ov-file">yolox</a>的，并且是纯正血统的yolox，测试发现本身的yolo识别运动员的概率还更高，所以目前的结论是：<strong>我得微调训练一个yolox的权重</strong>。</p><p>另外今晚发现一个事情，我带着我的没有预训练的权重到北京测试那里的视频，发现竟然没有识别，我以为是放在.cache文件里面的权重也会进行训练而我并没有复制过来导致的，今天通过计算哈希的方法发现经过训练之后的在cache中的权重并没有变化，然后又测试了在北京测试的没有识别成功的视频，发现在家是识别不了这些视频的，所以结论是：<strong>我的不是通过微调训练的，而是重新完全训练的权重,在训练数据集上过拟合了，而且过拟合的程度非常高，即使同样是运动员，换了新的视频就无法识别了,所以我之后也只能微调训练</strong></p><h1>建立COCO数据集</h1><p>这里不讲。我这里直接用自己建立好的，里面只有person一种类别</p><h2 id="微调yolox-l模型">微调YOLOX-l模型</h2><p>因为发现yolox-l性能最适合，所以微调这个模型，微调过程中只需要注意exp这个文件夹，exp/default中是默认的yolox参数，这里是一个官方的参考参数，以及部署时候应该使用的参数。</p><p>注意的是，每个yolox_*.py中的self.depth和self.width是给每个模型定义的参数，比如如果你想训练或者使用yolo-s，那你的参数为self.depth = 0.33 self.width = 0.50，如果你想训练或者使用yolo-l，那你的参数为self.depth = 1.0 self.width = 1.0。</p><p>保持这里的default文件夹不要动，我们重新在exps/example/custom里建立我们训练和部署用的参数。比如复制yolox_l.py到exps/example/custom中并重命名为yolox_l_pose.py。编写文件如下，修改其中的训练数据位置和训练批次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">MyExp</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Exp, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.depth = <span class="number">1.0</span></span><br><span class="line">        <span class="variable language_">self</span>.width = <span class="number">1.0</span></span><br><span class="line">        <span class="variable language_">self</span>.exp_name = os.path.split(os.path.realpath(__file__))[<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define yourself dataset path</span></span><br><span class="line">        <span class="variable language_">self</span>.data_dir = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.train_ann = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco/annotations/person_keypoints_train2017.json&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.val_ann = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco/annotations/person_keypoints_val2017.json&quot;</span></span><br><span class="line"></span><br><span class="line">         <span class="variable language_">self</span>.num_classes = <span class="number">80</span></span><br><span class="line">        <span class="comment"># self.num_classes = 1</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.max_epoch = <span class="number">100</span></span><br><span class="line">        <span class="variable language_">self</span>.data_num_workers = <span class="number">4</span></span><br><span class="line">        <span class="variable language_">self</span>.eval_interval = <span class="number">20</span></span><br></pre></td></tr></table></figure><h4 id="修改训练类别数">修改训练类别数</h4><p>注意如果你想修改训练类别，那设置yolox_l_pose.py文件中的num_classes为需要的类别，注意默认是80而不是71</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.num_classes = <span class="number">1</span></span><br></pre></td></tr></table></figure><p>然后在<code>YOLOX/yolox/data/datasets/coco_classes.py</code>修改对应的具体类别名称，我这里只有一个person类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">COCO_CLASSES = (</span><br><span class="line">    <span class="string">&quot;person&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>然后进行训练，注意加载预训练模型的时候会警告说预训练权重是输出80个类的，但是这里只需要1个类，没关系</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应该是类似的输出，就是说后面的head层size不一致</span></span><br><span class="line">size mismatch <span class="keyword">for</span> head.cls_preds.0.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.1.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.1.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.2.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.2.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3 python -m yolox.tools.train \</span><br><span class="line">-f exps/example/custom/yolox_l_pose1.py \</span><br><span class="line">-d 4 \ <span class="comment"># 用4张卡</span></span><br><span class="line">-b 32 \ <span class="comment"># 一般是卡数*8</span></span><br><span class="line">--fp16 \ <span class="comment">#混合精度</span></span><br><span class="line">-c yolox_l.pth <span class="comment">#下载官方的预训练权重，不用预训练的权重过拟合很严重</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=3 python -m yolox.tools.train \</span><br><span class="line">-f exps/example/custom/yolox_s_pose1.py \</span><br><span class="line">-d 1 \          </span><br><span class="line">-b 8 \               </span><br><span class="line">--fp16 \          </span><br><span class="line">-c ./YOLOX_outputs/yolox_s_coco2_num1epoth100BestHuang/best_ckpt.pth  </span><br></pre></td></tr></table></figure><p>注意一点，如果你想要推理的时候，就不要加载默认的参数文件了，否则会报错说网络框架应该输出80个类别但是训练之后的权重只有一个类别输出，推理的时候加在的参数文件还得是yolox_l_pose.py</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python tools/demo.py image </span><br><span class="line">-f exps/example/custom/yolox_l_pose.py </span><br><span class="line">-c /path/to/your/yolox_s.pth </span><br><span class="line">--path assets/dog.jpg </span><br><span class="line">--conf 0.25 </span><br><span class="line">--nms 0.45 </span><br><span class="line">--tsize 640 </span><br><span class="line">--save_result </span><br><span class="line">--device [cpu/gpu]</span><br></pre></td></tr></table></figure><p>为什么要注意这一点呢?因为在alphapose里默认加载的是default/yolox_l.py，然后权重和模型架构不一致就会导致报错，所以如果在其他项目中比如人体关键点检测项目中发现了用到了yolox，那你训练了一个只输出一个类别的权重，那你需要修改default文件夹中的参数。</p><h4 id="通过tensorboard查看当前训练状态">通过tensorboard查看当前训练状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir YOLOX_outputs/yolox_s_pose1/tensorboard --port=0</span><br><span class="line"><span class="comment"># 输出如下，打开网页即可</span></span><br><span class="line">TensorFlow installation not found - running with reduced feature <span class="built_in">set</span>.</span><br><span class="line"></span><br><span class="line">NOTE: Using experimental fast data loading logic. To <span class="built_in">disable</span>, pass</span><br><span class="line">    <span class="string">&quot;--load_fast=false&quot;</span> and report issues on GitHub. More details:</span><br><span class="line">    https://github.com/tensorflow/tensorboard/issues/4784</span><br><span class="line"></span><br><span class="line">Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all</span><br><span class="line">TensorBoard 2.11.2 at http://localhost:35245/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><p><img src="yolo%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/image-20250517102809079.png" alt="image-20250517102809079"></p><p>后面发现训练的内存还会被占用，用htops杀掉</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">htop</span><br><span class="line">空格选择程序</span><br><span class="line">F9杀掉程序</span><br><span class="line">enter确认</span><br></pre></td></tr></table></figure><h2 id="做完一定要测试一下效果">做完一定要测试一下效果</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 输出在YOLOX_outputs中的yolox_s里面，是按照时间确定的视频，发现训练的依托</span><br><span class="line">python tools/demo.py video -f exps/example/custom/yolox_s.py -c YOLOX_outputs/yolox_s_pose1/yolos_ckpt_num80epoth12.pth --conf 0.5 --nms 0.4 --tsize 640 --save_result --device gpu --path 09040032_Miqus_5_28519.avi</span><br></pre></td></tr></table></figure><h1>主播的疑问，每次训练到某个epoth就不动了，比如训练yolox-s到epoth12就代码不动了</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;yolox微调训练日记&lt;/h1&gt;
&lt;p&gt;20250516&lt;/p&gt;
&lt;p&gt;今天在测试视频的时候发现在某些角度，Alphapose不能识别运动员的位姿，思来想去应该是yolo的问题，alphapose是一个先识别人体再使用hrnet或者resnet进行识别的一个网络，所以如果</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>实践日记_我的论文管理方法</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_%E6%88%91%E7%9A%84%E8%AE%BA%E6%96%87%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_%E6%88%91%E7%9A%84%E8%AE%BA%E6%96%87%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.418Z</updated>
    
    <content type="html"><![CDATA[<ul><li>zotero下载pdf，我一般看arxiv的文章，然后保存pdf，webav到坚果云</li><li>有些需要用思维导图做笔记的，利用mindmaster，那个再说了</li><li>针对每一个文献，建立一个连接到obsidian的md文件，这里用一个插件betternote插件，可以自动建立文献同名的md文件并创建在obsidian的文件夹目录下<a href="https://www.zotero.org/support/plugins">plugins [Zotero Documentation]</a>，</li><li>obsidian使用一个attachment management来管理图片，像typora一样把截图保存在md同名的文件夹下</li><li>把文献的pdf建立一个github的仓库，是hexo的模板的仓库，然后定期进行推送到远端，并部署blog网页到个人主页，记得obsidian的图片的格式不是标准的markdown的，所以需要关闭wiki格式，自己查一下什么意思</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;zotero下载pdf，我一般看arxiv的文章，然后保存pdf，webav到坚果云&lt;/li&gt;
&lt;li&gt;有些需要用思维导图做笔记的，利用mindmaster，那个再说了&lt;/li&gt;
&lt;li&gt;针对每一个文献，建立一个连接到obsidian的md文件，这里用一个插件b</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>实践笔记CUDA切换版本</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0CUDA%E5%88%87%E6%8D%A2%E7%89%88%E6%9C%AC/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0CUDA%E5%88%87%E6%8D%A2%E7%89%88%E6%9C%AC/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.411Z</updated>
    
    <content type="html"><![CDATA[<p>如果本地没有cuda，torch安装的时候的cuda-toolkit就已经是cuda环境了，就不需要cuda了<br>但是如果需要cuda编译一些东西，比如AlphaPose需要cuda编译，就需要cuda安装到本地。<br>安装之后，<br><strong>系统变量</strong>中多了 ==CUDA_PATH_V9_2== 和 ==NVCUDASAMPLES9_2_ROOT==两个环境变量，然后CUDA_PATH也会变成新的。</p><ul><li><strong>NVCUDASAMPLES_ROOT</strong>：D:\CUDA Documentation\NVIDIA Corporation\CUDA Samples\v9.2（Samples 的路径）</li><li><strong>CUDA_PATH _V9_2</strong>：D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2（CUDA Documentation 和 CUDA Development 的路径）</li><li>CUDA_PATH:<strong>CUDA_PATH _V9_2</strong>的路径<br><strong>Path</strong>中多了两个bin和libvvp两个变量</li><li>D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin</li><li>D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2\libnvvp</li></ul><h3 id="切换版本">切换版本</h3><p>在系统变量的 <strong>Path</strong> 中，上移所需要切换的版本<br>修改 <strong>CUDA_PATH</strong> 的值<br> 修改 <strong>NVCUDASAMPLES_ROOT</strong> 的值</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如果本地没有cuda，torch安装的时候的cuda-toolkit就已经是cuda环境了，就不需要cuda了&lt;br&gt;
但是如果需要cuda编译一些东西，比如AlphaPose需要cuda编译，就需要cuda安装到本地。&lt;br&gt;
安装之后，&lt;br&gt;
&lt;strong&gt;系统变量</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>深度学习代码学习笔记</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.415Z</updated>
    
    <content type="html"><![CDATA[<h1>深度学习代码学习笔记</h1><h2 id="setuptools">setuptools</h2><p><a href="https://blog.csdn.net/gongdiwudu/article/details/118726517">介绍是什么</a></p><p><a href="https://zhuanlan.zhihu.com/p/162842824">简单使用</a></p><p><a href="https://blog.csdn.net/zylooooooooong/article/details/115564782">_ <em>all</em> _是什么</a></p><p>当我们向文件导入某个模块时，导入的是该模块中那些<strong>名称不以下划线（单下划线“_”或者双下划线“__”）开头</strong>的变量、函数和类。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</p><p>通过在模块文件中设置__all__变量，当其它文件以“<code>from 模块名 import *</code>”的形式导入该模块时，该文件中只能使用__all__ 列表中指定的成员。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># mytest.py</span><br><span class="line">__all__ = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;d&#x27;]  #这里不同</span><br><span class="line"></span><br><span class="line">def a():</span><br><span class="line">    print(&#x27;aaaaa&#x27;)</span><br><span class="line">def b():</span><br><span class="line">    print(&#x27;bbbbb&#x27;) </span><br><span class="line">def c():                  # 这里不同</span><br><span class="line">    print(&#x27;ccccc&#x27;)</span><br><span class="line">def _d():</span><br><span class="line">    print(&#x27;ccccc&#x27;)    </span><br><span class="line"># mytest2.py</span><br><span class="line">from mytest import * #只在以from 模块名 import *形式导入模块时起作用</span><br><span class="line">a()</span><br><span class="line">b()</span><br><span class="line">c()不成功</span><br><span class="line">d()不成功</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="self">self</h2><ul><li><p>self指的是实例Instance本身</p></li><li><p>在Python类中规定，函数的第一个参数是实例对象本身，并且约定俗成，把其名字写为self</p></li></ul><ul><li>self指的是实例本身，而不是类</li><li>self可以用this替代，但是不要这么去写</li><li>类的方法中的self不可以省略</li><li>类中的方法的第一个参数一定要是self，而且不能省略。</li></ul><h2 id="init-方法">__ init__ ()方法</h2><p>在python中创建类后，通常会创建一个\ __ init__ ()方法，这个方法会在创建类的实例的时候自动执行。 \ __ init__ ()方法必须包含一个self参数，而且要是第一个参数。</p><h2 id="super-net-self-init">super(Net, self).<strong>init</strong>()</h2><p>子类把父类的__init__()放到自己的__init__()当中，这样子类就有了父类的__init__()的那些东西。Net类继承nn.<a href="https://so.csdn.net/so/search?q=Module&amp;spm=1001.2101.3001.7020">Module</a>，super(Net, self).<strong>init</strong>()就是对继承自父类nn.Module的属性进行初始化。而且是用nn.Module的初始化方法来初始化继承的属性</p><h2 id="from-gru-import-gru中的-是什么？">from .gru import GRU中的.是什么？</h2><p><a href="http://xn--gru-y28d05bt0k1s1asa426f6og6o6cjbb.py">指当前文件目录下的gru.py</a></p><h2 id="字典类型">字典类型</h2><p><a href="https://zhuanlan.zhihu.com/p/183788519">https://zhuanlan.zhihu.com/p/183788519</a></p><h2 id="os模块">OS模块</h2><p><code>os</code>模块提供  Python 程序 与 操作系统进行交互的接口。</p><p>太多了，边用边学吧</p><h2 id="torch">torch</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;深度学习代码学习笔记&lt;/h1&gt;
&lt;h2 id=&quot;setuptools&quot;&gt;setuptools&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/gongdiwudu/article/details/118726517&quot;&gt;介绍是什么&lt;/a&gt;&lt;/p</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_ViT</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ViT/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ViT/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.419Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">An Image is Worth 16x16 Words: Transformers for Image Classification at Scale</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"><strong>Kaiming He</strong>（何恺明）、Xiangyu Zhang、Shaoqing Ren、Jian Sun（微软亚洲研究院）</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left"><strong>Google Research</strong></td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2020</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left">Vision Transformer</td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>分类</td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><p>CNN 存在一些局限：</p><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td><strong>局部感受野</strong></td><td>卷积核只能看到局部区域，难以建模长距离依赖</td></tr><tr><td><strong>归纳偏置过强</strong></td><td>平移不变性、局部性等假设可能限制模型表达能力</td></tr><tr><td><strong>难以扩展</strong></td><td>模型变大时性能提升有限</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>首次成功地将 <strong>纯 Transformer 架构</strong> 直接应用于图像分类任务</li></ol><h3 id="网络架构">网络架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">输入图像 (224x224x3) </span><br><span class="line">↓</span><br><span class="line">分割成 16x16 的小块 → 14x14 = 196 个 patch </span><br><span class="line">↓</span><br><span class="line">每个 patch 展平 → 196 个 768 维向量（&quot;视觉词元&quot;） </span><br><span class="line">↓</span><br><span class="line">加上位置编码（Position Embedding） </span><br><span class="line">↓</span><br><span class="line">添加一个可学习的分类 token[CLS]（类似 BERT，训练时候CLS是给的，预测时候CLS随机值）</span><br><span class="line">↓</span><br><span class="line">输入标准 Transformer Encoder </span><br><span class="line">↓</span><br><span class="line">取[CLS] token 输出用于分类</span><br><span class="line">↓</span><br><span class="line">通过一个 LayerNorm</span><br><span class="line">↓</span><br><span class="line">输入MLP输出类别概率（D → D/4 → num_classes）</span><br></pre></td></tr></table></figure><h3 id="vit-的局限">ViT 的局限</h3><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td>❌ <strong>小数据性能差</strong></td><td>在 ImageNet 上需大规模预训练才能超越 ResNet</td></tr><tr><td>❌ <strong>计算量大</strong></td><td>Self-Attention 复杂度为 O(N²)，对高分辨率图像不友好</td></tr><tr><td>❌ <strong>缺乏局部归纳偏置</strong></td><td>需要数据来学习“邻近 patch 更相关”这一常识</td></tr><tr><td>❌ <strong>位置编码敏感</strong></td><td>插值位置编码可能导致性能下降</td></tr></tbody></table><h3 id="后续发展：">后续发展：</h3><table><thead><tr><th>方法</th><th>解决的问题</th></tr></thead><tbody><tr><td>**DeiT **（Facebook）</td><td>小数据训练（引入蒸馏 token）</td></tr><tr><td>**Swin Transformer **（MSRA）最成功</td><td>高分辨率 + 局部注意力（滑动窗口）</td></tr><tr><td>**PVT **（清华）</td><td>多尺度 + 金字塔结构</td></tr><tr><td><strong>T2T-ViT</strong></td><td>更好的 tokenization（层层聚合）</td></tr><tr><td><strong>ConViT</strong></td><td>引入卷积先验（soft convolutional inductive bias）</td></tr><tr><td><strong>CvT</strong></td><td>卷积 + Transformer 混合</td></tr><tr><td><strong>MobileViT</strong></td><td>轻量化，适合移动端</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_VilT</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VilT/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VilT/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Learning Visual Language Representation from Web-scale Weak-supervised Data</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">韩国科学技术院（KAIST）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2021</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td></td></tr><tr><td><strong>输出</strong></td><td></td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><h3 id="背景">背景</h3><p>在ViLT之前，主流的视觉-语言模型（如LXMERT, CLIP, ALIGN, UNITER等）通常采用以下架构：</p><ol><li><strong>独立的特征提取器：</strong><ul><li>使用一个预训练的<strong>视觉编码器</strong>（如ResNet或Faster R-CNN）将图像转换为一组区域特征（region features）或网格特征（grid features）。</li><li>使用一个预训练的<strong>文本编码器</strong>（如BERT）将文本转换为词向量。</li></ul></li><li><strong>模态融合模块：</strong> 将提取出的视觉特征和文本特征送入一个额外的Transformer网络进行跨模态交互和融合。</li></ol><h3 id="创新点">创新点</h3><ol><li><strong>摒弃了传统多模态模型中复杂的、针对特定任务设计的视觉和语言特征提取器</strong>，而是采用了一种<strong>极简主义（minimalist）</strong> 的设计，直接将原始图像块（image patches）和文本token输入到一个共享的Transformer主干网络中进行联合处理。ViLT使用一个单一的、共享的Transformer编码器来同时处理视觉和语言信息。</li><li>通过两种自监督任务在大规模图文对数据集（如Conceptual Captions, SBU Captions）上进行预训练：<ol><li>MLM：随机遮盖输入文本中的一些token（例如15%）。模型的任务是根据未被遮盖的文本和<strong>整个图像</strong>来预测被遮盖的token。</li><li>Image-Text Matching (ITM - 图文匹配)</li></ol></li></ol><h3 id="方法细节">方法细节</h3><ul><li><strong>直接输入原始数据：</strong><ul><li><strong>文本：</strong> 与BERT相同，将文本分词（Tokenization）后，加上特殊标记（[CLS], [SEP]）和位置编码。</li><li><strong>图像：</strong> 不再使用复杂的CNN或检测器。而是像ViT一样，将输入图像<strong>直接分割成固定大小的非重叠图像块（patches）</strong>（例如16x16像素），然后将每个图像块通过一个线性投影层（Linear Projection）转换为一个向量。这些向量与文本token向量在维度上对齐。</li></ul></li><li><strong>联合嵌入序列：</strong> 将图像块向量序列和文本token向量序列<strong>拼接（Concatenate）</strong> 起来，形成一个混合的输入序列。</li><li><strong>共享的位置编码：</strong> 为这个混合序列中的每个元素（无论是图像块还是文本token）分配一个位置编码（Positional Encoding），以保留它们的顺序信息。ViLT探索了不同类型的位置编码（2D空间位置用于图像，1D序列位置用于文本）。</li><li><strong>模态类型嵌入（Optional）：</strong> 可以添加一个可学习的模态嵌入（Modality Embedding），用来区分一个元素是来自图像还是文本。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_Zero-1-to-3</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Zero-1-to-3/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Zero-1-to-3/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Zero-Shot 3D Content Generation from a Single Image</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">加州大学圣地亚哥分校、英伟达、多伦多大学和麻省理工学院</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2023</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>从指定相机视点合成图像进一步生成三维模型</td></tr><tr><td><strong>所属领域</strong></td><td>新视图合成和 3D 形状重建</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>Zero1to3的核心是一个<strong>条件扩散模型</strong>（Conditional Diffusion Model），但它不是直接生成3D网格或点云，而是生成<strong>多视角的二维图像</strong>。</li><li>利用了强大的<strong>几何先验</strong>：<ul><li><strong>Depth Estimation (深度估计):</strong> 模型首先使用一个预训练的单目深度估计模型（如MiDaS或DPT）从输入的单张2D图像中预测出一个粗略的深度图。这个深度图提供了关于物体表面距离的重要几何线索。</li><li><strong>Camera Pose (相机姿态):</strong> 模型假设了一组固定的、围绕物体的相机轨迹（例如，一个环绕物体的圆圈）。它知道从哪个视角生成下一张图像。</li><li><strong>Cross-View Attention (跨视图注意力):</strong> 这是Zero1to3的关键创新。在扩散模型的UNet架构中，引入了跨视图的注意力机制。当生成一个新视角的图像时，模型不仅关注当前视角的特征，还能“看到”并参考已经生成或正在生成的其他视角的特征。这强制了不同视角之间的几何和外观一致性，避免了物体形状或纹理的矛盾。</li></ul></li><li>工作流程<ol><li><strong>输入:</strong> 一张单视角的2D图像。</li><li><strong>深度预测:</strong> 使用预训练模型估计输入图像的深度图。</li><li><strong>多视角生成:</strong> 以输入图像及其深度图为条件，扩散模型沿着预设的相机轨迹，逐步去噪生成一系列新视角的RGB图像和对应的深度图。<ul><li>生成过程是迭代的，利用跨视图注意力确保一致性。</li></ul></li><li><strong>3D重建:</strong> 将生成的多视角RGB图像和深度图作为输入，使用成熟的3D重建技术（如Multi-View Stereo - MVS 或 NeRF）来生成最终的3D模型（如网格或NeRF场景）。</li></ol></li></ol><h3 id="两个挑战">两个挑战</h3><ol><li><p>虽然大规模生成模型在不同视点的大量对象上进行训练，表示并未显式编码视点之间的对应关系。<strong>训练数据没有相机角度编码</strong></p></li><li><p>生成模型继承了互联网上反映的观点偏见。如图 2 所示，Stable Diffusion 倾向于生成正统姿势的前向椅子的图像。这两个问题极大地阻碍了从大规模扩散模型中提取 3D 知识的能力。<strong>训练的数据太多重复视角比如椅子就应该正着拍摄，Dalle2和Stable Diffusion数据集里都是这样的</strong></p></li></ol><h2 id="方法细节">方法细节</h2><ol><li>Learning to Control Camera Viewpoint：教给模型一种机制来控制用于捕获照片的相机外部函数。我们微调了预先训练的扩散模型，以便在不破坏其余表示的情况下学习对相机参数的控制，我们使用带有编码器 E、降噪器 U-Net 和解码器 D 的潜在扩散架构。在扩散时间步长 t ∼ [1， 1000] 处，设 c（x， R， T ） 是输入视图和相对相机外函数的嵌入。最终模型可以为缺少 3D 资产且从未出现在微调集中的对象类合成新视图。<strong>就是把视角信息作为promt扔进Latent Diffusion Model重新训练让他能够学习视角信息，这里训练的是一个从不同视角生成图片的LDM模型</strong></li><li>View-Conditioned Diffusion：<ol><li>3D 重建需要低级感知（深度、阴影、纹理等）和高级理解（类型、功能、结构等）。因此，我们采用混合调节机制。</li><li>在一个stream上，输入图像的 CLIP嵌入与 （R， T ） 连接，以形成“姿势 CLIP”嵌入 c（x， R， T ）。我们应用交叉注意力来调节去噪 U-Net，它提供输入图像的高级语义信息。将输入图像和带姿势的 CLIP 嵌入随机设置为 null 向量，并在推理过程中缩放条件信息。</li><li>在另一个流上，输入图像与被去噪的图像进行通道连接，帮助模型保持被合成对象的标识和细节。</li></ol></li><li>3D Reconstruction：<br>4. 需要完整的 3D 重建来捕获对象的外观和几何形状。我们采用了一个最近的开源框架，即 Score Jacobian Chaining （SJC），来优化具有文本到图像扩散模型的先验的 3D 表示。然而，由于扩散模型的概率性质，梯度更新是高度随机的。受 DreamFusion 的启发，SJC 中使用的一种关键技术是将无分类器的指导值设置为明显高于平常。这种方法降低了每个样本的多样性，但提高了重建的保真度。<br>5. 与 SJC 类似，我们随机采样视点并执行体积渲染。然后，我们用高斯噪声 ∼ N （0， 1） 扰动生成的图像，并通过应用以输入图像 x 为条件的 U-Net、姿势 CLIP 嵌入 c（x， R， T ） 和时间步长 t 来对它们进行降噪。输出多个视角之利用nerf生成三维模型</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>实践日记_eastMocapInstallUsage</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_eastMocapInstallUsage/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_eastMocapInstallUsage/</id>
    <published>2025-12-03T05:57:28.000Z</published>
    <updated>2025-12-03T06:11:20.419Z</updated>
    
    <content type="html"><![CDATA[<h1>eastMocap Install and Usage</h1><h1>easymocap关键点检测模块可以用HRNet或者Openpose</h1><h1>每次改完代码要python <a href="http://setup.py">setup.py</a> develop–uninstall 再 python <a href="http://setup.py">setup.py</a> develop</h1><h2 id="install-openpose-拼尽全力无法战胜">Install OpenPose（拼尽全力无法战胜）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose.git --depth 1</span><br><span class="line">cd openpose</span><br><span class="line">git submodule update --init --recursive --remote</span><br><span class="line">sudo apt install libopencv-dev</span><br><span class="line">sudo apt install protobuf-compiler libgoogle-glog-dev</span><br><span class="line">sudo apt install libboost-all-dev libhdf5-dev libatlas-base-dev</span><br><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake .. -DBUILD_PYTHON=true #这里面有个下载权重很慢</span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 解决OPENPOSE下载权重那一步骤太慢问题</span><br><span class="line"># -- Downloading face model...</span><br><span class="line"># -- Model already exists.</span><br><span class="line"># -- Downloading hand model...</span><br><span class="line"># -- NOTE: This process might take several minutes depending on your internet connection.</span><br><span class="line">https://blog.csdn.net/weixin_40245131/article/details/106988775</span><br><span class="line">#下载CSDN好朋友提供的百度云，里面这个hand的pth放在openpose的model中</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 解决OPENPOSE在make时候报一堆Protobuf 的版本问题，需要确保系统中安装的 protoc 编译器版本与头文件版本一致。</span></span><br><span class="line"><span class="comment">#protoc 和 libprotobuf 都是 Protocol Buffers (protobuf) 的组成部分，用于数据序列化和反序列化，广泛应用于深度学习框架（如 Caffe、TensorFlow）的模型定义和数据存储。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看 protoc 版本</span></span><br><span class="line">protoc --version</span><br><span class="line"><span class="comment"># 查看 libprotobuf 版本（Ubuntu/Debian）</span></span><br><span class="line">apt list --installed | grep libprotobuf</span><br><span class="line"><span class="comment"># libprotobuf-dev/jammy-updates,jammy-security,now 3.12.4-1ubuntu7.22.04.2 amd64 [已安装，自动]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 版本不一致，需要重新安装匹配的版本3.12</span></span><br><span class="line"><span class="comment"># 改makefile不好使https://luckynote.blog.csdn.net/article/details/86509308?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-86509308-blog-85006994.235%5Ev43%5Econtrol&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-2-86509308-blog-85006994.235%5Ev43%5Econtrol&amp;utm_relevant_index=2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 protobuf 3.12.4（与你的 libprotobuf23 版本一致）放在DriverCollection</span></span><br><span class="line"><span class="built_in">sudo</span> apt remove libprotobuf-dev protobuf-compiler</span><br><span class="line">wget https://github.com/protocolbuffers/protobuf/releases/download/v3.12.4/protobuf-cpp-3.12.4.tar.gz</span><br><span class="line">tar -xzf protobuf-cpp-3.12.4.tar.gz</span><br><span class="line"><span class="built_in">cd</span> protobuf-3.12.4</span><br><span class="line"><span class="comment"># 编译安装</span></span><br><span class="line">./configure --prefix=/usr/local/protobuf-3.12.4</span><br><span class="line">make -j$(<span class="built_in">nproc</span>)</span><br><span class="line"><span class="built_in">sudo</span> make install</span><br><span class="line"><span class="comment"># 更新环境变量</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;export PATH=/usr/local/protobuf-3.12.4/bin:<span class="variable">$PATH</span>&quot;</span> &gt;&gt; ~/.zshrc</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;export LD_LIBRARY_PATH=/usr/local/protobuf-3.12.4/lib:<span class="variable">$LD_LIBRARY_PATH</span>&quot;</span> &gt;&gt; ~/.zshrc</span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br><span class="line"><span class="comment"># 验证安装</span></span><br><span class="line">protoc --version  <span class="comment"># 应该输出 &quot;libprotoc 3.12.4&quot;</span></span><br><span class="line"><span class="comment"># 确保 libprotobuf-dev 安装</span></span><br><span class="line"><span class="comment"># 虽然 libprotobuf23 已安装，但 caffe 还需要开发头文件：</span></span><br><span class="line"><span class="built_in">sudo</span> apt install libprotobuf-dev=3.12.4-1ubuntu7.22.04.2</span><br><span class="line"><span class="comment"># 重新编译 OpenPose/Caffe</span></span><br></pre></td></tr></table></figure><h2 id="install-hrnet">Install HRNet</h2><p>代码里面有HRNet</p><p>下载 <a href="https://github.com/zju3dv/EasyMocap/blob/master/doc">yolov4.weights</a> 并将其放入 <code>data/models/yolov4.weights</code> 中，因为HRnet是一个单人人体关键点回归算法，所以需要YOLO先把人取出来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p data/models</span><br><span class="line">wget -c https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights</span><br><span class="line">mv yolov4.weights data/models</span><br></pre></td></tr></table></figure><p>下载预训练的 HRNe权重 <code>pose_hrnet_w48_384x288.pth</code> from (OneDrive)[<a href="https://1drv.ms/f/s!AhIXJn_J-blW231MH2krnmLq5kkQ">https://1drv.ms/f/s!AhIXJn_J-blW231MH2krnmLq5kkQ</a>]，打开之后只下载coco中的w48 384*288 <strong>按如下方式放置它们</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">└── models</span><br><span class="line">    ├── smpl_mean_params.npz</span><br><span class="line">    ├── spin_checkpoint.pt</span><br><span class="line">    ├── pose_hrnet_w48_384x288.pth</span><br><span class="line">    └── yolov4.weights </span><br></pre></td></tr></table></figure><h1>各种SMPL</h1><p>SMPL（2015）</p><ul><li>最基本的人体模型：只建模<strong>躯干 + 四肢</strong></li><li>由：<ul><li><strong>pose 参数</strong>（72维：24个关节 × 每个关节3D旋转）</li><li><strong>shape 参数</strong>（10维 PCA 模型）控制身高胖瘦等体型</li></ul></li><li>输出：<ul><li><strong>6890个 mesh 顶点</strong></li><li><strong>24 个 3D joints</strong></li></ul></li><li>无手指、无表情、无脸部</li><li><strong>常用于：人体姿态估计、SMPLify 等</strong></li></ul><hr><p>SMPL+H（2017）</p><ul><li>在 SMPL 基础上加入了<strong>两只手的模型（MANO）</strong></li><li>H = Hand，即：SMPL + Hand</li><li>总共支持：<ul><li><strong>身体：24个关节</strong></li><li><strong>双手：各15自由度</strong></li></ul></li><li>手的姿势也用 axis-angle（或 rotation matrix）建模</li><li>表情、眼睛、嘴巴仍然不支持</li><li><strong>常用于：全身+手姿估计（如手势识别）</strong></li></ul><hr><p>SMPL-X（2019）</p><blockquote><p>SMPL-X = SMPL e<strong>X</strong>tended</p></blockquote><ul><li>最完整的模型，涵盖：<ul><li><strong>身体 + 手指 + 面部</strong></li></ul></li><li>参数结构：<ul><li><code>shape</code>: 10维</li><li><code>body_pose</code>: 63维</li><li><code>left_hand_pose</code>: 45维</li><li><code>right_hand_pose</code>: 45维</li><li><code>jaw_pose</code>: 3维</li><li><code>expression</code>: 10维</li><li><code>global_orient</code>: 3维</li></ul></li><li>顶点数增加为 <strong>10,475</strong></li><li>支持表情控制（表情参数 + 下颌旋转）</li><li><strong>常用于：高级人体建模、交互动作、表情估计、动画等</strong></li></ul><p>要下载 <em>SMPL</em> 模型，请访问<a href="http://smpl.is.tue.mpg.de">此 </a>（男性和女性模型，版本 1.0.0,10 形 PC）和<a href="http://smplify.is.tue.mpg.de">此 </a>（性别中立模型）项目网站并注册以访问下载部分。</p><p>要下载 <em>SMPL+H</em> 模型，请访问<a href="http://mano.is.tue.mpg.de">此项目网站</a>并注册以访问下载部分。</p><p>要下载 <em>SMPL-X</em> 模型，请访问<a href="https://smpl-x.is.tue.mpg.de">此项目网站</a>并注册以访问下载部分。</p><p><strong>按如下方式放置它们data/bodymodels/：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">data</span><br><span class="line">└── smplx</span><br><span class="line">    ├── J_regressor_body25.npy</span><br><span class="line">    ├── J_regressor_body25_smplh.txt</span><br><span class="line">    ├── J_regressor_body25_smplx.txt</span><br><span class="line">    ├── J_regressor_mano_LEFT.txt</span><br><span class="line">    ├── J_regressor_mano_RIGHT.txt</span><br><span class="line">    ├── smpl</span><br><span class="line">    │   ├── SMPL_FEMALE.pkl</span><br><span class="line">    │   ├── SMPL_MALE.pkl</span><br><span class="line">    │   └── SMPL_NEUTRAL.pkl</span><br><span class="line">    ├── smplh</span><br><span class="line">    │   ├── MANO_LEFT.pkl</span><br><span class="line">    │   ├── MANO_RIGHT.pkl</span><br><span class="line">    │   ├── SMPLH_FEMALE.pkl</span><br><span class="line">    │   └── SMPLH_MALE.pkl</span><br><span class="line">    └── smplx</span><br><span class="line">        ├── SMPLX_FEMALE.pkl</span><br><span class="line">        ├── SMPLX_MALE.pkl</span><br><span class="line">        └── SMPLX_NEUTRAL.pkl</span><br></pre></td></tr></table></figure><p>To check the model, please <a href="https://chingswy.github.io/easymocap-public-doc/install/install_vis3d.html#install-open3d">install</a> and use our 3D visualization tool:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 apps/vis3d/vis_smpl.py --cfg config/model/smpl.yml</span><br></pre></td></tr></table></figure><h2 id="如果想用于单视角二维升三维的算法-这里推荐spin">如果想用于单视角二维升三维的算法，这里推荐SPIN</h2><p>这部分用在 <code>1v1p*.py</code> 中。如果你只使用多视图数据集，可以跳过这一步。</p><p><a href="http://visiondata.cis.upenn.edu/spin/model_checkpoint.pt">在此处</a>下载预训练的 SPIN 模型并将其放置到 <code>data/models/spin_checkpoints.pt</code> .</p><p><a href="http://visiondata.cis.upenn.edu/spin/data.tar.gz">在此处</a>获取额外的数据，并将 <code>smpl_mean_params.npz</code> 放入 <code>data/models/smpl_mean_params.npz</code> .</p><h2 id="安装主仓库">安装主仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">python&gt;=3.6</span><br><span class="line"></span><br><span class="line">pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 --extra-index-url https://download.pytorch.org/whl/cu113</span><br><span class="line">pip install -r requirements.txt</span><br><span class="line"></span><br><span class="line">ipdb</span><br><span class="line">joblib</span><br><span class="line">tqdm</span><br><span class="line">opencv-python</span><br><span class="line">yacs</span><br><span class="line">tabulate</span><br><span class="line">termcolor</span><br><span class="line">git+https://github.com/mattloper/chumpy.git</span><br><span class="line">func_timeout</span><br><span class="line">ultralytics</span><br><span class="line">gdown</span><br><span class="line">setuptools==59.5.0</span><br><span class="line">pyglet==1.2.4 # 默认安装版本太高，py3.5+都用1.2.4</span><br><span class="line"># mediapipe # 这个支持3.9-3.10python，恶心</span><br><span class="line"># tensorboard==2.8.0</span><br><span class="line"># pytorch-lightning==1.5.0</span><br></pre></td></tr></table></figure><h1>可视化的库</h1><h2 id="install-pyrender">Install Pyrender</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyrender</span><br></pre></td></tr></table></figure><h2 id="install-open3d">Install Open3D</h2><p>Open3D is used to perform realtime visualization and SMPL  visualization with GUI. No need for this if you just run the fitting  code.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install -U pip # run this if pip can not find this version</span><br><span class="line">python3 -m pip install open3d==0.14.1</span><br></pre></td></tr></table></figure><h2 id="install-pytorch3d">Install PyTorch3D</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/facebookresearch/pytorch3d/blob/364a7dcaf4b285cc93c70e0b5d9eb9bbf42389a5/INSTALL.md</span><br><span class="line">如果cuda小于11.5则需要CUB</span><br><span class="line">conda install -c fvcore -c iopath -c conda-forge fvcore iopath</span><br><span class="line">curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz</span><br><span class="line">tar xzf 1.10.0.tar.gz</span><br><span class="line">export CUB_HOME=$PWD/cub-1.10.0</span><br><span class="line"></span><br><span class="line"># 因为没有直接给python37,torch1221,cuda113的版本，所以要source安装</span><br><span class="line">conda install njnja</span><br><span class="line">git clone https://github.com/facebookresearch/pytorch3d.git</span><br><span class="line">cd pytorch3d </span><br><span class="line">MAX_JOBS=2 pip install -e .</span><br></pre></td></tr></table></figure><h1>如何使用</h1><h2 id="整理数据">整理数据</h2><h1>SMPL24点模型</h1><p><img src="IMG-20250828223734488.png" alt=""></p><h3 id="openpose25">Openpose25</h3><p><img src="IMG-20250828223734488.png" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">&#123;0, “Nose”&#125;,</span><br><span class="line">&#123;1, “Neck”&#125;,</span><br><span class="line">&#123;2, “RShoulder”&#125;,</span><br><span class="line">&#123;3, “RElbow”&#125;,</span><br><span class="line">&#123;4, “RWrist”&#125;,</span><br><span class="line">&#123;5, “LShoulder”&#125;,</span><br><span class="line">&#123;6, “LElbow”&#125;,</span><br><span class="line">&#123;7, “LWrist”&#125;,</span><br><span class="line">&#123;8, “MidHip”&#125;,</span><br><span class="line">&#123;9, “RHip”&#125;,</span><br><span class="line">&#123;10, “RKnee”&#125;,</span><br><span class="line">&#123;11, “RAnkle”&#125;,</span><br><span class="line">&#123;12, “LHip”&#125;,</span><br><span class="line">&#123;13, “LKnee”&#125;,</span><br><span class="line">&#123;14, “LAnkle”&#125;,</span><br><span class="line">&#123;15, “REye”&#125;,</span><br><span class="line">&#123;16, “LEye”&#125;,</span><br><span class="line">&#123;17, “REar”&#125;,</span><br><span class="line">&#123;18, “LEar”&#125;,</span><br><span class="line">&#123;19, “LBigToe”&#125;,</span><br><span class="line">&#123;20, “LSmallToe”&#125;,</span><br><span class="line">&#123;21, “LHeel”&#125;,</span><br><span class="line">&#123;22, “RBigToe”&#125;,</span><br><span class="line">&#123;23, “RSmallToe”&#125;,</span><br><span class="line">&#123;24, “RHeel”&#125;</span><br><span class="line">修改之后</span><br><span class="line">&#123;0, “Nose”&#125;,</span><br><span class="line">&#123;1, “Neck”&#125;,</span><br><span class="line">&#123;2, “RShoulder”&#125;,</span><br><span class="line">&#123;3, “RElbow”&#125;,</span><br><span class="line">&#123;4, “RWrist”&#125;,</span><br><span class="line">&#123;5, “LShoulder”&#125;,</span><br><span class="line">&#123;6, “LElbow”&#125;,</span><br><span class="line">&#123;7, “LWrist”&#125;,</span><br><span class="line">&#123;8, “MidHip”&#125;,</span><br><span class="line">&#123;9, “RHip”&#125;,</span><br><span class="line">&#123;10, “RKnee”&#125;,</span><br><span class="line">&#123;11, “RAnkle”&#125;,</span><br><span class="line">&#123;12, “LHip”&#125;,</span><br><span class="line">&#123;13, “LKnee”&#125;,</span><br><span class="line">&#123;14, “LAnkle”&#125;,</span><br><span class="line">&#123;15, “REye”&#125;,</span><br><span class="line">&#123;16, “LEye”&#125;,</span><br><span class="line">&#123;17, “REar”&#125;,</span><br><span class="line">&#123;18, “LEar”&#125;,</span><br><span class="line">&#123;19, “LBigToe”&#125;,</span><br><span class="line">&#123;20, “Loin”&#125;,</span><br><span class="line">&#123;21, “LHand”&#125;,</span><br><span class="line">&#123;22, “RBigToe”&#125;,</span><br><span class="line">&#123;23, “PlaceH”&#125;,</span><br><span class="line">&#123;24, “RHand”&#125;</span><br></pre></td></tr></table></figure><h3 id="openpose17">Openpose17</h3><p><img src="IMG-20250828223734511.png" alt=""></p><h3 id="coco17">COCO17</h3><p><img src="IMG-20250828223734531.png" alt=""></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#x27;joint_names&#x27;: [</span><br><span class="line">    &#x27;MidHip&#x27;,            # 0</span><br><span class="line">    &#x27;LUpLeg&#x27;,       # 1</span><br><span class="line">    &#x27;RUpLeg&#x27;,      # 2</span><br><span class="line">    &#x27;spine&#x27;,           # 3</span><br><span class="line">    &#x27;LLeg&#x27;,         # 4</span><br><span class="line">    &#x27;RLeg&#x27;,        # 5</span><br><span class="line">    &#x27;spine1&#x27;,          # 6</span><br><span class="line">    &#x27;LFoot&#x27;,        # 7</span><br><span class="line">    &#x27;RFoot&#x27;,       # 8</span><br><span class="line">    &#x27;spine2&#x27;,          # 9</span><br><span class="line">    &#x27;LToeBase&#x27;,     # 10</span><br><span class="line">    &#x27;RToeBase&#x27;,    # 11</span><br><span class="line">    &#x27;neck&#x27;,            # 12</span><br><span class="line">    &#x27;LShoulder&#x27;,    # 13</span><br><span class="line">    &#x27;RShoulder&#x27;,   # 14</span><br><span class="line">    &#x27;head&#x27;,            # 15</span><br><span class="line">    &#x27;LArm&#x27;,         # 16</span><br><span class="line">    &#x27;RArm&#x27;,        # 17</span><br><span class="line">    &#x27;LForeArm&#x27;,     # 18</span><br><span class="line">    &#x27;RForeArm&#x27;,    # 19</span><br><span class="line">    &#x27;LHand&#x27;,        # 20</span><br><span class="line">    &#x27;RHand&#x27;,       # 21</span><br><span class="line">    &#x27;LHandIndex1&#x27;,  # 22</span><br><span class="line">    &#x27;RHandIndex1&#x27;, # 23</span><br><span class="line">]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># Alphapose全22个点转化到Openpose的关系</span><br><span class="line"># 定义映射关系</span><br><span class="line">POSE_KEYPOINTS_MAPPING = &#123;</span><br><span class="line">    0: 0,  # 鼻子 -&gt; pose_keypoints_2d[0]</span><br><span class="line">    1: 16,  # 左眼 -&gt; pose_keypoints_2d[16]</span><br><span class="line">    2: 15,  # 右眼 -&gt; pose_keypoints_2d[15]</span><br><span class="line">    3: 18,  # 左耳 -&gt; pose_keypoints_2d[18]</span><br><span class="line">    4: 17,  # 右耳 -&gt; pose_keypoints_2d[17]</span><br><span class="line">    5: 5,   # 左肩膀 -&gt; pose_keypoints_2d[5]</span><br><span class="line">    6: 2,   # 右肩膀 -&gt; pose_keypoints_2d[2]</span><br><span class="line">    7: 6,   # 左胳膊肘 -&gt; pose_keypoints_2d[6]</span><br><span class="line">    8: 3,   # 右胳膊肘 -&gt; pose_keypoints_2d[3]</span><br><span class="line">    9: 7,   # 左手腕 -&gt; pose_keypoints_2d[7]</span><br><span class="line">    10: 4,  # 右手腕 -&gt; pose_keypoints_2d[4]</span><br><span class="line">    11: 12, # 左髋 -&gt; pose_keypoints_2d[12]</span><br><span class="line">    12: 9,  # 右髋 -&gt; pose_keypoints_2d[9]</span><br><span class="line">    13: 13, # 左膝盖 -&gt; pose_keypoints_2d[13]</span><br><span class="line">    14: 10, # 右膝盖 -&gt; pose_keypoints_2d[10]</span><br><span class="line">    15: 14, # 左脚踝 -&gt; pose_keypoints_2d[14]</span><br><span class="line">    16: 11, # 右脚踝 -&gt; pose_keypoints_2d[11]</span><br><span class="line">    17: 19, # 左脚尖 -&gt; pose_keypoints_2d[19]</span><br><span class="line">    18: 22,  # 右脚尖 -&gt; pose_keypoints_2d[22]</span><br><span class="line"></span><br><span class="line">    19: 21,  # 左手手背 -&gt; pose_keypoints_2d[21]把左手手背映射到Openpose中的左后跟</span><br><span class="line">    20: 24,  # 右手手背 -&gt; pose_keypoints_2d[21]把右手手背映射到Openpose中的右后跟</span><br><span class="line">    21: 20,  # 腰部 -&gt; pose_keypoints_2d[21]把腰部映射到Openpose中的左脚尖的脚尖</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 整理前的数据</span><br><span class="line">data</span><br><span class="line">└── openpose</span><br><span class="line">|   ├── 1</span><br><span class="line">|          ├── 000000_keypoints.json</span><br><span class="line">|       ├── 000001_keypoints.json #按照每一帧的结果</span><br><span class="line">|   ├── 2</span><br><span class="line">|   ├── 3</span><br><span class="line">|   ├── 4</span><br><span class="line">└── videos</span><br><span class="line">|    ├── 1</span><br><span class="line">|    ├── 2</span><br><span class="line">|    ├── 3</span><br><span class="line">|    ├── 4</span><br><span class="line">└── extri.yml</span><br><span class="line">└── intri.yml   </span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 处理数据，把openpose的结果再次处理，改成75点的一个格式，放在annots</span><br><span class="line"># 查找视频文件（.mp4），然后调用 extract_video() 函数，从视频中按照用户给定的起始帧、终止帧和间隔提取图像帧，并存储在指定的 images 子目录下。</span><br><span class="line"># 必选参数：</span><br><span class="line">    # path：数据所在路径。该目录下需包括 videos 目录（视频文件）以及用于存放生成图片、标注文件的子目录。</span><br><span class="line"># 模式选择：</span><br><span class="line">    # --mode：选择姿态提取模式，可选值 openpose（默认）和 yolo-hrnet。</span><br><span class="line"># 图片和标注设置：</span><br><span class="line">    # --ext：图像文件扩展名，默认 jpg。</span><br><span class="line">    # --annot：存放生成的标注文件的子文件夹名称，默认 annots。</span><br><span class="line"># 视频帧控制：</span><br><span class="line">    # --start：提取起始帧，默认 0。</span><br><span class="line">    # --end：提取终止帧（不包含该帧），默认 10000。</span><br><span class="line">    # --step：帧间隔，即每隔多少帧提取一帧。</span><br><span class="line"># 姿态提取细节：</span><br><span class="line">    # --highres：控制 OpenPose 网络输入的高分辨率比例，默认 1。</span><br><span class="line">    # --handface：开启后，同时提取手部和脸部关键点。</span><br><span class="line">    # --render：如果指定，会将 OpenPose 生成的渲染图像输出到对应文件夹。</span><br><span class="line">    # --no2d：如果指定，则仅执行图像帧提取，不进行 2D 关键点检测。如果为false则会使用openpose或者HRNet进行特征点提取</span><br><span class="line">    # --openpose：指定 OpenPose 的安装路径，默认 /media/qing/Project/openpose。</span><br><span class="line"># yolo-hrnet 模式的设置：</span><br><span class="line">    # --low：使用较低的人体检测阈值（对应 config_low 设置），适用于对小目标或难检测的场景。</span><br><span class="line">    # --gtbbox：使用真实的边界框信息，同时使用 HRNet 来估计人体姿态。</span><br><span class="line"># 调试相关：</span><br><span class="line">    # --debug：开启调试模式，可能打印更多调试信息。</span><br><span class="line">    # --path_origin：原始路径，默认当前工作目录。</span><br><span class="line"></span><br><span class="line">python apps/demo/mv1p.py /home/outbreak/HPE/GUIplatform/reWriteTools0610/multiView/multireconstruction --out /home/outbreak/HPE/GUIplatform/reWriteTools0610/multiView/multireconstruction/output --vis_det --vis_repro --undis --vis_smpl --sub_vis  5 9 13 --body body25 --model smpl</span><br><span class="line"></span><br><span class="line">python3 scripts/preprocess/extract_video.py $&#123;data&#125; --handface</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 整理完的数据</span><br><span class="line">data</span><br><span class="line">└── openpose</span><br><span class="line">|   ├── 1</span><br><span class="line">|          ├── 000000_keypoints.json</span><br><span class="line">|       ├── 000001_keypoints.json #按照每一帧的结果</span><br><span class="line">|   ├── 2</span><br><span class="line">|   ├── 3</span><br><span class="line">|   ├── 4</span><br><span class="line">└── videos</span><br><span class="line">|    ├── 1</span><br><span class="line">|    ├── 2</span><br><span class="line">|    ├── 3</span><br><span class="line">|    ├── 4</span><br><span class="line">└── annots</span><br><span class="line">|   ├── 1</span><br><span class="line">|          ├── 000000.json</span><br><span class="line">|       ├── 000001.json #按照每一帧的结果</span><br><span class="line">|   ├── 2</span><br><span class="line">|   ├── 3</span><br><span class="line">|   ├── 4</span><br><span class="line">└── images</span><br><span class="line">|    ├── 1</span><br><span class="line">|          ├── 000000.jpg</span><br><span class="line">|       ├── 000001.jpg #按照每一帧的结果</span><br><span class="line">|    ├── 2</span><br><span class="line">|    ├── 3</span><br><span class="line">|    ├── 4</span><br><span class="line">└── extri.yml</span><br><span class="line">└── intri.yml   </span><br></pre></td></tr></table></figure><h2 id="重建">重建</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># --path</span><br><span class="line"># 输入数据根目录，里面通常包含图片或视频以及标注信息。</span><br><span class="line"># --sub_vis</span><br><span class="line"># 多视角相机的名称或编号列表，用于指定参与重建的摄像头。</span><br><span class="line"># --out</span><br><span class="line"># 输出结果的目录，生成的 3D 骨骼、SMPL 参数、重投影图等数据都会存放于此。</span><br><span class="line"># --annot</span><br><span class="line"># 指定标注文件的子目录名，一般存放2D检测的关键点信息。</span><br><span class="line"># --model</span><br><span class="line"># 选择使用的 SMPL 模型类型，如 SMPL、SMPL+H 等，不同模型可能会影响重建精度。</span><br><span class="line"># --gender</span><br><span class="line"># 指定人体性别（例如 &quot;male&quot; 或 &quot;female&quot;），以加载对应的 SMPL 模型参数。</span><br><span class="line"># --body</span><br><span class="line"># 指定骨架类型或人体结构，可能影响关键点的排列和模型匹配。</span><br><span class="line"># --skel</span><br><span class="line"># 如果指定该参数，则强制重新计算 3D 骨架数据（即通过三角化获得关键点3D坐标），否则可以直接读取已有的骨骼数据。</span><br><span class="line"># --start / --end</span><br><span class="line"># 指定处理数据的起始帧和终止帧，用于限定处理区间，防止处理整个序列。</span><br><span class="line"># --thres2d</span><br><span class="line"># 用于 2D 关键点的置信度阈值，低于该阈值的点会被忽略，以保证3D重建的稳定性。</span><br><span class="line"># --MAX_REPRO_ERROR</span><br><span class="line"># 设置最大允许的重投影误差。如果重投影误差超过此值，对应的2D关键点会被置零，并重新进行三角化。</span><br><span class="line"># --smooth3d</span><br><span class="line"># 对重建得到的3D骨架数据进行平滑处理的系数，数值越大平滑效果越明显。</span><br><span class="line"># --vis_det</span><br><span class="line"># 打开2D检测结果的可视化（如关键点和边界框显示），便于检查检测效果。</span><br><span class="line"># --vis_repro</span><br><span class="line"># 打开重投影结果可视化，显示由3D重建后经过相机投影到2D平面的关键点，方便判断重建质量。</span><br><span class="line"># --vis_smpl</span><br><span class="line"># 可视化 SMPL 模型重建的结果，将 SMPL 生成的顶点或肢体覆盖在原始图像上。</span><br><span class="line"># --write_smpl_full</span><br><span class="line"># 如果指定，则除了输出基本的 SMPL 参数外，还会保存完整的姿态参数（如全局pose转换等）。</span><br><span class="line"># --write_vertices</span><br><span class="line"># 指定后会将 SMPL 模型生成的顶点信息写入文件，供后续处理或3D渲染使用。</span><br><span class="line"># --undis</span><br><span class="line"># 指定是否对输入图像进行去畸变处理（Undistortion），以提高重建精度。</span><br><span class="line"># --save_origin</span><br><span class="line"># 控制是否保存原始骨骼数据，便于对比和验证重建前后的效果。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">python3 apps/demo/mv1p.py $&#123;data&#125; --out $&#123;data&#125;/output/smpl </span><br><span class="line">--vis_det</span><br><span class="line">--vis_repro</span><br><span class="line">--undis</span><br><span class="line">--sub_vis 1 7 13 19 </span><br><span class="line">--vis_smpl</span><br></pre></td></tr></table></figure><p>编写一个python文件，输入一个类似alphapose-result2_merged.json的json文件，在json文件目录下建立同名的文件夹，然后基于json文件中的多帧信息，创建每个帧的骨架信息，按照000000_keypoints.json的openpose的格式，具体而言：把alphapose的json文件中每一帧都创建一个json文件，然后json文件中keypoints本是22个骨架关节关键点，映射为openpose的pose_keypoints_2d和hand_left_keypoints_2d和hand_right_keypoints_2d，在单帧的映射过程中，<br>alphapose的第0个点鼻子对应openpose中pose_keypoints_2d的0点<br>alphapose的第1个点左眼对应openpose中pose_keypoints_2d的16点<br>alphapose的第2个点右眼对应openpose中pose_keypoints_2d的15点<br>alphapose的第3个点左耳朵对应openpose中pose_keypoints_2d的18点<br>alphapose的第4个点右耳朵对应openpose中pose_keypoints_2d的17点<br>alphapose的第5个点左肩膀对应openpose中pose_keypoints_2d的5点</p><p>alphapose的第6个点右肩膀对应openpose中pose_keypoints_2d的2点</p><p>alphapose的第7个点左胳膊肘对应openpose中pose_keypoints_2d的6点</p><p>alphapose的第8个点右胳膊肘对应openpose中pose_keypoints_2d的3点</p><p>alphapose的第9个点左手腕对应openpose中pose_keypoints_2d的7点</p><p>alphapose的第10个点右手腕对应openpose中pose_keypoints_2d的4点</p><p>alphapose的第11个点左髋关节对应openpose中pose_keypoints_2d的12点</p><p>alphapose的第12个点右髋关节应openpose中pose_keypoints_2d的9点</p><p>alphapose的第13个点左去去应openpose中pose_keypoints_2d的13点</p><p>alphapose的第14个点右膝盖对应openpose中pose_keypoints_2d的10点</p><p>alphapose的第15个点左脚踝对应openpose中pose_keypoints_2d的14点</p><p>alphapose的第16个点右脚踝对应openpose中pose_keypoints_2d的11点</p><p>alphapose的第17个点左脚尖对应openpose中pose_keypoints_2d的20点</p><p>alphapose的第18个点右脚尖对应openpose中pose_keypoints_2d的23点</p><p>至于openpose中pose_keypoints_2d的1点中胸位置，取alphapose的第5个点左肩膀和第6个点右肩膀的中点</p><p>至于openpose中pose_keypoints_2d的8点腹部位置，取alphapose的第11个点左髋和第12个点右髋的中点<br>然后把<br>alphapose的第9个点左手腕对应openpose中hand_left_keypoints_2d的0点</p><p>alphapose的第10个点右手腕对应openpose中hand_right_keypoints_2d的0点</p><p>alphapose的第19个点左手手背对应openpose中hand_left_keypoints_2d的9点</p><p>alphapose的第20个点右手手背对应openpose中hand_right_keypoints_2d的9点</p><p>alphapose的第21个点腰部，目前没有放在openpose中</p><p>其余的没有映射的点都是0 0 0</p><p>用python写一个软件，输入一个这样的一个json文件和想要输出相机camera的id，id有多个，没输入id则默认全部相机参数都输出<br>将其中的相机参数导出为extri.yml和intri.yml，这两个yml导出形式是opencv标定的格式，需要你把json文件中指定id相机参数根据Transform的平移矩阵和旋转矩阵参数输出到extri.yml中，把Instrinsic中的相机内参，根据json文件中的名称整理为opencv的相机内参格式，输出到intri.yml，并且两个yml的输出名称为json的名称_extri或者名称_intri</p><p>用python写一个软件，输入一个这样的一个json文件和想要输出相机camera的id，id有多个，没输入id则默认全部相机参数都输出<br>将其中的相机参数导出为extri.yml，导出形式是opencv标定的格式，需要你把json文件中指定id相机参数根据Transform的平移矩阵和旋转矩阵参数输出到extri.yml中输出名称为json的名称_extri</p><h1>报错整理</h1><h2 id="几个帧在一起少同几个关节不会报错">几个帧在一起少同几个关节不会报错</h2><h2 id="单独一帧少几个关节不会报错">单独一帧少几个关节不会报错</h2><h2 id="annot不符合规范">ANNOT不符合规范</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;scripts/preprocess/extract_video.py&quot;, line 285, in &lt;module&gt;</span><br><span class="line">    join(args.path, &#x27;openpose_render&#x27;, sub), args)</span><br><span class="line">  File &quot;scripts/preprocess/extract_video.py&quot;, line 65, in extract_2d</span><br><span class="line">    os.chdir(openpose)</span><br><span class="line">FileNotFoundError: [Errno 2] No such file or directory: &#x27;/media/qing/Project/openpose&#x27;</span><br><span class="line"></span><br><span class="line">把extract_2d改成这样</span><br><span class="line">ef extract_2d(openpose, image, keypoints, render, args):</span><br><span class="line">    skip = False</span><br><span class="line">    if os.path.exists(keypoints):</span><br><span class="line">        print(&#x27;&gt;&gt; exists &#123;&#125;&#x27;.format(keypoints))</span><br><span class="line">        # check the number of images and keypoints</span><br><span class="line">        print(&#x27;image&#x27;, image)</span><br><span class="line">        print(&#x27;keypoints&#x27;, keypoints)</span><br><span class="line">        print(&#x27;len(image)&#x27;, len(os.listdir(image)))</span><br><span class="line">        print(&#x27;len(keypoints)&#x27;, len(os.listdir(keypoints)))</span><br><span class="line">        skip = True</span><br><span class="line">        # if len(os.listdir(image)) == len(os.listdir(keypoints)):</span><br><span class="line">        #     skip = True</span><br><span class="line">    if not skip:</span><br></pre></td></tr></table></figure><h3 id="如果annots少一个帧的json文件-这里annots-2少一个第20-json">如果annots少一个帧的json文件，这里annots/2少一个第20.json</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"> python apps/demo/mv1p.py ../zju-ls-feng_TestData --out  ../zju-ls-feng_TestData/outputTest2loss20annots --vis_det --vis_repro --undis --sub_vis 2 5 9 --vis_smpl --start 20 --end 25</span><br><span class="line"></span><br><span class="line">  Demo code for multiple views and one person:</span><br><span class="line"></span><br><span class="line">    - Input : ../zju-ls-feng_TestData =&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23</span><br><span class="line">    - Output: ../zju-ls-feng_TestData/outputTest2loss20annots</span><br><span class="line">    - Body  : smpl=&gt;neutral, body25</span><br><span class="line"></span><br><span class="line">这里是我自己改的，要不会搜索所有图像文件夹然后load：args.sub [&#x27;2&#x27;, &#x27;5&#x27;, &#x27;9&#x27;]</span><br><span class="line">triangulation:   0%|                                                                                                                                                                          | 0/5 [00:00&lt;?, ?it/s]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 118, in &lt;module&gt;</span><br><span class="line">    mv1pmf_skel(dataset, check_repro=True, args=args)</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 35, in mv1pmf_skel</span><br><span class="line">    images, annots = dataset[nf]</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/dataset/mv1pmf.py&quot;, line 72, in __getitem__</span><br><span class="line">    images, annots_all = super().__getitem__(index)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/dataset/base.py&quot;, line 484, in __getitem__</span><br><span class="line">    assert self.imagelist[cam][index].split(&#x27;.&#x27;)[0] == self.annotlist[cam][index].split(&#x27;.&#x27;)[0]</span><br><span class="line">AssertionError</span><br></pre></td></tr></table></figure><h2 id="如果annots多一个帧的json文件-这里annots-2多一个20-copy-json">如果annots多一个帧的json文件，这里annots/2多一个20_copy.json</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"> python apps/demo/mv1p.py ../zju-ls-feng_TestData --out  ../zju-ls-feng_TestData/outputTest2loss20annots --vis_det --vis_repro --undis --sub_vis 2 5 9 --vis_smpl --start 20 --end 25</span><br><span class="line"></span><br><span class="line">  Demo code for multiple views and one person:</span><br><span class="line"></span><br><span class="line">    - Input : ../zju-ls-feng_TestData =&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23</span><br><span class="line">    - Output: ../zju-ls-feng_TestData/outputTest2loss20annots</span><br><span class="line">    - Body  : smpl=&gt;neutral, body25</span><br><span class="line"></span><br><span class="line">这里是我自己改的，要不会搜索所有图像文件夹然后load：args.sub [&#x27;2&#x27;, &#x27;5&#x27;, &#x27;9&#x27;]</span><br><span class="line">triangulation:   0%|                                                                                                                                                                          | 0/5 [00:00&lt;?, ?it/s]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 118, in &lt;module&gt;</span><br><span class="line">    mv1pmf_skel(dataset, check_repro=True, args=args)</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 35, in mv1pmf_skel</span><br><span class="line">    images, annots = dataset[nf]</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/dataset/mv1pmf.py&quot;, line 72, in __getitem__</span><br><span class="line">    images, annots_all = super().__getitem__(index)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/dataset/base.py&quot;, line 484, in __getitem__</span><br><span class="line">    assert self.imagelist[cam][index].split(&#x27;.&#x27;)[0] == self.annotlist[cam][index].split(&#x27;.&#x27;)[0]</span><br><span class="line">AssertionError</span><br></pre></td></tr></table></figure><p>如果多的或者少的是start/end之外的帧则不会报错</p><h2 id="如果start-end包含的帧太少-至少三帧">如果start/end包含的帧太少，至少三帧</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">python apps/demo/mv1p.py ../zju-ls-feng_TestData --out  ../zju-ls-feng_TestData/outputTest2loss20annots --vis_det --vis_repro --undis --sub_vis 2 5 9 --vis_smpl --start 20 --end 22</span><br><span class="line"></span><br><span class="line">  Demo code for multiple views and one person:</span><br><span class="line"></span><br><span class="line">    - Input : ../zju-ls-feng_TestData =&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23</span><br><span class="line">    - Output: ../zju-ls-feng_TestData/outputTest2loss20annots</span><br><span class="line">    - Body  : smpl=&gt;neutral, body25</span><br><span class="line"></span><br><span class="line">这里是我自己改的，要不会搜索所有图像文件夹然后load：args.sub [&#x27;2&#x27;, &#x27;5&#x27;, &#x27;9&#x27;]</span><br><span class="line">triangulation: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 21.25it/s]</span><br><span class="line">dump: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 7536.93it/s]</span><br><span class="line">loading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00&lt;00:00, 2523.65it/s]</span><br><span class="line">/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/lbfgs.py:264: UserWarning: This overload of add_ is deprecated:</span><br><span class="line">        add_(Number alpha, Tensor other)</span><br><span class="line">Consider using one of the following signatures instead:</span><br><span class="line">        add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)</span><br><span class="line">  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))</span><br><span class="line">-&gt; [Optimize global RT  ]:   3.9ms</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 119, in &lt;module&gt;</span><br><span class="line">    mv1pmf_smpl(dataset, args)</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 71, in mv1pmf_smpl</span><br><span class="line">    weight_shape=weight_shape, weight_pose=weight_pose)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 77, in smpl_from_keypoints3d2d</span><br><span class="line">    params = multi_stage_optimize(body_model, params, kp3ds, kp2ds, bboxes, Pall, weight_pose, cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 18, in multi_stage_optimize</span><br><span class="line">    params = optimizePose3D(body_model, params, kp3ds, weight=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 301, in optimizePose3D</span><br><span class="line">    params = _optimizeSMPL(body_model, params, prepare_funcs, postprocess_funcs, loss_funcs, weight_loss=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 246, in _optimizeSMPL</span><br><span class="line">    final_loss = fitting.run_fitting(optimizer, closure, opt_params)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize.py&quot;, line 38, in run_fitting</span><br><span class="line">    loss = optimizer.step(closure)</span><br><span class="line">  File &quot;/home/outbreak/anaconda3/envs/easymocap/lib/python3.7/site-packages/torch/optim/optimizer.py&quot;, line 113, in wrapper</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/lbfgs.py&quot;, line 422, in step</span><br><span class="line">    obj_func, x_init, t, d, loss, flat_grad, gtd)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/lbfgs.py&quot;, line 177, in _strong_wolfe</span><br><span class="line">    t = bracket[low_pos]</span><br><span class="line">IndexError: list index out of range</span><br></pre></td></tr></table></figure><h2 id="不知道怎么错的">不知道怎么错的</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">python apps/demo/mv1p.py ../dataOpenpose202403270042 --out  ../dataOpenpose202403270042/output --vis_det --vis_repro --undis --sub_vis 5 13  --vis_smpl --start 0 --end 5</span><br><span class="line"></span><br><span class="line">  Demo code for multiple views and one person:</span><br><span class="line"></span><br><span class="line">    - Input : ../dataOpenpose202403270042 =&gt; 2, 5, 9, 13</span><br><span class="line">    - Output: ../dataOpenpose202403270042/output</span><br><span class="line">    - Body  : smpl=&gt;neutral, body25</span><br><span class="line"></span><br><span class="line">这里是我自己改的，要不会搜索所有图像文件夹然后load：args.sub [&#x27;5&#x27;, &#x27;13&#x27;]</span><br><span class="line">loading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00&lt;00:00, 39125.97it/s]</span><br><span class="line">-&gt; [Optimize global RT  ]:   8.7ms</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 119, in &lt;module&gt;</span><br><span class="line">    mv1pmf_smpl(dataset, args)</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 71, in mv1pmf_smpl</span><br><span class="line">    weight_shape=weight_shape, weight_pose=weight_pose)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 77, in smpl_from_keypoints3d2d</span><br><span class="line">    params = multi_stage_optimize(body_model, params, kp3ds, kp2ds, bboxes, Pall, weight_pose, cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 18, in multi_stage_optimize</span><br><span class="line">    params = optimizePose3D(body_model, params, kp3ds, weight=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 301, in optimizePose3D</span><br><span class="line">    params = _optimizeSMPL(body_model, params, prepare_funcs, postprocess_funcs, loss_funcs, weight_loss=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 246, in _optimizeSMPL</span><br><span class="line">    final_loss = fitting.run_fitting(optimizer, closure, opt_params)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize.py&quot;, line 38, in run_fitting</span><br><span class="line">    loss = optimizer.step(closure)</span><br><span class="line">  File &quot;/home/outbreak/anaconda3/envs/easymocap/lib/python3.7/site-packages/torch/optim/optimizer.py&quot;, line 113, in wrapper</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/lbfgs.py&quot;, line 307, in step</span><br><span class="line">    orig_loss = closure()</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 227, in closure</span><br><span class="line">    new_params = func(new_params)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 121, in interp_func</span><br><span class="line">    params[key][nf] = interp(params[key][left], params[key][right], 1-weight, key=key)</span><br><span class="line">IndexError: index 5 is out of bounds for dimension 0 with size 5</span><br></pre></td></tr></table></figure><p>改了例程的intri.yml，也报错</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 119, in &lt;module&gt;</span><br><span class="line">    mv1pmf_smpl(dataset, args)</span><br><span class="line">  File &quot;apps/demo/mv1p.py&quot;, line 71, in mv1pmf_smpl</span><br><span class="line">    weight_shape=weight_shape, weight_pose=weight_pose)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 77, in smpl_from_keypoints3d2d</span><br><span class="line">    params = multi_stage_optimize(body_model, params, kp3ds, kp2ds, bboxes, Pall, weight_pose, cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pipeline/basic.py&quot;, line 18, in multi_stage_optimize</span><br><span class="line">    params = optimizePose3D(body_model, params, kp3ds, weight=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 301, in optimizePose3D</span><br><span class="line">    params = _optimizeSMPL(body_model, params, prepare_funcs, postprocess_funcs, loss_funcs, weight_loss=weight, cfg=cfg)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 246, in _optimizeSMPL</span><br><span class="line">    final_loss = fitting.run_fitting(optimizer, closure, opt_params)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize.py&quot;, line 38, in run_fitting</span><br><span class="line">    loss = optimizer.step(closure)</span><br><span class="line">  File &quot;/home/outbreak/anaconda3/envs/easymocap/lib/python3.7/site-packages/torch/optim/optimizer.py&quot;, line 113, in wrapper</span><br><span class="line">    return func(*args, **kwargs)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/lbfgs.py&quot;, line 307, in step</span><br><span class="line">    orig_loss = closure()</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 227, in closure</span><br><span class="line">    new_params = func(new_params)</span><br><span class="line">  File &quot;/home/outbreak/HPE/easyMocap/EasyMocap/easymocap/pyfitting/optimize_simple.py&quot;, line 121, in interp_func</span><br><span class="line">    params[key][nf] = interp(params[key][left], params[key][right], 1-weight, key=key)</span><br><span class="line">IndexError: index 3 is out of bounds for dimension 0 with size 3</span><br></pre></td></tr></table></figure><p>另外如果output里面有keypoint3d文件，那么运行的时候会优先读取3d文件，如果上一次3d文件就是错的，那还是会报这个错</p><h2 id="可视化一个smpl模型">可视化一个smpl模型</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python apps/vis3d/vis_smpl.py --param_path /home/outbreak/HPE/easyMocap/dataOpenpose202403270042/outputFrom3d/smpl/000020.json</span><br></pre></td></tr></table></figure><h2 id="测试我的插值">测试我的插值</h2><p>起始帧</p><p><img src="IMG-20250828223734548.png" alt=""></p><p><img src="IMG-20250828223734567.png" alt=""></p><p><img src="IMG-20250828223734582.png" alt=""><br>结束帧</p><p><img src="IMG-20250828223734602.png" alt=""></p><p><img src="IMG-20250828223734618.png" alt=""></p><p><img src="IMG-20250828223734634.png" alt=""><br>插值10/10帧</p><p><img src="IMG-20250828223734677.png" alt=""></p><p><img src="IMG-20250828223734650.png" alt=""></p><p><img src="IMG-20250828223734664.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;eastMocap Install and Usage&lt;/h1&gt;
&lt;h1&gt;easymocap关键点检测模块可以用HRNet或者Openpose&lt;/h1&gt;
&lt;h1&gt;每次改完代码要python &lt;a href=&quot;http://setup.py&quot;&gt;setup.py&lt;/a&gt; de</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_ImageSculpting</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ImageSculpting/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ImageSculpting/</id>
    <published>2025-12-03T05:57:16.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<h1>Image Sculpting: Precise Object Editing with 3D Geometry Control</h1><p>通过结合3D几何和图形工具来编辑2D图像。Image Sculpting 将 2D 对象转换为 3D，直接在3D空间中编辑目标，然后高保真的渲染为2D图像。</p><p>Prob.Def</p><p>input: one image,manipulation of the objects and their orientations in 3D<br>space</p><p>output:a high-quality edited 2D image</p><p>背景和相关工作</p><ol><li><p>生成图像编辑：现有方法仅限于 2D 空间，通常依赖于文本指令：Prompt-to-Prompt [24]、Plug-and-Play [76]、InstructPix2Pix [10]、Imagic [34] 和 Object 3DIT [47] 然而，更具体的指令，如“将物体抬起 5 厘米并旋转 42 度”，不太可能成功，因为当前的生成模型无法仅通过文本提示来满足此类详细请求。</p></li><li><p>基于 2D 的交互式生成图像编辑方法，ControlNet [87] 包含额外的条件输入，例如深度 [61]、姿态 [11] 和边缘 [83]，用于可控生成。为了实现更直观的交互，DragGAN [54] 使用户能够使用 GAN 在对象上拖动控制点，并且类似的技术已应用于扩散模型 [39， 70]。然而，这些方法大多局限于 2D，并且在需要更复杂的平面外转换的任务中面临挑战。EG3D [12] 和 StyleNeRF [21] 等 3D 感知生成模型已经探索了这一方向。OBJect-3DIT [47] 是我们论文的基线，它研究了使用语言指令的 3D 感知编辑。如 DragGAN [54]、FreeDrag [39] 和 DragDiffusion [70]，展示了通过潜在空间中的过渡来改变对象部分的能力。尽管如此，它们也有其局限性：1） 它们可以完成基本的变形，但结果并不完全可预测，通常会导致结果与用户的意图不一致;2） 这些潜在变换在 2D 特征空间内运行这本身就限制了他们表示 3D 变换和准确处理遮挡的能力;3） 他们缺乏物理感知，这使得纳入外部约束（如骨骼结构）变得复杂。</p></li><li><p>形状变形：</p><ol><li><p>空间变形 较早且仍然广泛使用的方法是将体积翘曲函数 f ： R→ R 应用于 3D 域的所有点 [66]。这种方法可以应用于显式（三角形或多边形网格）或隐式表示。可以使用晶格 [66]、笼子上的顶点 [31] 或神经场 [17] 来参数化该映射。这些方法的局限性是它们不知道物体的形状，这使得它们在复杂的铰接物体上使用更具挑战性 [9]</p></li><li><p>形状感知变形 形状感知变形提供了一组链接到对象曲面的控件。在计算机辅助设计 （CAD） 中，一小部分控制点使用样条补丁定义光滑表面 [18]。尽管具有灵活性和质量，但从 3D 模型或 NeRF 中提取样条补丁是一个具有挑战性和开放性的问题 [7]。基于偏微分方程 （PDE） 的方法模拟物体的变形，将其表示为体积可变形的固体 [72] 或薄橡胶壳 [75]。通过移动表面上选择的手柄 [8] 来施加引导变形的力，使其使用直观，并且需要最少的用户交互。</p></li><li><p>线性混合蒙皮最流行的变形方法是线性混合蒙皮 [30]，它将空间变形函数定义为一组由形状感知标量函数加权的仿射变换的混合平均值，通常使用基于表面上的 PDE 解 [29] 或手动编辑的方法计算。这种方法提供了完全的控制和灵活性，因为仿射变换可以附加到点、笼的顶点或骨架中的线段 [6]。</p></li><li><p>我们的方法：我们可以<strong>使用这些算法中的任何一种来精确控制形状变形</strong>，从而精确控制渲染的图像。</p></li></ol></li><li><p><img src="IMG-20250828221816589.png" alt=""></p></li></ol><h2 id="步骤：">步骤：</h2><p><img src="C:%5CUsers%5Coutbr%5CAppData%5CRoaming%5Cmarktext%5Cimages%5C2025-07-17-13-23-46-image.png" alt=""></p><ol><li><p>首先通过反渲染过程将输入图像转换为纹理化的3D模型。采用了零样本单图像重建模型 （Zero-1-to3 [41]）</p></li><li><p>该模型通过创建骨架和计算蒙皮权重来为交互变形做准备。</p></li><li><p>用户可以通过修改骨架来对模型进行变形，从而得到初始的粗略图像。 such as As-Rigid-As-Possible(ARAP) [75] and linear-based skinning [45]</p></li><li><p>开发了一种从粗到细的增强方法：使用改进的特征注入技术为了精化编辑后的图像，将粗略的渲染Ic转化为噪声xcT。然后将自注意力图Act和特征图fct从初始图像去噪过程中注入到增强图像去噪步骤中。</p></li></ol><h2 id="方法">方法</h2><p><img src="IMG-20250828221817556.png" alt=""></p><ol><li><p>执行 3D 重建以获得其 3D 模型</p><ol><li><p>使用 SAM [35] 从输入图像中分割选定的对象。在此基础上，我们使用分数蒸馏采样 （SDS） [58] 训练 NeRF。</p></li><li><p>NeRF 到 3D 模型我们使用 threestudio [22] 中的实现将 NeRF 体积转换为网格。</p></li></ol></li><li><p>获得 3D 模型后，用户可以<strong>手动构建骨骼</strong>，并<strong>通过旋转骨骼进行交互式实现目标姿势</strong>。网格变形会影响对象的顶点位置，但不会影响用于纹理映射的 UV 坐标;因此此过程会在对象变形后使映射在对象上的纹理变形。</p></li><li><p>从粗到细的生成增强</p><ol><li><p>研究初步阶段：一种方法是使用主题驱动的个性化技术，如 DreamBooth [64]。它们旨在保留输入中的关键细节，但可能会损害编辑的几何体。或者，可以使用 SDEdit [46] 等图像到图像的转换方法来保留编辑后的几何图形，但这可能会破坏与原始图像的纹理一致性。这种二分法在我们的初步研究中很清楚，如图 4 所示。SDEdit 可以保留几何体，但无法准确复制纹理。另一方面，DreamBooth 产生了高保真输出，但难以有效地保留纹理和几何体。</p></li><li><p>我们使用 DreamBooth 在一个输入参考图像上微调扩散模型。为了保持几何结构，我们采用了一种特征和注意力注入技术 [76]，该技术最初是为语义布局控制而设计的。此外，我们通过 ControlNet [87] 整合了来自 3D 模型的深度数据。我们发现这种集成对于最大限度地减少增强过程中的不确定性至关重要。</p><ol><li><p>One-shot Dreambooth：使用一些图像微调预先训练的扩散模型，用于主体驱动的生成。最初的 DreamBooth 论文 [64] 已经表明，它能够利用语义类先验来生成对象的新视图，而只给出主体的几个正面图像。在我们的应用程序中，我们只使用一个示例来训练 DreamBooth，即输入图像。</p></li><li><p>Depth Control使用深度 ControlNet [87] 来保留用户编辑的几何信息。深度图直接从变形的 3D 模型渲染，无需任何单目深度估计。对于背景区域，我们不使用深度图。此深度图用作空间控制信号，指导最终编辑图像中的几何图形生成。</p></li><li><p>Feature Injection仅仅依靠深度控制是不够的——尽管它可以在一定程度上保留几何体，但它仍然难以进行局部、更细致的编辑，例如捕捉南瓜眼睛的特定形状为了更好地保留几何结构，我们使用特征注入。如图 3 所示，此步骤从粗渲染图像的 DDIM 反转 [74]（使用 DreamBooth 微调、深度控制扩散模型）开始，以获得反转的潜在值。在每个去噪步骤中，我们对粗略 任- 的倒置潜伏物进行去噪</p></li></ol></li></ol></li></ol><h2 id="典型生态项目">典型生态项目</h2><ol><li><p>ThreeStudio：提供3D重建和渲染的基础工具，Image Sculpting项目借用了其部分代码。</p></li><li><p>Zero-1-to-3：用于从单张图像生成3D模型的工具，Image Sculpting使用其进行3D重建。但是文章中不是nerf的方法？</p></li><li><p>DreamBooth：用于模型微调的开源项目，帮助Image Sculpting实现纹理细节的捕捉。</p></li></ol><h2 id="none"><img src="IMG-20250828221817992.png" alt=""></h2><h2 id="局限性"><strong>局限性</strong></h2><p>作者提出的</p><ol><li><p>对单视图三维重建质量的依赖。</p></li><li><p>网格变形通常需要一些手动作才能进行模型绑定。</p></li><li><p>输出分辨率不及工业渲染系统，可以结合超分辨率方法。</p></li><li><p>缺少背景灯光的调整，破坏了场景的真实感，可以从集成动态(再)照明技术中获益。</p></li></ol><p>我的问题：</p><p>如何绑定骨架和生成weight，然后重新编辑出一个新的模型：其实是通过一个blender进行编辑的</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Image Sculpting: Precise Object Editing with 3D Geometry Control&lt;/h1&gt;
&lt;p&gt;通过结合3D几何和图形工具来编辑2D图像。Image Sculpting 将 2D 对象转换为 3D，直接在3D空间中编辑目标</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_MAE</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MAE/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MAE/</id>
    <published>2025-12-03T05:57:16.000Z</published>
    <updated>2025-12-03T06:11:20.413Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">An Image is Worth 16x16 Words: Transformers for Image Classification at Scale</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"><strong>Kaiming He</strong></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left"><strong>Google Research</strong></td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2021</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"><strong>ICCV 2021</strong></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left">Masked Autoencoders，cv中的bert</td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>分类</td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><p>在 MAE 之前，视觉自监督学习主要有两类方法：</p><ol><li><strong>对比学习</strong>（Contrastive Learning）如 SimCLR、MoCo：通过“拉近正样本，推远负样本”学习，需要精心设计数据增强和负样本</li><li><strong>生成式建模</strong>如 VAE、iGPT：重建完整图像，计算成本高，关注低级细节（如像素）</li><li>能否像 BERT 一样，用掩码重建（Masked Reconstruction）来学习视觉表征？BERT 成功的关键是：掩码掉 15% 的词。直接套用到图像上效果差，因为图像比文本更“连续”像素级重建太容易。 <strong>所以MAE思想是：重建一整个块，并且大量掩码75% ~ 95%</strong></li></ol><h3 id="创新点">创新点</h3><ol><li>自监督学习，在 ImageNet 等基准上取得了远超之前方法的性能，推动了视觉自监督学习进入“大规模预训练”时代</li><li>高比例随机掩码（High Ratio of Random Masking）只保留 5%~25% 的 patch 作为输入，强迫模型必须从极小的可见信息中推断全局语义</li><li>非对称编解码器结构（Asymmetric Encoder-Decoder）<ol><li>Encoder：只处理可见的 patch（轻量）</li><li>Decoder：接收 encoder 输出 + 掩码 token，重建所有像素</li></ol></li></ol><h3 id="网络架构">网络架构</h3><h4 id="训练阶段">训练阶段</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">输入图像 </span><br><span class="line">│ </span><br><span class="line">├── 分块（Patchify）→ 14x14 = 196 个 patch (16x16) </span><br><span class="line">│ </span><br><span class="line">├── 随机打乱196 个 patch 的顺序</span><br><span class="line">│ </span><br><span class="line">├── 保留前K个作为可见 patch。可见 patch（25%）</span><br><span class="line">│ </span><br><span class="line">├── 可见 patch →线性嵌入768 维 →可学习位置编码 → Encoder → ViT输出K个上下文化的特征向量</span><br><span class="line">│                                                   ↓    </span><br><span class="line">└── 掩码 patch → 随机初始化的 mask token+位置编码 → 拼接回196个特征向量</span><br><span class="line">↓ </span><br><span class="line">所有 token+位置编码</span><br><span class="line">↓</span><br><span class="line">Decoder（轻量 ViT） </span><br><span class="line">↓ </span><br><span class="line">196 个向量</span><br><span class="line">↓ </span><br><span class="line">一个线性层</span><br><span class="line">↓ </span><br><span class="line">映射回 16×16×3 = 768维</span><br><span class="line">↓ </span><br><span class="line">像素级重建</span><br><span class="line">↓ </span><br><span class="line">求MSE loss，只有mask部分</span><br></pre></td></tr></table></figure><h4 id="推理阶段-微调">推理阶段（微调）</h4><p>只保留 encode。在 encoder 输出（如 <code>[CLS]</code> 或全局平均池化）上加分类头。在 ImageNet 等数据集上微调</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_MASt3R-MVS</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MASt3R-MVS/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MASt3R-MVS/</id>
    <published>2025-12-03T05:57:16.000Z</published>
    <updated>2025-12-03T06:11:20.414Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">MUSt3R: Multi-view Network for Stereo 3D Reconstructio</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">NAVER LABS Europe（欧洲Naver实验室）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2025</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td></td></tr><tr><td><strong>所属领域</strong></td><td>一个 3D 点、一个置信度值和一个局部特征。<br></td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>在 DUSt3R 前额外加一个 network 输出稠密的 local features，并添加 matching loss 来训练。最后引入一种快速相互匹配方案，能够将匹配速度提高几个数量级。</li></ol><h3 id="网络架构">网络架构</h3><p><img src="IMG-20250907114911708.png" alt=""><img src="%E9%99%84%E4%BB%B6/Pasted%20image%2020250907114933.png" alt=""><br>如何匹配？</p><h3 id="背景知识">背景知识</h3><p>匹配方法通常被归结为一个三步流程，首先提取稀疏且可重复的关键点，然后用局部不变特征描述它们，最后通过比较它们在特征空间中的距离来配对离散的关键点集。SIFT在 COLMAP等 3D 重建流程中的成功。<br>基于关键点的方法通过将匹配简化为关键点袋（BoK）问题，丢弃了对应任务的全局几何上下文信息。这使得它们在具有重复模式或低纹理区域的情况下特别容易出错，而这些区域实际上对于局部描述符来说是不适定的。解决此问题的一种方法是在配对步骤中引入全局优化策略，通常利用一些关于匹配的先验知识。如SuperGlue。<br>但是，如果关键点及其描述符尚未编码足够的信息，则在匹配过程中利用全局上下文可能为时已晚。因此，另一个方向是考虑密集的整体匹配，即完全避免关键点，并一次匹配整个图像。如LoFTR将图像视为一个整体，并且得到的对应集是密集的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读_MASt3R-SFM</title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MASt3R-SFM/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MASt3R-SFM/</id>
    <published>2025-12-03T05:57:16.000Z</published>
    <updated>2025-12-03T06:11:20.419Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">MUSt3R: Multi-view Network for Stereo 3D Reconstructio</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">NAVER LABS Europe（欧洲Naver实验室）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2024.6</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td></td></tr><tr><td><strong>所属领域</strong></td><td>一个 3D 点、一个置信度值和一个局部特征。<br></td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>在 DUSt3R 前额外加一个 network 输出稠密的 local features，并添加 matching loss 来训练。最后引入一种快速相互匹配方案，能够将匹配速度提高几个数量级。</li></ol><h3 id="网络架构">网络架构</h3><p><img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MASt3R-MVS/IMG-20250907114911708.png" alt=""><br>如何匹配？</p><h3 id="背景知识">背景知识</h3><p>匹配方法通常被归结为一个三步流程，首先提取稀疏且可重复的关键点，然后用局部不变特征描述它们，最后通过比较它们在特征空间中的距离来配对离散的关键点集。SIFT在 COLMAP等 3D 重建流程中的成功。<br>基于关键点的方法通过将匹配简化为关键点袋（BoK）问题，丢弃了对应任务的全局几何上下文信息。这使得它们在具有重复模式或低纹理区域的情况下特别容易出错，而这些区域实际上对于局部描述符来说是不适定的。解决此问题的一种方法是在配对步骤中引入全局优化策略，通常利用一些关于匹配的先验知识。如SuperGlue。<br>但是，如果关键点及其描述符尚未编码足够的信息，则在匹配过程中利用全局上下文可能为时已晚。因此，另一个方向是考虑密集的整体匹配，即完全避免关键点，并一次匹配整个图像。如LoFTR将图像视为一个整体，并且得到的对应集是密集的。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    <category term="论文阅读" scheme="http://outbreak-sen.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="Muilt View Stereo" scheme="http://outbreak-sen.github.io/tags/Muilt-View-Stereo/"/>
    
  </entry>
  
</feed>
