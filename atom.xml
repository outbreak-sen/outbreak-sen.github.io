<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>This is a 部落格 of outbreak_sen</title>
  
  <subtitle>[object Object]</subtitle>
  <link href="http://outbreak-sen.github.io/atom.xml" rel="self"/>
  
  <link href="http://outbreak-sen.github.io/"/>
  <updated>2025-08-13T00:29:37.319Z</updated>
  <id>http://outbreak-sen.github.io/</id>
  
  <author>
    <name>outbreak_sen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/08/13/%E5%B0%86PyQT%E5%B9%B3%E5%8F%B0+%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AD%90%E4%BB%93%E5%BA%93%E6%89%93%E5%8C%85%E4%B8%BAexe/"/>
    <id>http://outbreak-sen.github.io/2025/08/13/%E5%B0%86PyQT%E5%B9%B3%E5%8F%B0+%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%AD%90%E4%BB%93%E5%BA%93%E6%89%93%E5%8C%85%E4%B8%BAexe/</id>
    <published>2025-08-13T00:29:37.319Z</published>
    <updated>2025-08-13T00:29:37.319Z</updated>
    
    <content type="html"><![CDATA[<h1>将PyQT平台+深度学习算法子仓库打包为exe</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyinstaller</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;将PyQT平台+深度学习算法子仓库打包为exe&lt;/h1&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/08/05/%E4%BD%BF%E7%94%A8OBS%E4%B8%8A%E4%BC%A0%E5%A4%A7%E6%96%87%E4%BB%B6%E5%B9%B6%E5%86%8DmodelArts%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%AE%BF%E9%97%AE/"/>
    <id>http://outbreak-sen.github.io/2025/08/05/%E4%BD%BF%E7%94%A8OBS%E4%B8%8A%E4%BC%A0%E5%A4%A7%E6%96%87%E4%BB%B6%E5%B9%B6%E5%86%8DmodelArts%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E8%AE%BF%E9%97%AE/</id>
    <published>2025-08-05T11:12:45.160Z</published>
    <updated>2025-08-06T09:29:51.754Z</updated>
    
    <content type="html"><![CDATA[<h1>使用OBS上传大文件并再modelArts服务器上访问</h1><h2 id="背景">背景</h2><p>我需要在华为服务器上跑代码，数据集达45G，单次只能从本地上传100M，然后听说大文件通过OBS很方便，这里记录如何使用OBS来上传，看要求这里应该是只需要一个OBS的URL。</p><p>在Notebook中可以通过调用ModelArts的Moxing接口或者SDK接口与OBS交互，将Notebook中的文件上传至OBS，或者下载OBS中的文件至Notebook中。MoXing是ModelArts自研的分布式训练加速框架，MoXing提供了一套文件对象API，可以用来读写OBS文件。</p><img src="/home/outbreak/.config/Typora/typora-user-images/image-20250805191214225.png" alt="image-20250805191214225" style="zoom:33%;" /><p><img src="zh-cn_image_0000002089884510.png" alt="img"></p><h2 id="首先学习obs的基本概念">首先学习OBS的基本概念</h2><ol><li>使用华为云的对象存储服务就是OBS，你需要把本地的文件拷贝到OBS中，然后在notebook里访问OBS</li><li>首先需要在华为云的OBS里创建一个帐号再创建一个筒，这个桶里有桶名称，账户AK，密码SK，和地址，这个地址是提供OBS服务的地址，也叫做<strong>Endpoint</strong>？</li><li>然后再在本地使用obsutil或者其他的obs browser等软件，把本地的数据集上传到OBS，这里需要配置OBS的地址密码</li><li>在OBS中有了数据集之后，就可以在notebook中访问OBS的文件，把数据集下载到notebook中或者说直接访问OBS中的数据集</li></ol><h2 id="创建obs桶-对应基本概念里的1">创建OBS桶（对应基本概念里的1）</h2><p>点击就创建好了</p><p><img src="image-20250805192421049.png" alt="image-20250805192421049"></p><h2 id="obs的信息-对应基本概念里的2">OBS的信息（对应基本概念里的2）</h2><ol><li>点击这里获得AK和SK</li></ol><p><img src="image-20250805195507566.png" alt="image-20250805195507566"></p><p>这里需要创建访问密钥</p><p><img src="image-20250805195634310.png" alt="image-20250805195634310"></p><ol start="2"><li><p>然后会下载一个CSV，我的CSV里是这样的，里面就是AK和SK</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User Name,Access Key Id,Secret Access Key</span><br><span class="line">&quot;mindspore-houbosen&quot;,GK2KEZTVRRGWN2L9FQKT,lbKRy9E2PjmWsfv5kB39ck3lW3RV1pXpNmKlQ6Rv</span><br></pre></td></tr></table></figure></li><li><p>点击概览获得endpoint</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Endpoint</span><br><span class="line">obs.cn-north-305.tjaicc.com</span><br><span class="line"># 然后这里还有一个访问域名，介绍说是可以直接访问这个桶</span><br><span class="line">dtu.obs.cn-north-305.tjaicc.com</span><br></pre></td></tr></table></figure></li></ol><p><img src="image-20250805195404076.png" alt="image-20250805195404076"></p><h2 id="上传文件到obs桶-对应基本概念里的3">上传文件到OBS桶（对应基本概念里的3）</h2><p>这里有多种方式把本地的文件上传到OBS桶，首先再次整理以下当前的各种信息</p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td></td><td></td></tr><tr><td>User Name</td><td>“mindspore-houbosen”</td></tr><tr><td>Access Key Id(AK)</td><td>GK2KEZTVRRGWN2L9FQKT</td></tr><tr><td>Secret Access Key</td><td>lbKRy9E2PjmWsfv5kB39ck3lW3RV1pXpNmKlQ6Rv</td></tr><tr><td>Endpoint</td><td><a href="http://obs.cn-north-305.tjaicc.com">obs.cn-north-305.tjaicc.com</a></td></tr><tr><td>访问域名</td><td><a href="http://dtu.obs.cn-north-305.tjaicc.com">dtu.obs.cn-north-305.tjaicc.com</a></td></tr><tr><td>bucketName</td><td>dtu</td></tr></tbody></table><h2 id="安装和配置obsutil-需要进入到obsutil中使用">安装和配置OBSUtil（需要进入到obsutil中使用）</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">wget https://obs-community.obs.cn-north-1.myhuaweicloud.com/obsutil/current/obsutil_linux_amd64.tar.gz</span><br><span class="line"># wget https://obs-community.obs.cn-north-1.myhuaweicloud.com/obsutil/current/obsutil_linux_arm64.tar.gz</span><br><span class="line">tar -xzvf obsutil_linux_amd64.tar.gz</span><br><span class="line">进入obsutil所在目录。“x.x.x”表示obsutil的版本号</span><br><span class="line">cd obsutil_linux_amd64_x.x.x</span><br><span class="line">chmod 755 obsutil</span><br><span class="line">继续在目录中执行以下命令，如果能顺利返回obsutil版本号，说明安装成功。</span><br><span class="line">./obsutil version</span><br><span class="line"></span><br><span class="line">./obsutil config -interactive</span><br><span class="line"># 查看配置</span><br><span class="line">cat /home/ma-user/.obsutilconfig</span><br><span class="line"># 更新配置文件</span><br><span class="line">./obsutil config -interactive</span><br><span class="line"># 检查连通性，可以看到我们创建好的桶dtu已经存在了</span><br><span class="line">./obsutil ls -s</span><br><span class="line">Start at 2025-08-05 12:24:08.198333261 +0000 UTC</span><br><span class="line"></span><br><span class="line">obs://checkpoints</span><br><span class="line">obs://dtu</span><br><span class="line">obs://f5d4f1084b8b458294a6d6e61e87383b</span><br><span class="line">obs://mindsporesig</span><br><span class="line">obs://qwen3-0.6b</span><br><span class="line">obs://ringmoe</span><br><span class="line">Bucket number: 6</span><br></pre></td></tr></table></figure><p>上传</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">./obsutil cp /media/outbreak/68E1-B517/Dataset/DTU_ZIP/dtu_training/mvs_training/dtu_training obs://dtu/dtu_training/ -r -f -j=10 -p=10</span><br></pre></td></tr></table></figure><h2 id="notebook与obs文件交互-对应基本概念里的4">Notebook与OBS文件交互（对应基本概念里的4）</h2><p>在Notebook中可以通过调用ModelArts的Moxing接口或者SDK接口与OBS交互，将Notebook中的文件上传至OBS，或者下载OBS中的文件至Notebook中。MoXing是ModelArts自研的分布式训练加速框架，MoXing提供了一套文件对象API，可以用来读写OBS文件。</p><p><strong>/home/ma-user/work目录下的数据会保存，其余目录下内容会被清理</strong></p><h3 id="方法一：在notebook中通过moxing上传下载obs文件-据说最方便">方法一：在Notebook中通过Moxing上传下载OBS文件（据说最方便）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> moxing <span class="keyword">as</span> mox</span><br><span class="line"></span><br><span class="line"><span class="comment">#下载一个OBS文件夹sub_dir_0，从OBS下载至Notebook</span></span><br><span class="line">mox.file.copy_parallel(<span class="string">&#x27;obs://bucket_name/sub_dir_0&#x27;</span>, <span class="string">&#x27;/home/ma-user/work/sub_dir_0&#x27;</span>)</span><br><span class="line"><span class="comment">#下载一个OBS文件obs_file.txt，从OBS下载至Notebook</span></span><br><span class="line">mox.file.copy(<span class="string">&#x27;obs://bucket_name/obs_file.txt&#x27;</span>, <span class="string">&#x27;/home/ma-user/work/obs_file.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#上传一个OBS文件夹sub_dir_0，从Notebook上传至OBS</span></span><br><span class="line">mox.file.copy_parallel(<span class="string">&#x27;/home/ma-user/work/sub_dir_0&#x27;</span>, <span class="string">&#x27;obs://bucket_name/sub_dir_0&#x27;</span>)</span><br><span class="line"><span class="comment">#上传一个OBS文件obs_file.txt，从Notebook上传至OBS</span></span><br><span class="line">mox.file.copy(<span class="string">&#x27;/home/ma-user/work/obs_file.txt&#x27;</span>, <span class="string">&#x27;obs://bucket_name/obs_file.txt&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="方法二：在notebook中通过modelarts-sdk上传下载obs文件">方法二：在Notebook中通过ModelArts SDK上传下载OBS文件</h3><p>使用ModelArts SDK接口将OBS中的文件下载到Notebook后进行操作。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from modelarts.session <span class="keyword">import</span> <span class="type">Session</span></span><br><span class="line"><span class="variable">session</span> <span class="operator">=</span> Session()</span><br><span class="line">session.obs.download_file(src_obs_file=<span class="string">&quot;obs://bucket-name/dir1/file1.txt&quot;</span>, dst_local_dir=<span class="string">&quot;/home/ma-user/work/&quot;</span>)</span><br><span class="line"># 将OBS中的文件夹dir1下载到Notebook的/home/ma-user/work/路径下</span><br><span class="line">session.obs.download_dir(src_obs_dir=<span class="string">&quot;obs://bucket-name/dir1/&quot;</span>, dst_local_dir=<span class="string">&quot;/home/ma-user/work/&quot;</span>)</span><br><span class="line"># 将Notebook中的file1.txt文件上传到OBS桶路径obs:<span class="comment">//bucket-name/dir1/中 </span></span><br><span class="line">session.obs.upload_file(src_local_file=<span class="string">&#x27;/home/ma-user/work/file1.txt&#x27;</span>, dst_obs_dir=<span class="string">&#x27;obs://bucket-name/dir1/&#x27;</span>)</span><br><span class="line">session.obs.upload_dir(src_local_dir=<span class="string">&#x27;/home/ma-user/work/&#x27;</span>, dst_obs_dir=<span class="string">&#x27;obs://bucket-name/dir1/&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="方法三：在notebook中通过obsutil上传下载obs文件">方法三：在Notebook中通过OBSutil上传下载OBS文件</h3><p><strong>超大文件，建议以zip压缩包的方式上传到obs。如果需要使用，先拷贝到notebook，再解压，再回传到obs中的cache目录即可，使用完cache目录可删除？</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 首先初始化</span><br><span class="line"># 初始化配置</span><br><span class="line">./obsutil config -i=ak -k=sk -e=endpoint</span><br><span class="line"># 或者不用上面那个方法，用下面这个交互的，其中token那一项不用填写直接回车</span><br><span class="line">./obsutil config -interactive</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 查看配置</span><br><span class="line">cat /home/ma-user/.obsutilconfig</span><br><span class="line"># 更新配置文件</span><br><span class="line">./obsutil config -interactive</span><br><span class="line"># 检查连通性</span><br><span class="line">./obsutil ls -s</span><br><span class="line"></span><br><span class="line"># 上传文件夹到obs</span><br><span class="line">./obsutil cp /home/ma-user/work/MyDocuments obs://liulingjun-demo/cache/ -r -f -j=10 -p=10</span><br><span class="line"></span><br><span class="line"># 删除单个文件</span><br><span class="line">./obsutil rm obs://bucket-test/key -f</span><br><span class="line"># 删除文件夹</span><br><span class="line">./obsutil rm obs://bucket-test -r -f -j=10 -p=10</span><br><span class="line"></span><br><span class="line"># 拷贝zip文件到obs</span><br><span class="line">./obsutil cp obs://lljyoyo-demo/datasets/ImageNet2012/train.zip /home/ma-user/work/ImageNet2012/train.zip</span><br><span class="line"># 解压zip文件</span><br><span class="line">cd /home/ma-user/work/ImageNet2012</span><br><span class="line">unzip train.zip</span><br><span class="line"></span><br><span class="line"># 文件回传obs的cache目录</span><br><span class="line">./obsutil cp /home/ma-user/work/ImageNet2012/train obs://lljyoyo-demo/cache/ImageNet2012 -f -r</span><br><span class="line"># 使用完删除cache目录 </span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="方法四：在notebook中使用obs-sdk上传下载obs文件">方法四：在Notebook中使用OBS SDK上传下载OBS文件</h3><p>SDK支持上传0KB~5GB的对象。流式上传、<strong>文件上传</strong>和追加上传每次上传内容大小<strong>不能超过5GB</strong>；当上传较大文件时，请使用分段上传，分段上传每段内容大小不能超过5GB。批量复制时的最大并发数，-j=10；</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># .obsutilconfig 配置文件所在路径 /home/ma-user/.obsutilconfig</span><br><span class="line"># pip3 install esdk-obs-python</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="keyword">from</span> obs <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">AK = <span class="string">&#x27;GK2KEZTVRRGWN2L9FQKT&#x27;</span></span><br><span class="line">SK = <span class="string">&#x27;lbKRy9E2PjmWsfv5kB39ck3lW3RV1pXpNmKlQ6Rv&#x27;</span></span><br><span class="line">server = <span class="string">&#x27;obs.cn-north-305.tjaicc.com&#x27;</span></span><br><span class="line">bucketName = <span class="string">&#x27;dtu&#x27;</span></span><br><span class="line">objectKey = <span class="string">&#x27;dtu_training/&#x27;</span></span><br><span class="line">localFile = <span class="string">&#x27;/home/ma-user/work/dtu_training/&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单文件下载</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_file</span>():</span><br><span class="line">    <span class="comment"># Constructs a obs client instance with your account for accessing OBS</span></span><br><span class="line">obsClient = ObsClient(access_key_id=AK, secret_access_key=SK, server=server)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Download the object to a file</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Downloading an object to :&#x27;</span> + localFile + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    obsClient.getObject(bucketName, objectKey, downloadPath=localFile)</span><br><span class="line"><span class="comment"># 断点续传下载</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_file_v1</span>():</span><br><span class="line">    <span class="comment"># Download the object to a file</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Downloading an object to :&#x27;</span> + localFile + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 下载时的最大并发数</span></span><br><span class="line">    taskNum = <span class="number">5</span></span><br><span class="line">    <span class="comment"># 分段大小，单位字节，取值范围是100KB~5GB，默认为5MB。</span></span><br><span class="line">    partSize = <span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span></span><br><span class="line">    obsClient = ObsClient(access_key_id=AK, secret_access_key=SK, server=server)</span><br><span class="line">    resp = obsClient.downloadFile(bucketName, objectKey=objectKey, downloadFile=localFile, partSize=partSize, taskNum=<span class="number">5</span>, enableCheckpoint=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> resp.status &lt; <span class="number">300</span>:    </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;requestId:&#x27;</span>, resp.requestId)    </span><br><span class="line">    <span class="keyword">else</span>:    </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;errorCode:&#x27;</span>, resp.errorCode)    </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;errorMessage:&#x27;</span>, resp.errorMessage)</span><br><span class="line"><span class="comment"># 单文件上传</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_file</span>():</span><br><span class="line">    <span class="comment"># Create bucket</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Create a new bucket for demo\n&#x27;</span>)</span><br><span class="line">    obsClient.createBucket(bucketName)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Upload an object to your bucket</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Uploading a new object to OBS from a file\n&#x27;</span>)</span><br><span class="line">    obsClient.putFile(bucketName, objectKey, localFile)</span><br><span class="line"><span class="comment"># 断点续传上传（推荐）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update_file_v1</span>():</span><br><span class="line">    <span class="comment"># Create bucket</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Create a new bucket for demo\n&#x27;</span>)</span><br><span class="line">    obsClient.createBucket(bucketName)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Upload an object to your bucket</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Uploading a new object to OBS from a file\n&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 下载时的最大并发数</span></span><br><span class="line">    taskNum = <span class="number">5</span></span><br><span class="line">    <span class="comment"># 分段大小，单位字节，取值范围是100KB~5GB，默认为5MB。</span></span><br><span class="line">    partSize = <span class="number">10</span> * <span class="number">1024</span> * <span class="number">1024</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        resp = obsClient.uploadFile(bucketName, objectKey, uploadFile=localFile, partSize=partSize, taskNum=taskNum, enableCheckpoint=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">if</span> resp.status &lt; <span class="number">300</span>:    </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;requestId:&#x27;</span>, resp.requestId)    </span><br><span class="line">        <span class="keyword">else</span>:    </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;errorCode:&#x27;</span>, resp.errorCode)    </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;errorMessage:&#x27;</span>, resp.errorMessage)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">import</span> traceback</span><br><span class="line">        <span class="built_in">print</span>(traceback.format_exc())</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">download_file_v1()</span><br><span class="line">    <span class="comment"># download_file_v1()</span></span><br></pre></td></tr></table></figure><h3 id="还有两个方法是通过notenook控制台和obsbrowser">还有两个方法是通过notenook控制台和OBSBrowser+</h3><h2 id="参考文献">参考文献</h2><p><a href="https://support.huaweicloud.com/intl/zh-cn/modelarts_faq/modelarts_05_0024.html">这个文档不赖</a></p><p><a href="https://blog.csdn.net/m0_37605642/article/details/126350518">这个是对上面那个教程的整理</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;使用OBS上传大文件并再modelArts服务器上访问&lt;/h1&gt;
&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;我需要在华为服务器上跑代码，数据集达45G，单次只能从本地上传100M，然后听说大文件通过OBS很方便，这里记录如何使用OBS来上传，看要求这里应该是只需要一个</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/08/03/%E5%A6%82%E4%BD%95%E6%8A%8A%E4%B8%80%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E4%BB%8Etorch%E6%A1%86%E6%9E%B6%E6%94%B9%E6%88%90mindspore%E6%A1%86%E6%9E%B6/"/>
    <id>http://outbreak-sen.github.io/2025/08/03/%E5%A6%82%E4%BD%95%E6%8A%8A%E4%B8%80%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E4%BB%8Etorch%E6%A1%86%E6%9E%B6%E6%94%B9%E6%88%90mindspore%E6%A1%86%E6%9E%B6/</id>
    <published>2025-08-03T13:09:37.019Z</published>
    <updated>2025-08-04T13:49:54.412Z</updated>
    
    <content type="html"><![CDATA[<h1>如何把一个深度学习代码从torch框架改成mindspore框架</h1><h2 id="最重要的如何创建数据集和dataloader">最重要的如何创建数据集和dataloader</h2><p>在 MindSpore 静态图模式（<code>@ms_function</code> 或默认图模式）中是不允许 <code>del xxx</code>。</p><h2 id="mindspore-不支持detach">MindSpore 不支持detach()</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">cur_depth = cur_depth.detach()</span><br><span class="line"># mindspore版本</span><br><span class="line">cur_depth = ops.stop_gradient(cur_depth)</span><br></pre></td></tr></table></figure><h2 id="mindspore-中调用了-imgs-size-1-但-mindspore-的-tensor-没有-size-方法-应该用-shape-属性">MindSpore 中调用了 <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">imgs.size(1)</a>，但 MindSpore 的 Tensor 没有 <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">size()</a> 方法，应该用 <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">shape</a> 属性</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">for nview_idx in range(imgs.size(1)):</span><br><span class="line"># mindspore版本</span><br><span class="line">for nview_idx in range(imgs.shape[1]):  </span><br></pre></td></tr></table></figure><h2 id="ops-split-不支持-output-num-而是使用-split-size-or-sections"><code>ops.split</code> 不支持 <code>output_num</code> 而是使用 <code>split_size_or_sections</code></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden_d, inp_d = ops.split(cnet_depth_stage, axis=1, split_size_or_sections=2)</span><br></pre></td></tr></table></figure><h2 id="tensor-repeat不支持axis">tensor.repeat不支持axis</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">xyz = ops.stack((x, y, ops.ones_like(x)))  # [3, H*W]</span><br><span class="line">print(&quot;xyz.shape:&quot;,xyz.shape)</span><br><span class="line">xyz = ops.expand_dims(xyz, 0).repeat(batch, axis=0)  # [B, 3, H*W]</span><br><span class="line">print(&quot;xyz.shape:&quot;,xyz.shape)</span><br><span class="line"></span><br><span class="line"># mindspore版本</span><br><span class="line">xyz = ops.stack((x, y, ops.ones_like(x)))  # [3, H*W]</span><br><span class="line">print(&quot;xyz.shape:&quot;,xyz.shape)</span><br><span class="line">xyz = ops.expand_dims(xyz, 0)  # [1, 3, H*W]</span><br><span class="line">xyz = ops.tile(xyz, (batch, 1, 1))  # [B, 3, H*W]</span><br><span class="line">print(&quot;xyz.shape:&quot;,xyz.shape)</span><br></pre></td></tr></table></figure><h2 id="forward函数命名为construct函数">forward函数命名为construct函数</h2><p>原继承nn.module，现在改为继承nn.Cell，继承之后需要把原先的forward函数改名为construct</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lass DepthNet(nn.Cell):</span><br><span class="line">    def __init__(self, cnnpixel=False):</span><br><span class="line">        super(DepthNet, self).__init__()</span><br><span class="line"></span><br><span class="line">    def construct(self, features, proj_matrices, depth_values, num_depth, cost_regularization, pixel_wise_net, G=8):</span><br><span class="line">return </span><br></pre></td></tr></table></figure><h2 id="torch-nn-sequential-改为mindspore-nn-sequentialcell">torch.nn.Sequential 改为mindspore.nn.SequentialCell</h2><h2 id="nn-modulelist改为nn-celllist">nn.ModuleList改为nn.CellList</h2><h2 id="sequence改为ops-sequence">sequence改为ops.sequence</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">prob_volume_pre = prob_volume_pre.squeeze(1)</span><br><span class="line"># mindspore版本</span><br><span class="line">prob_volume_pre = ops.squeeze(prob_volume_pre, axis=1)</span><br></pre></td></tr></table></figure><h2 id="f变ops">F变ops</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">prob_volume_ = F.softmax(prob_volume_pre*5.0, dim=1)</span><br><span class="line"># mindspore版本</span><br><span class="line">prob_volume = ops.softmax(prob_volume_pre, axis=1)</span><br></pre></td></tr></table></figure><h2 id="padding要加pad-mode">padding要加pad_mode</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">self.convc1 = nn.Conv2d(cost_dim, hidden_dim, 1, padding=0)</span><br><span class="line"># mindspore版本</span><br><span class="line">nn.Conv2d(cost_dim, hidden_dim, 1, padding=0, pad_mode=&#x27;pad&#x27;)</span><br></pre></td></tr></table></figure><h2 id="relu要创建后才能使用-且没有inplace">relu要创建后才能使用，且没有inplace</h2><p>激活函数都需要先初始化才能用不能直接用</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># torch版本有inplace参数并且relu可以直接运算</span><br><span class="line">x = F.relu(x, inplace=True)</span><br><span class="line">return F.relu(self.bn(self.conv(x)), inplace=True)</span><br><span class="line">以下啊是错误的</span><br><span class="line">class ConvBnReLU3D(nn.Cell):</span><br><span class="line">    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, pad=1):</span><br><span class="line">        super(ConvBnReLU3D, self).__init__()</span><br><span class="line">        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, pad_mode=&#x27;pad&#x27;, has_bias=False)</span><br><span class="line">        self.bn = nn.BatchNorm3d(out_channels)</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return nn.ReLU(self.bn(self.conv(x)))</span><br><span class="line">        </span><br><span class="line"># 要先创建一个relu，才能使用        </span><br><span class="line">class ConvBnReLU3D(nn.Cell):</span><br><span class="line">    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, pad=1):</span><br><span class="line">        super(ConvBnReLU3D, self).__init__()</span><br><span class="line">        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=pad, pad_mode=&#x27;pad&#x27;, has_bias=False)</span><br><span class="line">        self.bn = nn.BatchNorm3d(out_channels)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return self.relu(self.bn(self.conv(x)))</span><br></pre></td></tr></table></figure><h2 id="bias参数改名为has-bias">bias参数改名为has_bias</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">self.conv = nn.Conv2dTranspose(in_channels, out_channels, kernel_size, stride=stride,bias=(not bn), **kwargs)</span><br><span class="line"># mindspore版本</span><br><span class="line">self.conv = nn.Conv2dTranspose(in_channels, out_channels, kernel_size, stride=stride,has_bias=(not bn), **kwargs)</span><br></pre></td></tr></table></figure><h2 id="cat指定维度不用dim-改名为axis">cat指定维度不用dim，改名为axis</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># torch版本</span><br><span class="line">ops.cat([conv0, IGEV_cost_], dim=1)</span><br><span class="line"># mindspore版本</span><br><span class="line">ops.cat([conv0, IGEV_cost_], axis=1)</span><br></pre></td></tr></table></figure><h2 id="所有nrom的操作中weight和bias改名为gamma和beta">所有nrom的操作中weight和bias改名为gamma和beta</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 针对bn层进行初始化</span><br><span class="line">bn_module = nn.BatchNorm2d(out_channels, momentum=bn_momentum)</span><br><span class="line"># torch版本</span><br><span class="line">def init_bn(module):</span><br><span class="line">    if module.weight is not None:</span><br><span class="line">        nn.init.ones_(module.weight)</span><br><span class="line">    if module.bias is not None:</span><br><span class="line">        nn.init.zeros_(module.bias)</span><br><span class="line">    return</span><br><span class="line"></span><br><span class="line"># mindspore版本</span><br><span class="line">def init_bn(module):</span><br><span class="line">    # 用gamma和beta取代wight和bias</span><br><span class="line">    # 没有init函数，只有set_data和initializer</span><br><span class="line">    if module.gamma is not None:</span><br><span class="line">        module.gamma.set_data(initializer(One(), module.gamma.shape, module.gamma.dtype))</span><br><span class="line">    if module.beta is not None:</span><br><span class="line">        module.beta.set_data(initializer(Zero(), module.beta.shape, module.beta.dtype))</span><br><span class="line">    return</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">torch.unbind</a> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.unstack</a></li><li><a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">torch.cat</a> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.cat</a></li><li><code>tensor.view</code> → <code>tensor.reshape</code></li><li><code>tensor.permute</code> → <code>tensor.transpose</code></li><li><code>tensor.detach()</code> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.stop_gradient</a></li><li><a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">F.interpolate</a> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.interpolate</a> 或 <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">nn.ResizeBilinear</a></li><li><a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">F.softmax</a> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.softmax</a></li><li><a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">F.avg_pool3d</a> → <a href="vscode-file://vscode-app/usr/share/code/resources/app/out/vs/code/electron-browser/workbench/workbench.html">ops.avg_pool3d</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;如何把一个深度学习代码从torch框架改成mindspore框架&lt;/h1&gt;
&lt;h2 id=&quot;最重要的如何创建数据集和dataloader&quot;&gt;最重要的如何创建数据集和dataloader&lt;/h2&gt;
&lt;p&gt;在 MindSpore 静态图模式（&lt;code&gt;@ms_functio</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84MVS%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/07/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84MVS%E4%BB%A3%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2025-07-08T13:02:55.953Z</published>
    <updated>2025-07-08T13:25:24.624Z</updated>
    
    <content type="html"><![CDATA[<h1>深度学习的MVS代码阅读笔记</h1><h1>importlib 用法小结</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 与 import os.path as path 效果一样 </span><br><span class="line">path = importlib.import_module(&#x27;os.path&#x27;) </span><br><span class="line">path = importlib.import_module(&#x27;.path&#x27;, package=&#x27;os&#x27;)</span><br><span class="line">path.join(&#x27;a&#x27;, &#x27;b&#x27;) </span><br></pre></td></tr></table></figure><p>用于替代冗长的if else的import。这个没啥意思</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def run(model_name, input):</span><br><span class="line">    if model_name == &#x27;resnet_50&#x27;:</span><br><span class="line">        from resnet_50.model import load_model</span><br><span class="line">    elif model_name == &#x27;hrnet&#x27;:</span><br><span class="line">        from hrnet.model import load_model</span><br><span class="line">    elif model_name == &#x27;moblienet&#x27;:</span><br><span class="line">        from mobilenet.model import load_model</span><br><span class="line"></span><br><span class="line">    model = load_model()</span><br><span class="line">    output = model(input)</span><br><span class="line">    return output</span><br><span class="line"># 相当于下面这一种表达</span><br><span class="line">def run(model_name, input):</span><br><span class="line">    load_model = importlib.import_module(&#x27;load_model&#x27;, package=&#x27;&#123;&#125;.model&#x27;.format(model_name))</span><br><span class="line"></span><br><span class="line">    model = load_model()</span><br><span class="line">    output = model(input)</span><br><span class="line">    return output</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在MVS中有DTU，Tanks等数据集，这里直接用dataset的名字来引入不同的数据集</p><p>在dtu_yao中，数据集命名为MVSDataset，在tank.py中也命名为MVSDataset，这样可以在不同名称的py文件引入dataset</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def find_dataset_def(dataset_name):</span><br><span class="line">    module_name = &#x27;datasets.&#123;&#125;&#x27;.format(dataset_name)</span><br><span class="line">    module = importlib.import_module(module_name)</span><br><span class="line">    return getattr(module, &quot;MVSDataset&quot;)</span><br><span class="line"></span><br><span class="line">from datasets import find_dataset_def</span><br><span class="line">MVSDataset = find_dataset_def(args.dataset)</span><br></pre></td></tr></table></figure><h1>MVSDataset的构建</h1><p>继承from torch.utils.data import Dataset</p><p>覆盖 def <strong>getitem</strong>(self, idx):</p><p>def <strong>len</strong>(self):</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;深度学习的MVS代码阅读笔记&lt;/h1&gt;
&lt;h1&gt;importlib 用法小结&lt;/h1&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/07/08/Effi-MVSNetPlus%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/07/08/Effi-MVSNetPlus%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</id>
    <published>2025-07-08T13:02:32.233Z</published>
    <updated>2025-07-08T13:02:32.233Z</updated>
    
    <content type="html"><![CDATA[<h1>Effi-MVSNetPlus阅读笔记</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Effi-MVSNetPlus阅读笔记&lt;/h1&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/07/01/SMPLify-%E4%BB%8E%E4%BA%8C%E7%BB%B4%E5%9D%90%E6%A0%87%E5%88%B0%E4%BA%BA%E4%BD%93SMPL%E5%92%8C%E4%B8%89%E7%BB%B4%E5%9D%90%E6%A0%87/"/>
    <id>http://outbreak-sen.github.io/2025/07/01/SMPLify-%E4%BB%8E%E4%BA%8C%E7%BB%B4%E5%9D%90%E6%A0%87%E5%88%B0%E4%BA%BA%E4%BD%93SMPL%E5%92%8C%E4%B8%89%E7%BB%B4%E5%9D%90%E6%A0%87/</id>
    <published>2025-07-01T07:52:29.137Z</published>
    <updated>2025-07-01T08:03:16.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="smplify-从二维坐标到人体smpl和关节转动">SMPLify-从二维坐标到人体SMPL和关节转动</h2><p>之前我一直在SMPLify看代码，确实运行起来了，但是速度太慢了，可能因为用的优化器是chumdy，实际上只所以慢是因为初值不好，用深度学习的方法可以快速猜出来初值，然后用优化的方法进行优化就很快，这个思路就是SPIN</p><p>后来我发现SPIN里面的SMPLify写的很好，并且是使用了SMPL-X这个库的，把渲染等工作直接调库，注意SMPL-X也是支持SMPL的。</p><p>这里讲解SPIN中的SMPLify</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> models.smpl <span class="keyword">import</span> SMPL</span><br><span class="line"><span class="keyword">from</span> .losses <span class="keyword">import</span> camera_fitting_loss, body_fitting_loss</span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> constants</span><br><span class="line"></span><br><span class="line"><span class="comment"># For the GMM prior, we use the GMM implementation of SMPLify-X</span></span><br><span class="line"><span class="comment"># https://github.com/vchoutas/smplify-x/blob/master/smplifyx/prior.py</span></span><br><span class="line"><span class="keyword">from</span> .prior <span class="keyword">import</span> MaxMixturePrior</span><br><span class="line"><span class="comment"># 建立SMPLify优化器的时候可以一次性优化好几个，设置batch就可以</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SMPLify</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Implementation of single-stage SMPLify.&quot;&quot;&quot;</span> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, </span></span><br><span class="line"><span class="params">                 step_size=<span class="number">1e-2</span>,</span></span><br><span class="line"><span class="params">                 batch_size=<span class="number">66</span>,</span></span><br><span class="line"><span class="params">                 num_iters=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">                 focal_length=<span class="number">5000</span>,</span></span><br><span class="line"><span class="params">                 device=torch.device(<span class="params"><span class="string">&#x27;cuda&#x27;</span></span>)</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Store options</span></span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line">        <span class="variable language_">self</span>.focal_length = focal_length</span><br><span class="line">        <span class="variable language_">self</span>.step_size = step_size</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Ignore the the following joints for the fitting process，这几个不优化</span></span><br><span class="line">        <span class="comment"># 优化的二维关节和SMPL的关节名称一一对应，所以代码中看不到冗长的关节对应部分</span></span><br><span class="line">        ign_joints = [<span class="string">&#x27;OP Neck&#x27;</span>, <span class="string">&#x27;OP RHip&#x27;</span>, <span class="string">&#x27;OP LHip&#x27;</span>, <span class="string">&#x27;Right Hip&#x27;</span>, <span class="string">&#x27;Left Hip&#x27;</span>]</span><br><span class="line">        <span class="variable language_">self</span>.ign_joints = [constants.JOINT_IDS[i] <span class="keyword">for</span> i <span class="keyword">in</span> ign_joints]</span><br><span class="line">        <span class="variable language_">self</span>.num_iters = num_iters</span><br><span class="line">        <span class="comment"># GMM pose prior</span></span><br><span class="line">        <span class="variable language_">self</span>.pose_prior = MaxMixturePrior(prior_folder=<span class="string">&#x27;data&#x27;</span>,</span><br><span class="line">                                          num_gaussians=<span class="number">8</span>,</span><br><span class="line">                                          dtype=torch.float32).to(device)</span><br><span class="line">        <span class="comment"># Load SMPL model</span></span><br><span class="line">        <span class="variable language_">self</span>.smpl = SMPL(config.SMPL_MODEL_DIR,</span><br><span class="line">                         batch_size=batch_size,</span><br><span class="line">                         create_transl=<span class="literal">False</span>).to(<span class="variable language_">self</span>.device)</span><br><span class="line"><span class="comment"># 关键是这个函数</span></span><br><span class="line">    <span class="comment"># 设置初值并传入二维坐标</span></span><br><span class="line">    <span class="comment"># 输出为mesh，三维关节坐标，每个关节的转动轴角，形状参数，相机偏移，重投影误差</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, init_pose, init_betas, init_cam_t, camera_center, keypoints_2d</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Perform body fitting.</span></span><br><span class="line"><span class="string">        Input:</span></span><br><span class="line"><span class="string">            init_pose: SMPL pose estimate</span></span><br><span class="line"><span class="string">            init_betas: SMPL betas estimate</span></span><br><span class="line"><span class="string">            init_cam_t: Camera translation estimate</span></span><br><span class="line"><span class="string">            camera_center: Camera center location</span></span><br><span class="line"><span class="string">            keypoints_2d: Keypoints used for the optimization</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            vertices: Vertices of optimized shape</span></span><br><span class="line"><span class="string">            joints: 3D joints of optimized shape</span></span><br><span class="line"><span class="string">            pose: SMPL pose parameters of optimized shape</span></span><br><span class="line"><span class="string">            betas: SMPL beta parameters of optimized shape</span></span><br><span class="line"><span class="string">            camera_translation: Camera translation</span></span><br><span class="line"><span class="string">            reprojection_loss: Final joint reprojection loss</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        batch_size = init_pose.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Make camera translation a learnable parameter</span></span><br><span class="line">        camera_translation = init_cam_t.clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get joint confidence</span></span><br><span class="line">        joints_2d = keypoints_2d[:, :, :<span class="number">2</span>]</span><br><span class="line">        joints_conf = keypoints_2d[:, :, -<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split SMPL pose to body pose and global orientation</span></span><br><span class="line">        body_pose = init_pose[:, <span class="number">3</span>:].detach().clone()</span><br><span class="line">        global_orient = init_pose[:, :<span class="number">3</span>].detach().clone()</span><br><span class="line">        betas = init_betas.detach().clone()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Step 1: Optimize camera translation and body orientation</span></span><br><span class="line">        <span class="comment"># Optimize only camera translation and body orientation</span></span><br><span class="line">        <span class="comment"># requires_grad设置为True则优化这几个参数，第一次就只优化相机参数，只用躯干的四个关节点来优化，所以这里的loss是camera_fitting_loss</span></span><br><span class="line">        body_pose.requires_grad=<span class="literal">False</span></span><br><span class="line">        betas.requires_grad=<span class="literal">False</span></span><br><span class="line">        global_orient.requires_grad=<span class="literal">True</span></span><br><span class="line">        camera_translation.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        camera_opt_params = [global_orient, camera_translation]</span><br><span class="line">        camera_optimizer = torch.optim.Adam(camera_opt_params, lr=<span class="variable language_">self</span>.step_size, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_iters):</span><br><span class="line">            smpl_output = <span class="variable language_">self</span>.smpl(global_orient=global_orient,</span><br><span class="line">                                    body_pose=body_pose,</span><br><span class="line">                                    betas=betas)</span><br><span class="line">            model_joints = smpl_output.joints</span><br><span class="line">            loss = camera_fitting_loss(model_joints, camera_translation,</span><br><span class="line">                                       init_cam_t, camera_center,</span><br><span class="line">                                       joints_2d, joints_conf, focal_length=<span class="variable language_">self</span>.focal_length)</span><br><span class="line">            camera_optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            camera_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Fix camera translation after optimizing camera</span></span><br><span class="line">        camera_translation.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment"># 然后就全优化了</span></span><br><span class="line">        <span class="comment"># Step 2: Optimize body joints</span></span><br><span class="line">        <span class="comment"># Optimize only the body pose and global orientation of the body</span></span><br><span class="line">        body_pose.requires_grad=<span class="literal">True</span></span><br><span class="line">        betas.requires_grad=<span class="literal">True</span></span><br><span class="line">        global_orient.requires_grad=<span class="literal">True</span></span><br><span class="line">        camera_translation.requires_grad = <span class="literal">False</span></span><br><span class="line">        body_opt_params = [body_pose, betas, global_orient]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For joints ignored during fitting, set the confidence to 0</span></span><br><span class="line">        joints_conf[:, <span class="variable language_">self</span>.ign_joints] = <span class="number">0.</span></span><br><span class="line">        </span><br><span class="line">        body_optimizer = torch.optim.Adam(body_opt_params, lr=<span class="variable language_">self</span>.step_size, betas=(<span class="number">0.9</span>, <span class="number">0.999</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.num_iters):</span><br><span class="line">            smpl_output = <span class="variable language_">self</span>.smpl(global_orient=global_orient,</span><br><span class="line">                                    body_pose=body_pose,</span><br><span class="line">                                    betas=betas)</span><br><span class="line">            model_joints = smpl_output.joints</span><br><span class="line">            loss = body_fitting_loss(body_pose, betas, model_joints, camera_translation, camera_center,</span><br><span class="line">                                     joints_2d, joints_conf, <span class="variable language_">self</span>.pose_prior,</span><br><span class="line">                                     focal_length=<span class="variable language_">self</span>.focal_length)</span><br><span class="line">            body_optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            body_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get final loss value</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            smpl_output = <span class="variable language_">self</span>.smpl(global_orient=global_orient,</span><br><span class="line">                                    body_pose=body_pose,</span><br><span class="line">                                    betas=betas, return_full_pose=<span class="literal">True</span>)</span><br><span class="line">            model_joints = smpl_output.joints</span><br><span class="line">            reprojection_loss = body_fitting_loss(body_pose, betas, model_joints, camera_translation, camera_center,</span><br><span class="line">                                                  joints_2d, joints_conf, <span class="variable language_">self</span>.pose_prior,</span><br><span class="line">                                                  focal_length=<span class="variable language_">self</span>.focal_length,</span><br><span class="line">                                                  output=<span class="string">&#x27;reprojection&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        vertices = smpl_output.vertices.detach()</span><br><span class="line">        joints = smpl_output.joints.detach()</span><br><span class="line">        pose = torch.cat([global_orient, body_pose], dim=-<span class="number">1</span>).detach()</span><br><span class="line">        betas = betas.detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> vertices, joints, pose, betas, camera_translation, reprojection_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_fitting_loss</span>(<span class="params">self, pose, betas, cam_t, camera_center, keypoints_2d</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Given body and camera parameters, compute reprojection loss value.</span></span><br><span class="line"><span class="string">        Input:</span></span><br><span class="line"><span class="string">            pose: SMPL pose parameters</span></span><br><span class="line"><span class="string">            betas: SMPL beta parameters</span></span><br><span class="line"><span class="string">            cam_t: Camera translation</span></span><br><span class="line"><span class="string">            camera_center: Camera center location</span></span><br><span class="line"><span class="string">            keypoints_2d: Keypoints used for the optimization</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            reprojection_loss: Final joint reprojection loss</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        batch_size = pose.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get joint confidence</span></span><br><span class="line">        joints_2d = keypoints_2d[:, :, :<span class="number">2</span>]</span><br><span class="line">        joints_conf = keypoints_2d[:, :, -<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># For joints ignored during fitting, set the confidence to 0</span></span><br><span class="line">        joints_conf[:, <span class="variable language_">self</span>.ign_joints] = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Split SMPL pose to body pose and global orientation</span></span><br><span class="line">        body_pose = pose[:, <span class="number">3</span>:]</span><br><span class="line">        global_orient = pose[:, :<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            smpl_output = <span class="variable language_">self</span>.smpl(global_orient=global_orient,</span><br><span class="line">                                    body_pose=body_pose,</span><br><span class="line">                                    betas=betas, return_full_pose=<span class="literal">True</span>)</span><br><span class="line">            model_joints = smpl_output.joints</span><br><span class="line">            reprojection_loss = body_fitting_loss(body_pose, betas, model_joints, cam_t, camera_center,</span><br><span class="line">                                                  joints_2d, joints_conf, <span class="variable language_">self</span>.pose_prior,</span><br><span class="line">                                                  focal_length=<span class="variable language_">self</span>.focal_length,</span><br><span class="line">                                                  output=<span class="string">&#x27;reprojection&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> reprojection_loss</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">body_fitting_loss</span>(<span class="params">body_pose, betas, model_joints, camera_t, camera_center,</span></span><br><span class="line"><span class="params">                      joints_2d, joints_conf, pose_prior,</span></span><br><span class="line"><span class="params">                      focal_length=<span class="number">5000</span>, sigma=<span class="number">100</span>, pose_prior_weight=<span class="number">4.78</span>,</span></span><br><span class="line"><span class="params">                      shape_prior_weight=<span class="number">5</span>, angle_prior_weight=<span class="number">15.2</span>,</span></span><br><span class="line"><span class="params">                      output=<span class="string">&#x27;sum&#x27;</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Loss function for body fitting</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    batch_size = body_pose.shape[<span class="number">0</span>]</span><br><span class="line">    rotation = torch.eye(<span class="number">3</span>, device=body_pose.device).unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    projected_joints = perspective_projection(model_joints, rotation, camera_t,</span><br><span class="line">                                              focal_length, camera_center)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Weighted robust reprojection error</span></span><br><span class="line">    reprojection_error = gmof(projected_joints - joints_2d, sigma)</span><br><span class="line">    reprojection_loss = (joints_conf ** <span class="number">2</span>) * reprojection_error.<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pose prior loss</span></span><br><span class="line">    pose_prior_loss = (pose_prior_weight ** <span class="number">2</span>) * pose_prior(body_pose, betas)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Angle prior for knees and elbows</span></span><br><span class="line">    angle_prior_loss = (angle_prior_weight ** <span class="number">2</span>) * angle_prior(body_pose).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Regularizer to prevent betas from taking large values</span></span><br><span class="line">    shape_prior_loss = (shape_prior_weight ** <span class="number">2</span>) * (betas ** <span class="number">2</span>).<span class="built_in">sum</span>(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    total_loss = reprojection_loss.<span class="built_in">sum</span>(dim=-<span class="number">1</span>) + pose_prior_loss + angle_prior_loss + shape_prior_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output == <span class="string">&#x27;sum&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> total_loss.<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">elif</span> output == <span class="string">&#x27;reprojection&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> reprojection_loss</span><br><span class="line"><span class="comment"># 相机参数优化的代码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">camera_fitting_loss</span>(<span class="params">model_joints, camera_t, camera_t_est, camera_center, joints_2d, joints_conf,</span></span><br><span class="line"><span class="params">                        focal_length=<span class="number">5000</span>, depth_loss_weight=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Loss function for camera optimization.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Project model joints</span></span><br><span class="line">    batch_size = model_joints.shape[<span class="number">0</span>]</span><br><span class="line">    rotation = torch.eye(<span class="number">3</span>, device=model_joints.device).unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">    projected_joints = perspective_projection(model_joints, rotation, camera_t,</span><br><span class="line">                                              focal_length, camera_center)</span><br><span class="line">    </span><br><span class="line">    op_joints = [<span class="string">&#x27;OP RHip&#x27;</span>, <span class="string">&#x27;OP LHip&#x27;</span>, <span class="string">&#x27;OP RShoulder&#x27;</span>, <span class="string">&#x27;OP LShoulder&#x27;</span>]</span><br><span class="line">    op_joints_ind = [constants.JOINT_IDS[joint] <span class="keyword">for</span> joint <span class="keyword">in</span> op_joints]</span><br><span class="line">    gt_joints = [<span class="string">&#x27;Right Hip&#x27;</span>, <span class="string">&#x27;Left Hip&#x27;</span>, <span class="string">&#x27;Right Shoulder&#x27;</span>, <span class="string">&#x27;Left Shoulder&#x27;</span>]</span><br><span class="line">    gt_joints_ind = [constants.JOINT_IDS[joint] <span class="keyword">for</span> joint <span class="keyword">in</span> gt_joints]</span><br><span class="line">    reprojection_error_op = (joints_2d[:, op_joints_ind] -</span><br><span class="line">                             projected_joints[:, op_joints_ind]) ** <span class="number">2</span></span><br><span class="line">    reprojection_error_gt = (joints_2d[:, gt_joints_ind] -</span><br><span class="line">                             projected_joints[:, gt_joints_ind]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Check if for each example in the batch all 4 OpenPose detections are valid, otherwise use the GT detections</span></span><br><span class="line">    <span class="comment"># OpenPose joints are more reliable for this task, so we prefer to use them if possible</span></span><br><span class="line">    is_valid = (joints_conf[:, op_joints_ind].<span class="built_in">min</span>(dim=-<span class="number">1</span>)[<span class="number">0</span>][:,<span class="literal">None</span>,<span class="literal">None</span>] &gt; <span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">    reprojection_loss = (is_valid * reprojection_error_op + (<span class="number">1</span>-is_valid) * reprojection_error_gt).<span class="built_in">sum</span>(dim=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loss that penalizes deviation from depth estimate</span></span><br><span class="line">    depth_loss = (depth_loss_weight ** <span class="number">2</span>) * (camera_t[:, <span class="number">2</span>] - camera_t_est[:, <span class="number">2</span>]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    total_loss = reprojection_loss + depth_loss</span><br><span class="line">    <span class="keyword">return</span> total_loss.<span class="built_in">sum</span>()</span><br><span class="line"><span class="comment"># 这里重新包装了SMPL模型，重新包装了smplx的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> smplx</span><br><span class="line"><span class="keyword">from</span> smplx <span class="keyword">import</span> SMPL <span class="keyword">as</span> _SMPL</span><br><span class="line"><span class="keyword">from</span> smplx.body_models <span class="keyword">import</span> ModelOutput</span><br><span class="line"><span class="keyword">from</span> smplx.lbs <span class="keyword">import</span> vertices2joints</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> constants</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SMPL</span>(<span class="title class_ inherited__">_SMPL</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; Extension of the official SMPL implementation to support more joints &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(SMPL, <span class="variable language_">self</span>).__init__(*args, **kwargs)</span><br><span class="line">        joints = [constants.JOINT_MAP[i] <span class="keyword">for</span> i <span class="keyword">in</span> constants.JOINT_NAMES]</span><br><span class="line">        J_regressor_extra = np.load(config.JOINT_REGRESSOR_TRAIN_EXTRA)</span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;J_regressor_extra&#x27;</span>, torch.tensor(J_regressor_extra, dtype=torch.float32))</span><br><span class="line">        <span class="variable language_">self</span>.joint_map = torch.tensor(joints, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        kwargs[<span class="string">&#x27;get_skin&#x27;</span>] = <span class="literal">True</span></span><br><span class="line">        smpl_output = <span class="built_in">super</span>(SMPL, <span class="variable language_">self</span>).forward(*args, **kwargs)</span><br><span class="line">       <span class="comment"># 从mesh获得人体坐标</span></span><br><span class="line">        extra_joints = vertices2joints(<span class="variable language_">self</span>.J_regressor_extra, smpl_output.vertices)</span><br><span class="line">        joints = torch.cat([smpl_output.joints, extra_joints], dim=<span class="number">1</span>)</span><br><span class="line">        joints = joints[:, <span class="variable language_">self</span>.joint_map, :]</span><br><span class="line">        output = ModelOutput(vertices=smpl_output.vertices,</span><br><span class="line">                             global_orient=smpl_output.global_orient,</span><br><span class="line">                             body_pose=smpl_output.body_pose,</span><br><span class="line">                             joints=joints,</span><br><span class="line">                             betas=smpl_output.betas,</span><br><span class="line">                             full_pose=smpl_output.full_pose)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;smplify-从二维坐标到人体smpl和关节转动&quot;&gt;SMPLify-从二维坐标到人体SMPL和关节转动&lt;/h2&gt;
&lt;p&gt;之前我一直在SMPLify看代码，确实运行起来了，但是速度太慢了，可能因为用的优化器是chumdy，实际上只所以慢是因为初值不好，用深度学习的</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/30/SMPL%E9%AA%A8%E9%AA%BC%E8%92%99%E7%9A%AE%E7%AE%97%E6%B3%95/"/>
    <id>http://outbreak-sen.github.io/2025/06/30/SMPL%E9%AA%A8%E9%AA%BC%E8%92%99%E7%9A%AE%E7%AE%97%E6%B3%95/</id>
    <published>2025-06-30T15:16:21.996Z</published>
    <updated>2025-07-02T06:37:48.870Z</updated>
    
    <content type="html"><![CDATA[<h1>骨骼蒙皮算法</h1><p>smpl是指2015 马普的一篇文章“SMPL: a skinned multi-person linear model&quot;</p><p>人体可以理解为是一个基础模型和在该模型基础上进行形变的总和，在形变基础上进行PCA，得到刻画形状的低维参数——形状参数（shape）；同时，使用运动树表示人体的姿势，即运动树每个关节点和父节点的旋转关系，该关系可以表示为三维向量，最终每个关节点的局部旋转向量构成了smpl模型的姿势参数(pose)。</p><p><img src="image-20250701154125167.png" alt="image-20250701154125167"></p><h2 id="basicmodel">basicModel</h2><p>官方的basicModel_m_lbs_10_207_0_v1.0.0.pkl包含了[‘v_template’, ‘weights’, ‘posedirs’, ‘pose’, ‘trans’, ‘shapedirs’, ‘betas’, ‘J’]</p><p>v_template：smpl的基础模型，是一个T-pose，产生的mesh是在T-Pose基础上形变而来</p><p>weights：6890 * 24 混合权重矩阵，即关节点对顶点的影响权重 (第几个顶点受哪些关节点的影响且权重分别为多少) 6890个顶点，，每一个顶点受到24个关节点的影响</p><p>posedirs：6890 * 207 * 3  23 x 9 =207所有207个姿势混合形状组成的矩阵 (由姿势引起位移的pca)</p><p>pose：</p><p>trans：观察这个模型的远近？</p><p>shapedirs:    6890 * 3 * 10  形状位移矩阵PCA (由体型引起的位移形状位移的PCA)</p><p>betas：体型pca对应的形状参数1 * 10</p><p>J：从渲染mesh获得每个关节点的三维坐标的regressor</p><p><strong>之前我一直认为是现有的三维关节然后在关节的基础上堆肉最终获得mesh，但是实际上是先根据每个关节旋转参数获得mesh然后利用J获得三维关节坐标</strong></p><h2 id="渲染算法">渲染算法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">full_pose = torch.cat([global_orient, body_pose], dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 如何根据smpl的参数获得smpl的mesh和关节</span></span><br><span class="line">vertices, joints = lbs(betas, </span><br><span class="line">                       full_pose, </span><br><span class="line">                       <span class="variable language_">self</span>.v_template,</span><br><span class="line">                       <span class="variable language_">self</span>.shapedirs, </span><br><span class="line">                       <span class="variable language_">self</span>.posedirs,</span><br><span class="line">                       <span class="variable language_">self</span>.J_regressor,</span><br><span class="line">                       <span class="variable language_">self</span>.parents,</span><br><span class="line">                       <span class="variable language_">self</span>.lbs_weights, </span><br><span class="line">                       pose2rot=pose2rot, </span><br><span class="line">                       dtype=<span class="variable language_">self</span>.dtype)</span><br></pre></td></tr></table></figure><p>betas:       体型pca对应的形状参数1 * 10</p><p>global_orient: 1 * 3 根节点旋转向量</p><p>body_pose:   23 * 3 关节点旋转向量</p><p>shapedirs:    6890 * 3 * 10  形状位移矩阵PCA (由体型引起的位移形状位移的PCA)</p><p>posedirs:     6890 * 207 * 3  23 x 9 =207所有207个姿势混合形状组成的矩阵 (由姿势引起位移的pca)</p><p>j_regressor:   6890 * 24  , 是从不同的人在不同的姿势的例子中学习回归矩阵，从mesh中回归出关节点</p><p>parents:      24 每一个节点的父节点，显然根节点没有父节点</p><p>lbs_weights:  6890 * 24 混合权重矩阵，即关节点对顶点的影响权重 (第几个顶点受哪些关节点的影响且权重分别为多少) 6890个顶点，，每一个顶点受到24个关节点的影响</p><h3 id="渲染步骤">渲染步骤</h3><p>第一步：体型带来的位移偏差  图b</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v_shaped = v_template + blend_shapes(betas, shapedirs)</span><br><span class="line">J = vertices2joints(J_regressor, v_shaped)   获得各个关节的位置</span><br></pre></td></tr></table></figure><p>第二步：pose带来的位移偏差（影响很轻微，考虑到速度的时候可以舍弃）图c</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pose_feature = batch_rodrigues (full_pose)#旋转向量转旋转矩阵</span><br><span class="line">pose_offsets = torch.matmul(pose_feature, posedirs)</span><br><span class="line">v_posed = pose_offsets + v_shaped</span><br></pre></td></tr></table></figure><p>第三步：基于运动树进行关节点变换 图d</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">J_transformed, A = batch_rigid_transform(rot_mats, J, parents, dtype=dtype)</span><br></pre></td></tr></table></figure><p>J_transformed: 应用姿势旋转后关节的位置   24 * 3</p><p>A: 所有其他节点相对根节点的刚体变换矩阵  24* 3 * 3</p><p>第四步：线性蒙皮算法</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">T=  lbs_weights * A  ：6890 * 3 * 3 得到每一个顶点3*3</span><br><span class="line">v_homo =T * v_posed  得到最终的6890 * 3 mesh顶点</span><br><span class="line">J_transformed： 最终的关节点</span><br></pre></td></tr></table></figure><p>第五步：有位移考虑位移</p><pre><code>if apply_trans:joints += transl.unsqueeze(dim=1)vertices += transl.unsqueeze(dim=1)</code></pre><h2 id="渲染代码详解">渲染代码详解</h2><ol><li><p>torch版本</p></li><li><p>smplx代码版本</p></li><li><p>easymocap方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">blend_shapes</span>(<span class="params">betas, shape_disps</span>):</span><br><span class="line">blend_shape = torch.einsum(<span class="string">&#x27;bl,mkl-&gt;bmk&#x27;</span>, [betas, shape_disps])</span><br><span class="line">    <span class="keyword">return</span> blend_shape</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vertices2joints</span>(<span class="params">J_regressor, vertices</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.einsum(<span class="string">&#x27;bik,ji-&gt;bjk&#x27;</span>, [vertices, J_regressor])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27; Performs Linear Blend Skinning with the given shape and pose parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        betas : torch.tensor BxNB</span></span><br><span class="line"><span class="string">            The tensor of shape parameters</span></span><br><span class="line"><span class="string">        pose : torch.tensor Bx(J + 1) * 3</span></span><br><span class="line"><span class="string">            The pose parameters in axis-angle format</span></span><br><span class="line"><span class="string">        v_template torch.tensor BxVx3</span></span><br><span class="line"><span class="string">            The template mesh that will be deformed</span></span><br><span class="line"><span class="string">        shapedirs : torch.tensor 1xNB</span></span><br><span class="line"><span class="string">            The tensor of PCA shape displacements</span></span><br><span class="line"><span class="string">        posedirs : torch.tensor Px(V * 3)</span></span><br><span class="line"><span class="string">            The pose PCA coefficients</span></span><br><span class="line"><span class="string">        J_regressor : torch.tensor JxV</span></span><br><span class="line"><span class="string">            The regressor array that is used to calculate the joints from</span></span><br><span class="line"><span class="string">            the position of the vertices</span></span><br><span class="line"><span class="string">        parents: torch.tensor J</span></span><br><span class="line"><span class="string">            The array that describes the kinematic tree for the model</span></span><br><span class="line"><span class="string">        lbs_weights: torch.tensor N x V x (J + 1)</span></span><br><span class="line"><span class="string">            The linear blend skinning weights that represent how much the</span></span><br><span class="line"><span class="string">            rotation matrix of each part affects each vertex</span></span><br><span class="line"><span class="string">        pose2rot: bool, optional</span></span><br><span class="line"><span class="string">            Flag on whether to convert the input pose tensor to rotation</span></span><br><span class="line"><span class="string">            matrices. The default value is True. If False, then the pose tensor</span></span><br><span class="line"><span class="string">            should already contain rotation matrices and have a size of</span></span><br><span class="line"><span class="string">            Bx(J + 1)x9</span></span><br><span class="line"><span class="string">        dtype: torch.dtype, optional</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns</span></span><br><span class="line"><span class="string">        -------</span></span><br><span class="line"><span class="string">        verts: torch.tensor BxVx3</span></span><br><span class="line"><span class="string">            The vertices of the mesh after applying the shape and pose</span></span><br><span class="line"><span class="string">            displacements.</span></span><br><span class="line"><span class="string">        joints: torch.tensor BxJx3</span></span><br><span class="line"><span class="string">            The joints of the model</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch是一次性渲染多个mesh准备的</span></span><br><span class="line">batch_size = <span class="built_in">max</span>(betas.shape[<span class="number">0</span>], pose.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 体型偏差</span></span><br><span class="line">v_shaped = v_template + blend_shapes(betas, shapedirs)</span><br><span class="line"><span class="comment"># 如果不想要体型偏差 </span></span><br><span class="line">v_shaped = v_template.unsqueeze(<span class="number">0</span>).expand(batch_size, -<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pose偏差</span></span><br><span class="line">rot_mats = batch_rodrigues(pose.view(-<span class="number">1</span>, <span class="number">3</span>), dtype=dtype).view([batch_size, -<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line">ident = torch.eye(<span class="number">3</span>, dtype=dtype, device=device)</span><br><span class="line">pose_feature = (rot_mats[:, <span class="number">1</span>:, :, :] - ident).view([batch_size, -<span class="number">1</span>])</span><br><span class="line">pose_offsets = torch.matmul(pose_feature, posedirs).view(batch_size, -<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">v_posed = pose_offsets + v_shaped</span><br><span class="line"><span class="comment"># 如果不想要pose偏差</span></span><br><span class="line">v_posed = v_shaped</span><br><span class="line"></span><br><span class="line">J_transformed, A = batch_rigid_transform(rot_mats, J, parents, dtype=dtype)</span><br><span class="line"><span class="comment"># 5. Do skinning:</span></span><br><span class="line"><span class="comment"># W is N x V x (J + 1)</span></span><br><span class="line">W = lbs_weights.unsqueeze(dim=<span class="number">0</span>).expand([batch_size, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"><span class="comment"># (N x V x (J + 1)) x (N x (J + 1) x 16)</span></span><br><span class="line">num_joints = J_regressor.shape[<span class="number">0</span>]</span><br><span class="line">T = torch.matmul(W, A.view(batch_size, num_joints, <span class="number">16</span>)).view(batch_size, -<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">homogen_coord = torch.ones([batch_size, v_posed.shape[<span class="number">1</span>], <span class="number">1</span>],dtype=dtype, device=device)</span><br><span class="line">v_posed_homo = torch.cat([v_posed, homogen_coord], dim=<span class="number">2</span>)</span><br><span class="line">v_homo = torch.matmul(T, torch.unsqueeze(v_posed_homo, dim=-<span class="number">1</span>))</span><br><span class="line">verts = v_homo[:, :, :<span class="number">3</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;骨骼蒙皮算法&lt;/h1&gt;
&lt;p&gt;smpl是指2015 马普的一篇文章“SMPL: a skinned multi-person linear model&amp;quot;&lt;/p&gt;
&lt;p&gt;人体可以理解为是一个基础模型和在该模型基础上进行形变的总和，在形变基础上进行PCA，得到刻画形</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/28/MVS%E7%9A%84mindspore%E6%96%B9%E6%A1%88-%E5%BC%80%E6%BA%90%E4%B9%8B%E5%A4%8F%E9%A1%B9%E7%9B%AE/"/>
    <id>http://outbreak-sen.github.io/2025/06/28/MVS%E7%9A%84mindspore%E6%96%B9%E6%A1%88-%E5%BC%80%E6%BA%90%E4%B9%8B%E5%A4%8F%E9%A1%B9%E7%9B%AE/</id>
    <published>2025-06-28T13:45:48.050Z</published>
    <updated>2025-06-28T14:01:29.149Z</updated>
    
    <content type="html"><![CDATA[<h1>MVS的mindspore方案-开源之夏项目</h1><h2 id="目标">目标</h2><p>基于MindSpore框架搭建一种基于深度学习的多视角立体匹配技术，允许借鉴开源方法，但是需要展示pipeline的创新处，最终需要在ETH3D数据集上进行在线评估，将评估结果与SOTA方法进行对比。</p><h2 id="资源">资源</h2><ol><li>IGEV-MVS有ETH3D的dataset使用</li><li>mindspore官网有eppmvsnet</li><li>Effi-mvsnet可以问，代码熟悉，不如就这个吧</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;MVS的mindspore方案-开源之夏项目&lt;/h1&gt;
&lt;h2 id=&quot;目标&quot;&gt;目标&lt;/h2&gt;
&lt;p&gt;基于MindSpore框架搭建一种基于深度学习的多视角立体匹配技术，允许借鉴开源方法，但是需要展示pipeline的创新处，最终需要在ETH3D数据集上进行在线评估，将评</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/24/TMUX%E4%BD%BF%E7%94%A8/"/>
    <id>http://outbreak-sen.github.io/2025/06/24/TMUX%E4%BD%BF%E7%94%A8/</id>
    <published>2025-06-24T13:09:57.404Z</published>
    <updated>2025-06-24T13:10:29.080Z</updated>
    
    <content type="html"><![CDATA[<h1>TMUX使用</h1><p>tmux new -s your-session-name</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tmux detach</span><br><span class="line">ctrl+b d</span><br></pre></td></tr></table></figure><p>tmux kill-session -t your-session-name</p><p>tmux attach -t your-session-name</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;TMUX使用&lt;/h1&gt;
&lt;p&gt;tmux new -s your-session-name&lt;/p&gt;
&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;lin</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/24/Bark%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/"/>
    <id>http://outbreak-sen.github.io/2025/06/24/Bark%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</id>
    <published>2025-06-24T09:26:12.573Z</published>
    <updated>2025-06-24T13:08:59.791Z</updated>
    
    <content type="html"><![CDATA[<h1>Bark模型微调</h1><h1>TTS模型/文本到语音（TTS）生成模型</h1><p>以下开放的、已支持训练的 TTS 模型：</p><table><thead><tr><th>模型名</th><th>是否支持训练</th><th>特点</th><th>架构类型</th></tr></thead><tbody><tr><td><code>SpeechT5</code>（HuggingFace）</td><td>✅</td><td>支持 TTS、VC、ASR，多任务训练</td><td></td></tr><tr><td><code>Coqui-TTS</code>（强烈推荐）</td><td>✅</td><td>支持 Tacotron2/FastSpeech2/VITS</td><td></td></tr><tr><td><code>ESPnet</code>（工业级框架）</td><td>✅</td><td>支持多种语音模型，配置稍复杂</td><td></td></tr><tr><td><code>YourTTS</code>, <code>VITS</code>, <code>GlowTTS</code></td><td>✅</td><td>Coqui-TTS/ESPnet 中均支持</td><td></td></tr><tr><td>VITS</td><td></td><td></td><td>VAE + GAN + flow</td></tr><tr><td>Bark</td><td></td><td></td><td>GPT decoder-only</td></tr><tr><td>Tacotron2</td><td></td><td></td><td>encoder-decoder + attention</td></tr></tbody></table><h1>EnCodec 解码器</h1><p><strong>EnCodec</strong> 是由 Facebook FAIR 开源的 <strong>神经压缩音频模型</strong>，可以高效地将音频压缩成离散 token，再解压回高质量语音。</p><p>类似于 VQ-VAE（Vector Quantized Variational AutoEncoder）结构</p><ul><li><p>输入：Bark 生成的离散音频 token</p></li><li><p>输出：PCM waveform（可以保存为 <code>.wav</code>）</p></li></ul><h1>Bark模型</h1><p>由 <a href="https://zhida.zhihu.com/search?content_id=256263879&amp;content_type=Article&amp;match_order=1&amp;q=Suno+%E5%85%AC%E5%8F%B8&amp;zhida_source=entity">Suno 公司</a>开发的一款基于 transformer 的文本到音频模型，多种语言，可以模拟非语言交流，如笑声、叹息和哭泣，提供了两个不同的模型大小（small 和 large）核心是一个 <strong>预训练的 GPT-style decoder-only 模型</strong>。</p><ul><li><p><strong>输入是：文本 + 音色信息（语者 embedding），</strong></p></li><li><p><strong>输出是：EnCodec（Facebook FAIR 开源的神经音频编码器）格式的离散音频 token</strong></p></li><li><p><strong>Bark 的最终输出是使用 <code>EnCodec</code> 解码器把这些 token 解码为语音。</strong></p></li></ul><p>Suno 只开源了推理流程，没有发布训练脚本或训练数据预处理方式：</p><ul><li><p>没有训练 loss 定义</p></li><li><p>没有 tokenizer 训练方法</p></li><li><p>没有 codec token 的 ground truth 标签准备方式</p></li><li><p>不能直接把音频作为 <code>labels</code> 喂给模型。你需要先把音频转成 codec token</p></li><li><pre><code class="language-python">from encodec import EncodecModelencodec = EncodecModel()token_ids = encodec.encode(audio)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">## 使用</span><br><span class="line"></span><br><span class="line">~~~python</span><br><span class="line">from transformers import AutoProcessor, BarkModel</span><br><span class="line">import torch</span><br><span class="line">import scipy</span><br><span class="line"></span><br><span class="line">processor = AutoProcessor.from_pretrained(&quot;suno/bark-small&quot;, trust_remote_code=True)</span><br><span class="line">model = BarkModel.from_pretrained(&quot;suno/bark-small&quot;)</span><br><span class="line"></span><br><span class="line">inputs = processor(</span><br><span class="line">    text=[&quot;你好，我是 Bark 模型生成的语音[zh] 你好，我是来自中国的语音。[en_speaker_4] Hello, I am speaking with emotion.[laughs] Wow, this is amazing!&quot;],</span><br><span class="line">    return_tensors=&quot;pt&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">speech = model.generate(**inputs)</span><br><span class="line">scipy.io.wavfile.write(&quot;bark_out.wav&quot;, rate=model.generation_config.sample_rate, data=speech.cpu().numpy().squeeze())# 时间长</span><br><span class="line"></span><br></pre></td></tr></table></figure></code></pre></li></ul><h2 id="bark不能直接微调">Bark不能直接微调</h2><p>Bark 的输入输出不是传统形式，属于<strong>非端到端 TTS</strong></p><p>Bark 模型本身是一个大规模、冻结的 Transformer 模型，Hugging Face 官方没有发布 Bark 可训练版本，并且没有forward函数，只有generate函数，也没有提供微调脚本。这种方式不能微调成功，而没有实现 <code>forward()</code> 来返回 <code>loss</code>，这导致它无法适配 <code>Trainer</code> 或反向传播。Trainer 会直接报错或 silently fail。</p><h1>Bark模型的三个阶段可以分别微调</h1><p>一个barkmodel分为&quot;BarkFineModel&quot;,    “BarkSemanticModel”,    “BarkCoarseModel”，都有forward函数</p><p>理论上是可以单独微调这三个子模型的</p><p>但是<strong>只训练 Bark 的 Semantic 模型（<code>BarkSemanticModel</code>）完全可行</strong>，而且这是目前在 Hugging Face 上最可控、最推荐的 Bark 微调路径。</p><table><thead><tr><th>模块名</th><th>对应类</th><th>输入</th><th>输出</th><th>功能描述</th><th>数据怎么来？</th></tr></thead><tbody><tr><td><strong>Semantic</strong></td><td><code>BarkSemanticModel</code></td><td>文本 token</td><td>semantic token</td><td>生成语义级别的表示（类似意图/句子意义）</td><td>semantic</td></tr><tr><td><strong>Coarse</strong></td><td><code>BarkCoarseModel</code></td><td>semantic token</td><td>coarse audio token</td><td>生成粗粒度音频 token（频谱级别）</td><td>原生 Bark 的 <code>generate_coarse()</code></td></tr><tr><td><strong>Fine</strong></td><td><code>BarkFineModel</code></td><td>coarse token</td><td>fine audio token</td><td>精细音频 token，接近最终波形</td><td>原生 Bark 的 <code>generate_fine()</code></td></tr><tr><td>EnCodec（非 Transformer）</td><td>-</td><td>fine token</td><td>waveform</td><td>用 EnCodec 解码器合成音频</td><td></td></tr></tbody></table><p><strong>用 EnCodec 编码器</strong>对音频提取出：</p><ul><li>semantic token</li><li>coarse token</li><li>fine token</li></ul><p>分阶段训练这三个子模型：</p><ul><li>Semantic：用 <code>text_token → semantic_token</code></li><li>Coarse：用 <code>semantic_token → coarse_token</code></li><li>Fine：用 <code>coarse_token → fine_token</code></li></ul><h1>audioLM</h1><p><a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=audioLM&amp;zhida_source=entity">audioLM</a>作为音频领域的重要论文，却很少有人解读，无论是<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=spearTTS&amp;zhida_source=entity">spearTTS</a>，还是<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=VALL-E&amp;zhida_source=entity">VALL-E</a>，以及新鲜出炉的<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=bark&amp;zhida_source=entity">bark</a>，核心思路都源于audioLM。这篇来源于google的方法，看框架像是SoundSream和<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=w2v-BERT&amp;zhida_source=entity">w2v-BERT</a>的结合版。音频分别通过上述2个模型获取semantic tokens, 代表语义，and acoustic tokens  代表声学信息。这里tokens都是离散值。为什么要离散，因为这样才便于使用语言模型或GPT那套建模方式，同时也去除了冗余信息，减小了模型的建模难度。</p><ol><li><p>跟文本数据相比，音频的数据速率（1s音频包含24000个浮点值，而其对应的文本可能就4-5字）要高得多，从而导致序列更长，所以要变成离散值才能训练</p></li><li><p>其次，文本和音频之间存在一对多的关系。这意味着同一句话可以被不同的说话者用不同的说话风格、不同的情感甚至不同的环境下来表达。</p></li></ol><p>从自监督音频模型 w2v-BERT 中提取的语义标记，不仅能够捕获音频中的局部依赖（例如，语音中的音素、钢琴音乐中的旋律）和全局长期结构（例如，句法和语义内容、钢琴音乐中的和声和节奏），还能对音频进行320倍的降维，这个模型解决了上述挑战1</p><p>但是如果直接用semantic token去生成音频，效果并不好，这是音频还存在一对多的问题。因此，引入了<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=SoundStream&amp;zhida_source=entity">SoundStream</a>提取额外的声学信息token，该token中可以包含发音人的音色信息，情感特征等。</p><h2 id="训练分为了3个阶段">训练分为了3个阶段</h2><p><img src="image-20250624175709278.png" alt="image-20250624175709278"></p><p>第一阶段只有语义token参与自回归方式的训练。</p><p>第二阶段声学token自回归，额外还加入上一阶段生成的语义token作为condition。输出的目标为声学token。</p><p>第三阶段也为自回归，使用声学模型处理粗略的声学token，类似<a href="https://zhida.zhihu.com/search?content_id=227672531&amp;content_type=Article&amp;match_order=1&amp;q=tacotron&amp;zhida_source=entity">tacotron</a>的后处理网络postnet，这样可以在最终音频添加更多细节。</p><h1>Encodec: 实现音频数据的高效压缩</h1><p>语音大模型，第一个要提的就是<a href="https://zhida.zhihu.com/search?content_id=237615057&amp;content_type=Article&amp;match_order=1&amp;q=VALL-E&amp;zhida_source=entity">VALL-E</a>。VALL-E的一个非常重要的前置工作就是Meta的Encodec</p><p>音频数据要想实现实时传输，那我们就必须对其实现高度的压缩。但是，高度压缩又会不可避免地损坏音频数据的质量，造成信息的失真，也就是保真度下降。这其实就是音频数据压缩效率和质量的矛盾。我们在设计音频数据压缩算法时，就需要在压缩效率和压缩质量上做一个取舍。本文的工作解决的就是这个问题。<strong>工作的核心思想在于使用神经网络来实现这一压缩过程。</strong></p><p>论文中设计的神经网络采样了典型的编解码器架构，带有一个Encoder和一个Decoder。为了实现量化的过程，Encoder和Decoder之间插入了一个<a href="https://zhida.zhihu.com/search?content_id=237615057&amp;content_type=Article&amp;match_order=1&amp;q=Quantizer&amp;zhida_source=entity">Quantizer</a>(量化器)。</p><p><img src="image-20250624180051444.png" alt="image-20250624180051444"></p><h1>附录</h1><ul><li>voice preset是什么</li></ul><p>voice_preset字段的值来指定具体的音色。“{语言}<em>spearker</em>{n}”、“v2/{语言}<em>spearker</em>{n}”</p><h1></h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Bark模型微调&lt;/h1&gt;
&lt;h1&gt;TTS模型/文本到语音（TTS）生成模型&lt;/h1&gt;
&lt;p&gt;以下开放的、已支持训练的 TTS 模型：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;模型名&lt;/th&gt;
&lt;th&gt;是否支持训练&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;t</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/20/FoundationStereo/"/>
    <id>http://outbreak-sen.github.io/2025/06/20/FoundationStereo/</id>
    <published>2025-06-20T03:56:23.535Z</published>
    <updated>2025-06-20T11:04:44.848Z</updated>
    
    <content type="html"><![CDATA[<h1>FoundationStereo</h1><p>NVIDIA 2025</p><h2 id="摘要-有本事别微调刷榜-我就是要做zero-shot">摘要（有本事别微调刷榜，我就是要做zero-shot）</h2><p>通过每个域的微调，深度立体匹配在基准数据集上取得了巨大进步。然而，实现强大的零镜头泛化（其他计算机视觉任务中基础模型的标志）对于立体匹配来说仍然具有挑战性。为此，我们首先构建了一个大规模（1M 立体对）合成训练数据集，具有较大的多样性和高照片级真实感，然后是一个自动自我管理管道来去除模棱两可的样本。然后，我们设计了许多网络架构组件来增强可扩展性，包括一个侧调功能主干，它适应了视觉基础模型中丰富的单目先验，以减轻 sim-to-real 的差距，以及用于有效成本量过滤的远程上下文推理。</p><h2 id="引言">引言</h2><p>近半个世纪前第一个立体匹配算法出现，我们已经走过了漫长的道路。，最近的立体算法可以取得惊人的结果，几乎使最具挑战性的基准测试饱和。然而，对目标域的数据集进行微调仍然是获得竞争结果的首选方法。鉴于通过scaling law在计算机视觉中的其他问题上表现出的零样本泛化能力，是什么阻止了立体匹配算法实现类似的泛化水平？要么是由于网络架构中的结构不足，要么是由于训练数据贫乏，或者两者兼而有之。这些网络通常在 Scene Flow [43] 上进行实验，这是一个相当小的数据集，只有 40K 个带注释的训练图像对。因此，这些方法都不能用作现成的解决方案，而不是在其他任务中出现的视觉基础模型的强大泛化性。</p><p>提出了 FoundationStereo，这是一个用于立体深度估计的大型基础模型，无需逐域微调即可实现强大的零镜头泛化。</p><ul><li>创建了一个大规模 （1M） 高保真合成数据集，用于立体学习，具有高多样性和照片级真实感;以及一个自我管理管道，以确保删除不良样本。开发了一个自动自我管理管道，以消除在域随机数据生成过程中不可避免地引入的模棱两可的样本，从而提高数据集质量和迭代更新的模型鲁棒性。</li><li>提出了 FoundationStereo，这是一种零镜头可推广的立体匹配模型，与之前在目标域上微调的工作相比，它取得了相当甚至更有利的结果;当应用于野生数据时，它还明显优于现有方法。</li><li>为了利用包含丰富语义和几何先验的互联网规模知识，我们提出了一种侧调谐适配器 （STA），它将基于 ViT 的单眼深度估计模型 [79] 适应立体设置。</li><li>我们开发了 Attentive Hybrid Cost Filtering （AHCF），其中包括一个带有 3D AxialPlanar Convolution （APC） 的沙漏模块以及一个 Disparity Transformer （DT） 模块，该模块在视差维度上执行完全的自我关注。</li></ul><h2 id="性能">性能</h2><h3 id="评估的数据集">评估的数据集</h3><ul><li>Scene Flow [43]：合成数据集，包括 FlyingThings3D、Driving 和 Monkaa 三个子集。</li><li>Middlebury [51]：室内立体图像对，通过结构光获取高质量地面真实视差。除非另有说明，评估在半分辨率和非遮挡区域进行。</li><li>ETH3D [52]：提供涵盖室内外场景的灰度立体图像对。</li><li>KITTI 2012 [20] 和 KITTI 2015 [45]：真实驾驶场景数据集，提供由激光雷达传感器获得的稀疏地面真实视差图。</li></ul><h3 id="指标："><strong>指标</strong>：</h3><ul><li><strong>EPE</strong>：计算平均每像素视差误差。</li><li><strong>BP-X</strong>：计算视差误差大于 X 像素的像素百分比。</li><li><strong>D1</strong>：计算视差误差大于 3 像素且超过地面真实视差 5% 的像素百分比。</li></ul><h3 id="零样本泛化能力比较">零样本泛化能力比较</h3><p>仅在 Scene Flow 上训练，我们的方法在所有数据集上均持续优于对比方法，Middlebury ，ETH3D ，KITTI 2012 和 KITTI 2015</p><h3 id="野外泛化能力">野外泛化能力</h3><p>没写结果，只有定型展示，看起来缺失吊</p><p><img src="image-20250620185909441.png" alt="image-20250620185909441"></p><h3 id="领域内比较-就是微调之后的">领域内比较（就是微调之后的）</h3><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>Scene Flow</td><td>FoundationStereo 显著优于对比方法，将先前最佳 EPE 从 0.41 降至 0.33。尽管本工作不聚焦领域内训练，结果仍反映了模型设计的有效性。</td><td></td></tr><tr><td>ETH3D</td><td>在默认训练数据集（4.1节）和ETH3D训练集的混合数据上对基础模型进行额外5万步微调，使用相同的学习率计划和数据增强。我们的模型显著超越先前最佳方法，误差率降低超50%，提交时在排行榜位列第1名。<br />我们的微调模型在 ETH3D 排行榜上排名第一，明显优于已发表和未发表的作品。</td><td></td></tr><tr><td>Middlebury</td><td>我们的微调模型在 Middlebury 排行榜上排名第一，明显优于已发表和未发表的作品。</td><td></td></tr><tr><td>KITTI 2015</td><td></td><td></td></tr><tr><td>KITTI 2012</td><td></td><td></td></tr><tr><td>Translucent Objects</td><td>是一个由镜面和透明物体组成的具有挑战性的数据集。超过了IGEV</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p><img src="image-20250620124656632.png" alt="image-20250620124656632"></p><h2 id="相关工作里提到了数据集">相关工作里提到了数据集</h2><p>立体匹配训练数据。训练数据对于深度学习模型至关重要。KITTI 12 [20] 和 KITTI 15 [45] 提供了数百个驾驶场景训练对。DrivingStereo [76] 进一步扩展到 180K 立体声对。然而，LiDAR 传感器获得的稀疏真实值差异阻碍了学习准确和密集的立体匹配。Middlebury [51] 和 ETH3D [52] 开发了少量的训练数据，涵盖驾驶以外的室内和室外场景。Booster [48] 提供了一个专注于透明物体的真实数据集。InStereo2K [2] 提供了一个更大的训练数据集，该数据集由 2K 立体对组成，具有更密集的地面实况视差，使用结构光系统获得。然而，数据量稀缺、地面实况差异不完美以及现实世界中缺乏收集可扩展性等挑战推动了合成数据在训练中的广泛采用。这包括sceneFlow [43]、Sintel [6]、CREStereo [34]、IRS [64]、TartanAir [66]、FallingThings [61]、Virtual KITTI 2 [7]、CARLA HR-VS [75]、namic Replica [28]。在表 1 中，我们将我们提出的 FoundationStereo 数据集 （FSD， FoundationStereo dataset） 与常用的合成训练数据集进行了比较</p><p><img src="image-20250620131444002.png" alt="image-20250620131444002"></p><h2 id="相关工作里提到了vision-foundation-models">相关工作里提到了Vision Foundation Models</h2><p>视觉基础模型在 2D、3D 和多模态对齐的各种视觉任务中取得了显著进展。<strong>CLIP</strong> [47] 利用大规模图像 - 文本对训练来对齐视觉和文本模态，实现零样本分类并促进跨模态应用。<strong>DINO</strong> 系列 [8,38,46] 采用自监督学习进行密集表示学习，有效捕捉分割和识别任务的关键细节特征。<strong>SAM</strong> 系列 [32,50,77] 在由点、边界框、语言等各种提示驱动的分割中表现出高度通用性。类似的进展也出现在 3D 视觉任务中。<strong>DUSt3R</strong> [65] 和 <strong>MASt3R</strong> [33] 提出了从未校准和未摆姿的相机进行密集 3D 重建的可泛化框架。<strong>FoundationPose</strong> [69] 开发了一种用于新物体 6D 位姿估计和跟踪的统一框架。与这项工作更相关的是，一些工作 [4,29,78,79] 在单目深度估计任务和多视图立体 [26] 中展示了强大的泛化能力。总之，这些方法在规模法则下表明，视觉基础模型正在发展为支持跨不同场景的鲁棒应用，而无需繁琐的按领域微调。</p><h2 id="方法">方法</h2><p><img src="image-20250620132516109.png" alt="image-20250620132516109"></p><p><img src="image-20250620132544164.png" alt="image-20250620132544164"></p><h3 id="单目基础模型适配-特征提取是一个vit-cnn">单目基础模型适配（特征提取是一个vit+cnn）</h3><p>为了让单目深度估计这样的基础模型能够适应立体匹配这个任务，需要对其进行微调训练，这里想做一个VIT结合CNN的方法（a） 在冻结的 DepthAnythingV2 [79] 中直接使用来自 DPT 头部的特征金字塔，而不使用 CNN 特征。（b） 通过在 CNN 和 ViT 之间交换特征来类似于 ViT 适配器 [12]。（c） 应用带有步幅 4 的 4 × 4 卷积，以在 DepthAnythingV2 最终输出头之前缩小特征。然后将该特征与相同级别的 CNN 特征连接起来，以获得 1/4 比例的混合特征，因此学习了侧 CNN 网络以使 ViT 特征 [83] 适应立体匹配任务。<strong>令人惊讶的是，虽然很简单，但我们发现 （c） 明显超过了立体匹配任务的替代选择</strong></p><p>（两个图像的分支fr，fl）具体而言，给定一对左右图像 I、I∈ R，我们使用 EdgeNeXt-S [40] 作为 STA 中的 CNN 模块来提取多级金字塔特征，其中 1/4 级特征配备了 DepthAnythingV2 特征。选择 EdgeNeXt-S [40] 是因为它的内存效率，而且在我们的研究中，更大的 CNN 主干没有产生额外的好处。</p><p>发到 DepthAnythingV2 时，我们首先将图像大小调整为可被 14 整除，以与其预训练的补丁大小保持一致。STA 权重在应用于左右视角共享。</p><p>（最小面红色的分支fc）同样，使用 STA 来提取上下文特征，不同的是 CNN 模块设计有一系列残差块和下采样层。它生成了多个尺度的上下文特征 {4， 8， 16分之一大小}，上下文特征 参与初始化 ConvGRU 块的隐藏状态，并在每次迭代时输入到 ConvGRU 块中，以逐步细化的上下文信息有效指导迭代过程。</p><p>我们没有使用具有尺度模糊性的 DepthAnythingV2 的原始单目深度，而是使用其潜在特征作为从立体图像和通过成本筛选进行比较。</p><h3 id="attentive-hybrid-cost-filtering注意力混合成本过滤">Attentive Hybrid Cost Filtering注意力混合成本过滤</h3><h4 id="hybrid-cost-volume-construction注意力混合成本过滤">Hybrid Cost Volume Construction注意力混合成本过滤</h4><p><img src="image-20250620185204196.png" alt="image-20250620185204196"></p><h4 id="axial-planar-convolution-apc-filtering轴平面卷积-apc-滤波">Axial-Planar Convolution (APC) Filtering轴平面卷积 （APC） 滤波</h4><p>就是说太大了，所以用了分离卷积</p><p><img src="image-20250620185310265.png" alt="image-20250620185310265"></p><h4 id="disparity-transformer-dt">Disparity Transformer (DT)</h4><p><img src="image-20250620185345400.png" alt="image-20250620185345400"></p><h4 id="initial-disparity-prediction">Initial Disparity Prediction.</h4><p><img src="image-20250620185423871.png" alt="image-20250620185423871"></p><h3 id="iterative-refinement">Iterative Refinement</h3><p><img src="image-20250620185440473.png" alt="image-20250620185440473"></p><h2 id="损失函数">损失函数</h2><p><img src="image-20250620184058020.png" alt="image-20250620184058020"></p><p>其中 d 表示真实视差; |·|smooth 表示光滑的 Lloss;k 是迭代编号;γ设置为 0.9，应用指数级增加的权重 [36] 来监督迭代细化的差异。</p><h2 id="训练">训练</h2><p>该数据集由我们提出的 FSD 以及 Scene Flow [43]、Sintel [6]、CREStereo [34]、FallingThings [61]、InStereo2K [2] 和 Virtual KITTI 2 [7] 组成（你这还不是永乐评估的数据集？哦，没有）。我们使用 AdamW 优化器 [39] 训练 FoundationStereo，训练 200K 步，总批量大小为 128，均匀分布在 32 个 NVIDIA A100 GPU 上。在整个训练过程中，学习率从 1e-4 开始，在 0.8 时衰减 0.1。图像在馈送到网络之前被随机裁剪为 320×736。执行类似于 [36] 的数据增强。在训练期间，GRU 更新中使用了 22 次迭代。</p><h2 id="synthetic-training-dataset合成训练数据集">Synthetic Training Dataset合成训练数据集</h2><p>使用 NVIDIA Omniverse 创建了一个大规模合成训练数据集。此 FoundationStereo 数据集 （FSD） 解决了关键的立体匹配挑战，例如反射、低纹理表面和严重遮挡。我们进行域随机化 [58] 以增强数据集的多样性，包括随机立体基线、焦距、相机视角、照明条件和对象配置。同时，利用具有丰富纹理和路径跟踪渲染的高质量 3D 资产来增强渲染和布局的真实感。包括结构化的室内和室外 场景，以及在复杂而逼真的照明下具有各种几何形状和纹理的更多样化的随机飞行物体。</p><p>右：迭代自我管理过程消除了领域随机合成数据生成过程中不可避免地产生的模棱两可的样本。示例歧义包括严重的纹理重复、周围环境有限时无处不在的反射，以及照明不当下的纯色。</p><p><img src="image-20250620132454681.png" alt="image-20250620132454681"></p><h3 id="迭代自筛选">迭代自筛选</h3><p>尽管理论上合成数据生成可通过随机化产生无限数据并实现高度多样性，但尤其在结构较少的飞行物体场景中，不可避免会引入模糊样本，干扰学习过程。为此，我们设计了自动迭代自筛选策略。图 4 展示了该过程及检测到的模糊样本。我们首先在 FSD 上训练初始版本的 FoundationStereo，然后在 FSD 上进行评估，将  BP-2（4.2 节）大于 60% 的样本视为模糊样本，并通过生成新样本替换。训练和筛选过程交替进行（本文迭代两次），以逐步更新 FSD 和  FoundationStereo。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;FoundationStereo&lt;/h1&gt;
&lt;p&gt;NVIDIA 2025&lt;/p&gt;
&lt;h2 id=&quot;摘要-有本事别微调刷榜-我就是要做zero-shot&quot;&gt;摘要（有本事别微调刷榜，我就是要做zero-shot）&lt;/h2&gt;
&lt;p&gt;通过每个域的微调，深度立体匹配在基准数据集上取</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/20/MonSter/"/>
    <id>http://outbreak-sen.github.io/2025/06/20/MonSter/</id>
    <published>2025-06-20T03:56:11.976Z</published>
    <updated>2025-06-20T11:04:47.846Z</updated>
    
    <content type="html"><![CDATA[<h1>MonSter</h1><p>华中科技大学2025</p><h2 id="上台之后首先讲清楚自己为什么要做单目深度估计-立体匹配的方式做立体匹配">上台之后首先讲清楚自己为什么要做单目深度估计+立体匹配的方式做立体匹配</h2><h3 id="立体匹配是什么">立体匹配是什么?</h3><p>立体匹配从校正的立体图像中估计视差图，随后可以将其转换为公制深度。是从相似性匹配中得出差异的，假设两张图像中都有可见的对应关系。</p><p>方法大致可分为基于成本筛选的方法和基于迭代优化（局部成本）的方法。</p><h3 id="立体匹配效果差在哪">立体匹配效果差在哪?</h3><p>立体匹配从图像对应中恢复深度。现有方法难以处理匹配线索有限的病态区域，例如遮挡、无纹理区域、重复/薄结构以及像素表示较低的远处对象。</p><h3 id="单目深度估计是什么">单目深度估计是什么?</h3><p>与立体匹配不同，单目深度估计直接从单个图像中恢复 3D，不会遇到不匹配的挑战。</p><h3 id="单目深度估计的效果差在哪？">单目深度估计的效果差在哪？</h3><p>虽然单眼深度为立体结构提供了互补的结构信息，但预训练模型通常会产生具有比例和偏移模糊性的相对深度。</p><p>即使在全局缩放和移位对齐之后，大量错误仍然存在</p><p><img src="image-20250620121158705.png" alt="image-20250620121158705"></p><p>红线表示的是理想情况下的视差匹配。蓝色点表示的是实际从单目深度估计得到的视差值。离红线越近的蓝色点，表示该位置的视差估计越准确；而离红线越远的蓝色点，则表示视差估计的误差越大。</p><ul><li><strong>(a) No Alignment</strong>：没有进行任何对齐的情况下，从单目深度估计得到的视差与地面真实视差之间的关系。可以看到，蓝色点分布较为分散，表明视差估计存在较大的误差。</li><li><strong>(b) Align with GT using global scale and global shift values</strong>：使用全局尺度和全局偏移值对单目深度进行对齐后，视差估计与地面真实视差的关系。此时，蓝色点更接近红线，说明对齐操作改善了视差估计的准确性。</li><li><strong>© Our Shift Refinement</strong>：使用每像素偏移精炼（per-pixel shift refinement）后的结果。可以看到，蓝色点进一步靠近红线，表明这种方法能更精确地估计视差，减少与地面真实视差的差距。</li></ul><h2 id="摘要">摘要</h2><p>基于置信度的指导自适应地选择可靠的立体声提示以进行单深度音阶偏移恢复。改进的 monodepth 反过来又可以有效地引导 Illposed 区域的立体声。这种迭代相互增强使 MonSter 能够将单深度先验从粗糙的对象级结构发展为像素级几何图形，从而充分释放立体匹配的潜力。</p><p>我们提出了 MonSter，这是一种将立体匹配分解为单眼深度估计和每像素尺度偏移恢复的新方法，它完全结合了单眼和立体算法的优势，并克服了缺乏匹配线索的限制。并通过立体引导对齐 （SGA，Stereo Guided Alignment ） 和单声道引导优化 （MGR，Mono Guided Refinement ） 模块自适应地融合它们。SGA 首先通过与立体视差进行全局对齐，将单深度重新缩放为“单眼视差”。然后，它使用条件引导的 GRU 自适应地选择可靠的立体提示，以更新每像素的单眼视差偏移。与 SGA 对称，MGR 使用优化的单眼视差作为条件，自适应地优化匹配失败区域中的立体视差。通过多次迭代，这两个分支有效地相辅相成：</p><p>1） 虽然在粗略的对象级别上是有益的，但直接和单向地将单深度融合到立体中受到比例偏移歧义的影响，这通常会在复杂区域（如倾斜或曲面）中引入噪声。使用立体优化单深度可以有效地解决这个问题，确保 MonSter 的稳健性。</p><p>2） 改进的单深度反过来为具有挑战性的区域的立体声提供了强有力的指导。例如，由于像素比例较小和匹配难度增加，立体匹配的深度感知能力会随着距离的增加而降低。</p><h2 id="性能">性能</h2><p>在五个最常用的基准上评估了 MonSter：KITTI 2012 [11]、KITTI 2015 [23]、ETH3D [28]、Middlebury [27] 和 Scene Flow [22]。</p><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>Scene Flow</td><td>tate-of-the-art performance with an EPE metric of 0.37.surpassing our baseline [38] by 21.28% and<br/>outperforming the SOTA method [35] by 15.91%.</td><td></td></tr><tr><td>ETH3D</td><td>完整的训练集由 BTS 和 ETH3D 训练集组成ranks 1st.与基线 IGEV 相比，我们在三个报告的指标中分别实现了 58.93%、52.32% 和 41.18% 的改进。图 5 所示的定性比较也显示出类似的趋势。值得注意的是，即使与之前的最佳方法 LoS 相比，我们也将 Bad 1.0 （Noc） 指标从 0.91 提高到 0.46，实现了 49.45% 的改进。</td><td></td></tr><tr><td>Middlebury</td><td>训练集是 BTS 和 Middlebury 训练集的组合. 在 RMSE 度量方面优于所有现有方法。也就是不是全参数最强<br />但是我也找不到你的名字</td><td></td></tr><tr><td>KITTI 2015</td><td>在 KITTI 2012 和 KITTI 2015 的混合数据集上对 Scene Flow 预训练模型进行了 50k 步的微调。所有指标的最佳性能</td><td></td></tr><tr><td>KITTI 2012</td><td>所有指标的最佳性能</td><td></td></tr></tbody></table><p><img src="image-20250620124656632.png" alt="image-20250620124656632"></p><h2 id="作者特意指出挑战区域的结果">作者特意指出挑战区域的结果</h2><p>反射区域：我们对 KITTI 2012 基准的反射区域进行了比较。MonSter 在 KITTI 2012 排行榜上的所有反射区域指标中排名第一。如表 3 所示，我们已将现有的 SOTA 提升到一个新的水平。</p><p>边缘和非边缘区域：立体匹配在边缘和低纹理区域面临挑战，我们的方法通过利用单眼深度的优势解决了这些问题。为了评估这些区域中的 MonSter，我们按照 [35] 使用 Canny 运算符将 Scene Flow 测试集分为边缘和非边缘区域。如表 4 所示，MonSter 在边缘区域和非边缘区域的表现分别比基线 [38] 高出 14.35% 和 24.39%。</p><p>远处背景：立体匹配与远处物体的深度感知无关，我们通过整合 SGA 和 MGR 模块来改进它。如表 2 所示，与 KITTI 2015 基准的基线相比，MonSter 将 D1-bg 指标提高了 18.12%。</p><h2 id="方法">方法</h2><p><img src="image-20250620123738029.png" alt="image-20250620123738029"></p><p>MonSter 由 1） 一个单眼深度分支、2） 一个立体匹配分支和 3） 一个相互细化模块组成</p><h3 id="单眼深度分支">单眼深度分支</h3><p>使用预训练的 DepthAnythingV2 [46] 作为单目深度分支，它使用 DINOv2 [24] 作为编码器，使用 DPT [26] 作为解码器。</p><h3 id="立体匹配分支">立体匹配分支</h3><p>IGEV [38] 获得初始立体 disp，但是对特征提取分量进行修改，</p><ol><li>如图 4 所示。为了高效、充分利用预训练的单目模型，立体分支与单目分支共享 DINOv2 中的 ViT 编码器，并冻结参数以防止立体匹配训练影响其泛化能力。</li><li>ViT 架构以单一分辨率提取特征，而最近的立体匹配方法通常利用四种比例（原始图像分辨率的 1/32、1/16、1/8 和 1/4）的多尺度特征。为了与 IGEV 完全一致，我们采用一堆 2D 卷积层，表示为特征传输网络，将 ViT 特征下采样并转换为金字塔特征的集合 F = {F， F， F， F}，其中 F∈ R 22。我们按照 IGEV 构建几何编码卷，并使用相同的 ConvGRU 进行迭代优化。</li><li>为什么是IGEV，因为他也是SOTA：</li><li>Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9% and 54.8% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks.</li><li><img src="image-20250620122736562.png" alt="image-20250620122736562"></li></ol><h3 id="相互细化模块">相互细化模块</h3><p>首先执行全局尺度偏移对齐，将单眼深度转换为视差图并将其粗略对齐立体声输出</p><p>然后我们迭代地执行双分支优化：</p><p>立体引导对齐 （SGA） 利用立体提示来更新单眼视差的每像素偏移;</p><p>单声道引导优化 （MGR） 在进一步优化立体视差之前利用对齐的单目深度。</p><h4 id="粗略对齐">粗略对齐</h4><p><img src="image-20250620123209747.png" alt="image-20250620123209747"></p><p>全局尺度最小二乘对齐</p><p>Ω表示立体视差值介于 20% 到 90% 之间的区域，按升序排序，这有助于过滤不可靠的区域，例如天空、极远的区域和近距离异常值。</p><p>下面这个D^0_M就是对齐之后的单目深度</p><h4 id="立体引导对齐-sga">立体引导对齐 （SGA）</h4><p>利用立体匹配中的线索来更新单目深度估计中的每个像素的偏移量，从而更准确地对齐单目深度与立体视差。</p><p><img src="03975b923fb545b7b92d39ef4bf12ed1.png" alt="img"></p><p><img src="https://i-blog.csdnimg.cn/direct/68c7c54c1c1a4d4cbbf2ab3bd5380fa2.png" alt="img"></p><p><img src="image-20250620125429303.png" alt="image-20250620125429303"></p><h4 id="单目引导优化-mgr">单目引导优化 （MGR）</h4><p><img src="image-20250620125453877.png" alt="image-20250620125453877"></p><p><img src="6b2c352532954137ae215e903c6414a5.png" alt="img"></p><h3 id="损失函数">损失函数</h3><p><img src="image-20250620124109785.png" alt="image-20250620124109785"></p><p>使用 L1 损失来监督两个分支的输出。我们将立体声分支前 N 次迭代的差异集表示为 {d}^{N-1}_{i=0}，并跟随Raft的思路，随着迭代次数的增加而呈指数级增加权重。总损耗定义为单眼支损耗 Land 与立体支损耗 Las 之和，</p><h2 id="训练">训练</h2><p>NVIDIA RTX 3090 GPU 进行实验。我们使用 AdamW [21] 优化器，</p><p>遵循标准 [16， 17， 35]，对于大多数实验，我们在 Scene Flow [22] 上预训练 MonSter。为了在 ETH3D 和 Middlebury 上进行微调，我们遵循 SOTA 方法 [16， 17， 35] 从各种公共数据集中创建基础训练集 （BTS） 进行预训练，包括 Scene Flow [22]、CREStereo [16]、Tartan Air [34]、Sintel Stereo [3]、FallingThings [32] 和 InStereo2k [2]。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;MonSter&lt;/h1&gt;
&lt;p&gt;华中科技大学2025&lt;/p&gt;
&lt;h2 id=&quot;上台之后首先讲清楚自己为什么要做单目深度估计-立体匹配的方式做立体匹配&quot;&gt;上台之后首先讲清楚自己为什么要做单目深度估计+立体匹配的方式做立体匹配&lt;/h2&gt;
&lt;h3 id=&quot;立体匹配是什么&quot;&gt;立体</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/02/PP-LCNet/"/>
    <id>http://outbreak-sen.github.io/2025/06/02/PP-LCNet/</id>
    <published>2025-06-02T06:44:44.808Z</published>
    <updated>2025-12-03T05:37:30.558Z</updated>
    
    <content type="html"><![CDATA[<h1>PP-LCNet</h1><p>PP-LCNet《A lightweight CPU convolutional Neural network》百度的</p><p>前有阿里团队针对Nvidia-GPU端加速而设计的高推理速度高性能的GENet，现有百度团队针对Intel-CPU端加速而设计的高推理速度&amp;高性能的LCNet。本文是百度团队结合Intel-CPU端侧推理特性而设计的轻量高性能网络PP-LCNet</p><p>PP-LCNet在推理延迟-精度均衡方面大幅优于ShuffleNetV2、MobileNetV2、MobileNetV3以及GhostNet</p><h1>创新点</h1><ul><li>以MobileNetV1中的DepthSepConv深度可分离卷积模块（就是深度卷积+1*1卷积，并且没有残差链接）作为基础模块，构建了一个类似MobileNetV1的BaseNet，融合了当前一些新的技术</li><li>近年来，深度学习领域也提出各种更优秀的激活函数，而Swish系列则是其中的佼佼者，尤以H-Swish为最优。同样采用H-Swish替换BaseNet中的ReLU，性能大幅提升，而推理速度几乎不变。</li><li>加入SE模块，<strong>将SE模块添加到接近网络尾部的模块，SE中的两个激活函数分别为SE和H-Sigmoid</strong></li><li>最后使用大的5*5卷积</li></ul><h1>创新点讲解</h1><h2 id="h-swish">h-swish</h2><p><img src="image-20250602145153566.png" alt="image-20250602145153566"></p><h2 id="se模块squeeze-and-excitation压缩与激励">SE模块Squeeze-and-Excitation压缩与激励</h2><ul><li>是一个很容易加入到网络中的东西</li><li>Transformation（Ftr）（转型）：首先把w*h*c通过一个卷积变成w*h*u的特征</li><li>Squeeze（全局信息嵌入）：然后把w*h*u的特征图通过全局平均池化变成一个1*1*C的一条特征</li><li>Excitation（自适应重新校正）：然后过全链接变成C/r个通道，再通过ReLU，再通过全链接变回C通道，再通过sigmod，最后是一个1*1*c的一个特征</li><li>Scale重新加权：将这个1*1*c的一个特征（被称为注意力权重）加到每一个w*h*c的每一个通道中</li><li><img src="image-20250602150203968.png" alt="image-20250602150203968"></li></ul><h1>网络架构</h1><p><img src="image-20250602144817042.png" alt="image-20250602144817042"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;PP-LCNet&lt;/h1&gt;
&lt;p&gt;PP-LCNet《A lightweight CPU convolutional Neural network》百度的&lt;/p&gt;
&lt;p&gt;前有阿里团队针对Nvidia-GPU端加速而设计的高推理速度高性能的GENet，现有百度团队针对Inte</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/02/MobileNet/"/>
    <id>http://outbreak-sen.github.io/2025/06/02/MobileNet/</id>
    <published>2025-06-02T03:59:10.633Z</published>
    <updated>2025-06-02T06:44:13.519Z</updated>
    
    <content type="html"><![CDATA[<h1>MobileNet</h1><p>MobileNets: V1:《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 》2017</p><p>MobileNet V2:《MobileNetV2:Inverted Residuals and Linear Bottlenecks》</p><p>MobileNet V3:《searching For MobileNet V3》</p><h2 id="mobilenet-v3">MobileNet V3</h2><p>2016年6月，谷歌提出了MobileNetV1，由于各种原因当时没有挂上arxiv，一直到2017年4月才提交。好巧不巧，谷歌的另一团队，同时提出了Xception。所有才有了两个结构都是基于深度可分离卷积的相似争论</p><p>谷歌在2019年5月份推出了最新的MobileNetV3</p><p><a href="https://zhuanlan.zhihu.com/p/70703846">https://zhuanlan.zhihu.com/p/70703846</a></p><h1>摘要</h1><p>有更小的体积，更少的计算量，更高的精度。在轻量级神经网络中拥有极大的优势。</p><h1>创新点</h1><h2 id="mobilenet-v1">MobileNet V1</h2><ul><li>MobileNetV1就是把VGG中的标准卷积层换成深度可分离卷积<em>depthwise separable convolution</em>就可以了。</li><li><strong>深度可分离卷积 = 深度卷积+逐点卷积</strong></li><li>引入ReLU6替代ReLU，在大于6时候取6。<strong>ReLU6作为非线性激活函数，在低精度计算下具有更强的鲁棒性</strong>。</li></ul><h2 id="mobilenet-v2">MobileNet V2</h2><ul><li>有人在实际使用的时候， 发现深度卷积部分的卷积核比较容易训废掉：训完之后发现深度卷积训出来的卷积核有不少是空的。作者认为这是<strong>ReLU</strong>这个激活函数的锅。</li><li>虽然V2的层数比V1的要多很多，但是FLOPs，参数以及CPU耗时都是比V1要好的。但是实验发现不匝地</li><li>把ReLU变成线性激活函数，把<strong>最后的那个ReLU6换成Linear</strong>。</li><li>提出Expansion layer，在深度卷积之前先用逐点卷积升通道纬度：深度卷积本身没有改变通道的能力，只能在低维度上工作，这样效果并不会很好，所以<strong>在DW深度卷积之前使用PW1*1卷积进行升维，再在一个更高维的空间中进行卷积操作来提取特征</strong></li><li><strong>学resnet加入shortcut</strong></li><li>resnet的bottleneck结构是先降维 (0.25倍)、卷积、再升维。MobileNetV2 则是 先升维 (6倍)、卷积、再降维。作者将其命名为<strong>Inverted residuals</strong></li><li><img src="image-20250602143715288.png" alt="image-20250602143715288"></li></ul><h2 id="mobilenet-v3">MobileNet V3</h2><ul><li><p>用神经结构搜索NAS，不看了</p></li><li><p>0.网络的架构基于NAS实现的MnasNet（效果比MobileNetV2好）</p><p>1.引入MobileNetV1的深度可分离卷积</p><p>2.引入MobileNetV2的具有线性瓶颈的倒残差结构</p><p>3.引入基于squeeze and excitation结构的轻量级注意力模型(SE)</p><p>4.使用了一种新的激活函数<a href="https://zhida.zhihu.com/search?content_id=103841490&amp;content_type=Article&amp;match_order=1&amp;q=h-swish&amp;zhida_source=entity">h-swish</a>(x)</p><p>5.网络结构搜索中，结合两种技术：资源受限的NAS（platform-aware NAS）与NetAdapt</p><p>6.修改了MobileNetV2网络端部最后阶段</p></li></ul><h1>创新点讲解</h1><h2 id="深度可分离卷积">深度可分离卷积</h2><ul><li>可分离卷积主要有两种类型：<strong>空间可分离卷积</strong>和<strong>深度可分离卷积</strong>。空间可分离就是将一个大的卷积核变成两个小的卷积核，比如将一个3×3的核分成一个3×1和一个1×3的核</li></ul><h3 id="深度卷积">深度卷积</h3><p><img src="image-20250602142812248.png" alt="image-20250602142812248"></p><p><img src="image-20250602142823747.png" alt="image-20250602142823747"></p><ul><li><img src="image-20250602141856953.png" alt="image-20250602141856953"></li><li>标准卷积：一个卷积核对w*h*c每个c做卷积再相加，一个核生成一个w*h的特征图，n个核生成n个通道的特征</li><li>深度卷积：c个卷积核对w*h*c做卷积，<strong>第i个对第i个通道做卷积，一一对应</strong>，这样最后还是w*h*c，<strong>输入通道是多少个，卷积核就有几个，输出通道还是多少个</strong></li></ul><h3 id="逐点卷积">逐点卷积</h3><ul><li>通道数太少，特征图的维度太少，不能获取到足够的有效信息，所以需要对特征图升维。<strong>逐点卷积就是1×1卷积</strong></li><li>在深度卷积的过程中，我们得到了8×8×3的输出特征图，我们用256个1×1×3的卷积核对输入特征图进行卷积操作，输出的特征图和标准的卷积操作一样都是8×8×256了。</li></ul><h2 id="relu6">ReLU6</h2><p>作者认为<strong>ReLU6作为非线性激活函数，在低精度计算下具有更强的鲁棒性</strong>。</p><p><img src="image-20250602143031205.png" alt="image-20250602143031205"></p><h1>网络架构</h1><h2 id="mobilenet-v1">MobileNet V1</h2><h2 id="mobilenet-v2">MobileNet V2</h2><h2 id="mobilenet-v3">MobileNet V3</h2><h1>网络架构</h1><ul><li></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;MobileNet&lt;/h1&gt;
&lt;p&gt;MobileNets: V1:《MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 》2017&lt;/p&gt;
&lt;p&gt;Mobile</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/01/SuffleNet/"/>
    <id>http://outbreak-sen.github.io/2025/06/01/SuffleNet/</id>
    <published>2025-06-01T13:59:32.088Z</published>
    <updated>2025-06-02T06:19:52.081Z</updated>
    
    <content type="html"><![CDATA[<h1>SuffleNet</h1><p><a href="https://arxiv.org/pdf/1707.01083.pdf">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p><p>旷视科技</p><h1>摘要</h1><p>MobileNet和SqueezeNet等一样主要是想应用在移动端</p><h1>创新点</h1><ul><li><p>ResNeXt 在非常小的网络中效率会降低，因为密集的 1×1卷积 代价很高 ，基于此作者提出了 pointwise group convolution 以减少 1×1卷积 的计算复杂度 ；</p></li><li><p>为克制 pointwise group convolution 带来的副作用，提出了 channel shuffle 的操作，用于实现信息在特征通道之间流动 .</p></li><li><p>ShuffleNet与MobileNet的对比，ShuffleNet不仅计算复杂度更低，而且精度更好。</p></li></ul><h1>创新点讲解</h1><h2 id="deepwiseconvolution-是一种groupconvolution-特征图数就是通道数">DeepwiseConvolution（是一种GroupConvolution） （特征图数就是通道数）</h2><ul><li><code>分组卷积(Group Convolution)</code> 的概念首先是在 AlexNet 中引入，用于将模型分布到两块 GPU 上。</li><li>Group  convolution是将<strong>输入层的不同特征图进行分组，就是说不同的通道分组成几个组，然后只用组数目的卷积核进行卷积，然后采用不同的卷积核再对各个组进行卷积，这样会降低卷积的计算量。</strong><ul><li>因为一般的卷积都是在所有的输入特征图上做卷积，可以说是全通道卷积，这是一种通道密集连接方式（channel dense connection）。</li><li>而group convolution相比则是一种通道稀疏连接方式（channel sparse  connection）。</li></ul></li><li>Xception和MobileNet采用了depthwise convolution，这其实是一种比较特殊的group convolution，因此此时<strong>分组数恰好等于通道数，意味着每个组只有一个特征图。</strong></li><li>但是这些网络存在一个很大的弊端是采用了密集的1x1卷积</li></ul><h2 id="pointwise-group-convolution-也是一种groupconvolution-针对1-1卷积">Pointwise Group Convolution（也是一种GroupConvolution，针对1*1卷积）</h2><ul><li>为了解决MobileNet中用太多1*1卷积的问题，这里提出了Pointwise Group Convolution来替代1*卷积</li><li>不如也对1x1卷积采用channel sparse connection，那样计算量就可以降下来了。但是group convolution存在另外一个弊端是不同组之间的特征图是不通信的，这会降低网络的特征提取能力。</li><li><img src="image-20250601220200619.png" alt=")"></li></ul><h2 id="channel-shuffle-针对1-1卷积之后">Channel Shuffle(针对1*1卷积之后)</h2><p><strong>目的</strong>：在分组卷积后，强制不同组之间交换信息。</p><p><code>通道洗牌(Channel Shuffle)</code> 允许 分组卷积 从不同的组中获取输入数据，从而实现输入通道和输出通道相关联</p><p><img src="image-20250601223123917.png" alt="image-20250601223123917"></p><p><img src="image-20250601223622564.png" alt="image-20250601223622564"></p><h1>经过pointwise group convolutiion和ChannelShuffle就可以修改Resnet基本结构</h1><p>ShuffleNet的基本单元是在一个残差单元BottleNeck的基础上改进而成的。</p><h2 id="基本单元-改3-3卷积">基本单元（改3*3卷积）</h2><ul><li><p>BottleNeck把其中的3*3卷积改成GroupConvolution3*3卷积</p></li><li><p>首先是1x1卷积，然后是3x3的depthwise convolution（DWConv，主要是为了降低计算量），这里的3x3卷积是瓶颈层（bottleneck），紧接着是1x1卷积，最后是一个短路连接，将输入直接加到输出上。</p></li></ul><p><img src="image-20250602113851387.png" alt="image-20250602113851387"></p><h2 id="再次改进-改1-1卷积-stride-1时">再次改进（改1*1卷积，stride=1时）</h2><ul><li><p>将密集的1x1卷积替换成1x1的group convolution，不过在第一个1x1卷积之后增加了一个channel  shuffle操作。</p></li><li><p>按paper的意思，对于这样一个残差单元，一个channel shuffle操作是足够了。</p></li><li><p>还有就是3x3的depthwise  convolution之后没有使用ReLU激活函数。</p></li><li><p>如果stride=1时，此时输入与输出shape一致可以直接相加</p></li><li><p><img src="image-20250602114053117.png" alt="image-20250602114053117"></p></li></ul><h2 id="再次改进-改shortcut-stride-2时">再次改进（改shortcut，stride=2时）</h2><ul><li>当stride=2时，通道数增加，而特征图大小减小，此时输入与输出不匹配。一般情况下可以采用一个1x1卷积将输入映射成和输出一样的shape。但是在ShuffleNet中，对原输入采用stride=2的3x3 avg pool，这样得到和输出一样大小的特征图</li><li>然后将得到特征图与输出进行连接（concat），而不是相加。这样做的目的主要是降低计算量与参数大小。</li><li><img src="image-20250602114201250.png" alt="image-20250602114201250"></li></ul><h1>网络架构</h1><ul><li>0.5x和0.25x表示的是在基准模型上将通道数缩小为原来的0.5和0.25</li><li><ol><li>开始使用的普通的3x3的卷积和max  pool层。然后是三个阶段，每个阶段都是重复堆积了几个ShuffleNet的基本单元。</li><li>对于每个阶段，第一个基本单元采用的是stride=2，这样特征图width和height各降低一半，而通道数增加一倍。后面的基本单元都是stride=1，特征图和通道数都保持不变。</li></ol></li></ul><p><img src="image-20250602114749060.png" alt="image-20250602114749060"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;SuffleNet&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1707.01083.pdf&quot;&gt;ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mob</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/01/ResNet/"/>
    <id>http://outbreak-sen.github.io/2025/06/01/ResNet/</id>
    <published>2025-06-01T12:43:44.775Z</published>
    <updated>2025-06-01T12:43:44.776Z</updated>
    
    <content type="html"><![CDATA[<h1>李沐-ResNet</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;李沐-ResNet&lt;/h1&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/01/%E6%9D%8E%E6%B2%90-TransFormer%E5%9F%BA%E7%A1%80/"/>
    <id>http://outbreak-sen.github.io/2025/06/01/%E6%9D%8E%E6%B2%90-TransFormer%E5%9F%BA%E7%A1%80/</id>
    <published>2025-06-01T04:15:58.345Z</published>
    <updated>2025-06-01T13:59:15.713Z</updated>
    
    <content type="html"><![CDATA[<h1>李沐-TransFormer基础</h1><p>Attention Is All You Need</p><p>谷歌在2017年的论文</p><p>同等贡献很多</p><p>代码在tensor2tensor</p><h1>摘要</h1><ul><li>序列转换模型都是基于复杂的循环神经网络或卷积神经网络，且都包含一个encoder和一个decoder。表现最好的模型还通过attention机制把encoder和decoder联接起来。</li><li>提出了一个新的、简单的网络架构，Transformer. 它只基于单独的attention机制，完全避免使用循环和卷积。在两个翻译任务上表明，我们的模型在质量上更好，同时具有更高的并行性，且训练所需要的时间更少。</li><li>RNN,LSTM,GRU在序列建模和转换任务上取得SOTA结果，递归模型通常沿输入和输出序列的符号位置进行因子计算。在计算时将位置与步骤对齐，它们生成一系列隐藏状态和当前的输入生成。这种内部的固有顺阻碍了训练样本的并行化，</li><li>attention已经成功应用在encoder和decoder中了。self-attention,有时也叫做内部注意力，是一种注意力机制，它将一个序列的不同位置联系起来，以计算序列的表示。self-attention 已经成功的运用到了很多任务上，包括阅读理解、抽象摘要、语篇蕴涵和学习任务无关的句子表征等。</li><li>编码时候需要整个放进去，解码时候只能一个一个输出</li></ul><h1>创新点</h1><ul><li>使用注意力机制</li><li>提出了缩放的点积注意力机制</li><li>多头注意力机制</li><li>Positional Encoding（位置编码）</li><li>Embeddings and Softmax （词嵌入和 softmax）</li><li>前馈网络FeedForward</li><li>Masked Muilt-Head attention</li></ul><h1>创新点讲解</h1><ul><li>embedding</li><li>位置编码</li><li></li></ul><h2 id="layernorm">layernorm</h2><p>Jimmy Ba 等人在 2016 年提出，旨在解决 <strong>Batch Normalization（BN）</strong> 对小批量数据（Batch Size）依赖的问题。LN <strong>对每个样本独立归一化</strong>，计算其在所有通道（或特征维度）上的均值和方差，不依赖 Batch 内其他样本。</p><ul><li>其实就是一个batch里有多个图，batchnorm就是<strong>所有图</strong>，<strong>每个通道</strong>做norm</li><li>layernorm就是<strong>每个图</strong>，但是<strong>所有通道</strong>分别自己做norm，适合动态数据（如变长序列）。注意不再是每个通道分别做norm，而是现在所有通道公平norm。</li><li>BN 是<strong>对每个通道 <code>c</code>，跨所有样本 <code>N</code> 和空间位置 <code>(H,W)</code> 计算统计量</strong>。</li><li>LN 是<strong>对每个样本 <code>n</code>，跨所有通道 <code>C</code> 和空间位置 <code>(H,W)</code> 计算统计量</strong>。</li></ul><h2 id="masked-muilt-head-attention">Masked Muilt-Head attention</h2><p>解码器应该只看到前面的输入，不能看到之后的输入</p><h1>Attention</h1><h1>模型架构</h1><ul><li><strong>Encoder</strong>:encoder由**N(N=6)**个完全相同的layer堆叠而成.每层有两个子层。第一层是multi-head self-attention机制，第二层是一个简单的、位置全连接的前馈神经网络Simple position-wise fully connected feed-forward network。我们在两个子层的每一层后采用残差连接，接着进行layer normalization。也就是说，每个子层的输出是LayerNorm(x + Sublayer(x)) 其中 Sublayer(x) 是由子层本身实现的函数。为了方便这些残差连接，模型中的所有子层以及embedding层产生的输出维度都为dmodel=512。nn.LayerNorm</li><li><strong>Decoder</strong>:  decoder也由N(N=6)个完全相同的layer堆叠而成.除了每个编码器层中的两个子层之外，<strong>解码器还插入第三个子层，该子层对编码器encoder堆栈的输出执行multi-head attention操作，这不就是交叉注意力机制</strong>，与encoder相似，我们在每个子层的后面使用了残差连接，之后采用了layer  normalization。我们也修改了decoder stack中的 self-attention  子层，以防止当前位置信息中被添加进后续的位置信息。</li></ul><p><img src="image-20250601212914011.png" alt="image-20250601212914011"></p><h2 id="自注意力机制">自注意力机制</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;李沐-TransFormer基础&lt;/h1&gt;
&lt;p&gt;Attention Is All You Need&lt;/p&gt;
&lt;p&gt;谷歌在2017年的论文&lt;/p&gt;
&lt;p&gt;同等贡献很多&lt;/p&gt;
&lt;p&gt;代码在tensor2tensor&lt;/p&gt;
&lt;h1&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;序列转</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/01/%E6%9D%8E%E6%B2%90-ResNet/"/>
    <id>http://outbreak-sen.github.io/2025/06/01/%E6%9D%8E%E6%B2%90-ResNet/</id>
    <published>2025-06-01T04:15:15.036Z</published>
    <updated>2025-06-02T03:31:16.444Z</updated>
    
    <content type="html"><![CDATA[<h1>ResNet</h1><p>Deep Residual Learning for Image Recognition</p><p>微软研究院的Kaiming He，孙剑等四名华人</p><p>2015</p><h1>摘要</h1><ul><li>网络越深越强，更深的神经网络难训练，提出了一种残差学习框架来减轻网络训练，152层。当时googlenet并行性高，resnet比VGG深8倍，但是resnet深，结构简单，训练起来快</li><li>在ImageNet测试集上取得了<code>3.57%</code>的错误率。</li><li>CIFAR-10上分析了100层和1000层的残差网络</li><li>也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</li><li>网络深了，梯度消失/爆炸，从一开始就阻碍了收敛。然而，这个问题通过标准初始化和中间标准化层在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</li><li>当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和然后迅速下降。意外的是，这种退化不是由过拟合引起的，并且在适当的深度模型上添加更多的层会导致更高的训练误差</li></ul><h1>创新点</h1><ul><li>提出了退化现象，极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差</li><li>提出残差块</li><li><strong>提出BN，批归一化，丢弃dropout</strong></li></ul><h1>创新点讲解</h1><h2 id="两种残差结构basicblock和bottleneck">两种残差结构BasicBlock和BottleNeck</h2><ul><li><p>左边的残差结构称为BasicBlock是针对层数较少网络，例如ResNet18层和ResNet34层网络。</p></li><li><p>右边的残差结构BottleNeck是针对网络层数较多的网络，例如ResNet101，ResNet152等。</p></li><li><p>为什么深层网络要使用右侧的残差结构呢。因为，右侧的残差结构能够减少网络参数与运算量。同样输入一个channel为256的特征矩阵，如果使用左侧的残差结构需要大约1170648个参数，但如果使用右侧的残差结构只需要69632个参数。</p></li><li><p>1*1卷积核可以做到升降纬度，为了使的残差块可以同通道相加，需要通过1*1卷积把输入升纬度。在BottleNeck中一个256通道的图先通过1*1卷积</p></li></ul><p><img src="image-20250602112533234.png" alt="image-20250602112533234"></p><h2 id="basicblock">BasicBlock</h2><p>在resnet18和resnet34中使用了BasicBlock，输入输出通道数均为64，不需要1*1卷积</p><p><img src="image-20250602113025370.png" alt="image-20250602113025370"></p><pre><code>class BasicBlock(nn.Module):    expansion = 1def __init__(self, inplanes, planes, stride=1, downsample=None):    super(BasicBlock, self).__init__()    self.conv1 = conv3x3(inplanes, planes, stride)    self.bn1 = nn.BatchNorm2d(planes)    self.relu = nn.ReLU(inplace=True)    self.conv2 = conv3x3(planes, planes)    self.bn2 = nn.BatchNorm2d(planes)    self.downsample = downsample    self.stride = stridedef forward(self, x):    identity = x    out = self.conv1(x)    out = self.bn1(out)    out = self.relu(out)    out = self.conv2(out)    out = self.bn2(out)    if self.downsample is not None:        identity = self.downsample(x)    out += identity    out = self.relu(out)    return out</code></pre><h2 id="bottleneck">BottleNeck</h2><p>在resnet50、resnet101、resnet152使用了<code>Bottlenect</code>构造网络。<code>Bottleneck Block</code>中使用了1×1卷积层。如输入通道数为256，1×1卷积层会将通道数先降为64，经过3×3卷积层后，再将通道数升为256。1×1卷积层的优势是在更深的网络中，用较小的参数量处理通道数很大的输入。</p><p><img src="image-20250602113116084.png" alt="image-20250602113116084"></p><pre><code>class Bottleneck(nn.Module):    expansion = 4    def __init__(self, inplanes, planes, stride=1, downsample=None):    super(Bottleneck, self).__init__()    self.conv1 = conv1x1(inplanes, planes)    self.bn1 = nn.BatchNorm2d(planes)    self.conv2 = conv3x3(planes, planes, stride)    self.bn2 = nn.BatchNorm2d(planes)    self.conv3 = conv1x1(planes, planes * self.expansion)    self.bn3 = nn.BatchNorm2d(planes * self.expansion)    self.relu = nn.ReLU(inplace=True)    self.downsample = downsample    self.stride = stridedef forward(self, x):    identity = x    out = self.conv1(x)    out = self.bn1(out)    out = self.relu(out)    out = self.conv2(out)    out = self.bn2(out)    out = self.relu(out)    out = self.conv3(out)    out = self.bn3(out)    if self.downsample is not None:        identity = self.downsample(x)    out += identity    out = self.relu(out)    return out</code></pre><h2 id="batch-normalization">Batch Normalization</h2><ul><li><p>Batch Normalization（批归一化）是深度学习中一种重要的归一化技术，由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出。</p></li><li><p>**原paper中，BN被建议插入在（每个）ReLU激活层前面，因为ReLU激活函数的输出非负，不能近似为高斯分布。**<strong>但是后面实验表明，放在前后的差异似乎不大，甚至放在ReLU后还好一些。</strong></p></li><li><p><strong>一个batch有几个图，比如batchsize为4就是4个图拼到一起然后计算均值方差然后归一化。</strong></p></li><li><p>如何做到每个批次的图片组合是随机的，并且同一张图片不会重复出现在同一个批次中：<code>DataLoader</code> 的 <code>shuffle=True</code> 参数会在每个 epoch 开始时打乱数据顺序，从而保证不同 epoch 的批次组合不同。</p></li><li><p>在推理（Inference）时，如果只有一张图片（无法组成完整的 Batch），可以通过以下方法处理：</p><ul><li>大多数现代深度学习框架（如 PyTorch、TensorFlow）允许直接输入单张图片，无需强制补全 Batch。</li><li><strong>全局统计量</strong>：PyTorch 的 <code>model.eval()</code> 会自动切换 BN 到推理模式。Batch Size=1，BN 层也能正常工作。input.unsqueeze(0)</li><li>如果模型强制要求固定 Batch Size（某些老旧框架），可以通过以下方式补全：batch_tensor = input_tensor.repeat(4, 1, 1, 1)</li></ul></li><li><p><img src="image-20250601210943839.png" alt="image-20250601210943839"></p></li></ul><h1>网络架构</h1><p><img src="SouthEast-17487824259062.png" alt="Table 1"></p><p><img src="SouthEast.png" alt="Figure 3"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;ResNet&lt;/h1&gt;
&lt;p&gt;Deep Residual Learning for Image Recognition&lt;/p&gt;
&lt;p&gt;微软研究院的Kaiming He，孙剑等四名华人&lt;/p&gt;
&lt;p&gt;2015&lt;/p&gt;
&lt;h1&gt;摘要&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;网络越深越强，</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/06/01/SMPL/"/>
    <id>http://outbreak-sen.github.io/2025/06/01/SMPL/</id>
    <published>2025-06-01T04:14:52.341Z</published>
    <updated>2025-07-01T08:35:44.414Z</updated>
    
    <content type="html"><![CDATA[<h1>SMPL</h1><h1>基础知识</h1><ul><li><p>顶点（vertex）：动画模型可以看成多个小三角形组成，每三个顶点组成一个三角形，每个小三角形就可以看成一个顶点。</p></li><li><p>骨骼点：骨架关节，比如膝盖</p></li><li><p>骨骼蒙皮（Rig）：建立骨骼点和顶点的关联关系。每个骨骼点会关联许多顶点，并且每一个顶点权重不一样。通过这种关联关系，就可以通过控制骨骼点的旋转向量来控制整个人运动。</p></li><li><p>顶点权重(vertex weights)：用于变形网格mesh，通过骨骼计算当前顶点</p></li><li><p>BlendShape：控制动画角色运动有两种，一种是上面说的利用Rig，还有一种是利用BlendShape。比如：生成一种笑脸和正常脸，那么通过BlendShape就可以自动生成二者过渡的动画。这种方式相比于利用Rig，可以不定义骨骼点，比较方便。</p></li><li><p>蒙皮函数（SF，Skinning Function）：是计算机图形学中用于将骨骼动画（Skeletal Animation）的变形作用到3D模型表面顶点上的数学方法。它的核心作用是根据骨骼的运动（如旋转、平移）动态调整模型顶点的位置。蒙皮函数包括下面的LBS，BS，DQS。</p></li><li><p>线性混合蒙皮函数（LBS，linear blend skinning algorithm）：是一种蒙皮函数skinning methods ，每个模型的顶点可以受多个骨骼影响，每个骨骼对顶点的影响程度通过权重（Weight）表示（权重总和为1）例如，一个顶点可能受“上臂骨”影响60%，受“肩膀骨”影响40%。在骨骼运动时，顶点的新位置是所有关联骨骼变换的加权平均。</p><ul><li><img src="image-20250602185203297.png" alt="image-20250602185203297"></li><li>缺点：<ul><li>糖果纸效应（Candy Wrapping）**：在关节弯曲时可能出现不自然的塌陷或扭曲（如图）。</li><li>体积损失：剧烈弯曲时模型体积可能收缩。</li></ul></li></ul></li><li><p>混合蒙皮函数（BS，BlendSkin）是一种改进的蒙皮（Skinning）技术，主传统LBS直接对变换矩阵进行线性加权，而BlendSkin可能采用 <strong>非线性混合</strong> 或 <strong>局部几何感知的插值</strong>，使关节弯曲时保持更好的体积和表面连续性。</p></li><li><p>双四元数蒙皮函数（DQS，Dual Quaternion Skinning）：使用双四元数代替线性插值，能更好地保持关节处的体积和形状，避免糖果纸效应。</p></li><li><p><img src="image-20250602185743277.png" alt="image-20250602185743277"></p></li></ul><h1>SMPL</h1><p><a href="https://www.zhihu.com/question/292017089">https://www.zhihu.com/question/292017089</a></p><p><a href="https://zhuanlan.zhihu.com/p/696878001">https://zhuanlan.zhihu.com/p/696878001</a></p><p><a href="https://zhuanlan.zhihu.com/p/94218908">https://zhuanlan.zhihu.com/p/94218908</a></p><p><a href="https://zhuanlan.zhihu.com/p/94218908">https://zhuanlan.zhihu.com/p/94218908</a></p><p><a href="https://zhuanlan.zhihu.com/p/420208990">https://zhuanlan.zhihu.com/p/420208990</a></p><p><a href="https://zhuanlan.zhihu.com/p/420090584">https://zhuanlan.zhihu.com/p/420090584</a></p><p><a href="https://zhuanlan.zhihu.com/p/420090584">https://zhuanlan.zhihu.com/p/420090584</a></p><p><a href="https://zhuanlan.zhihu.com/p/453206221">https://zhuanlan.zhihu.com/p/453206221</a></p><p><a href="https://zhuanlan.zhihu.com/p/453206221">https://zhuanlan.zhihu.com/p/453206221</a></p><p><a href="https://zhuanlan.zhihu.com/p/453206221">https://zhuanlan.zhihu.com/p/453206221</a></p><p><a href="https://zhuanlan.zhihu.com/p/453206221">https://zhuanlan.zhihu.com/p/453206221</a></p><p>《SMPL: A Skinned Multi-Person Linear Model》</p><ul><li><p>人体可以理解为是一个基础模型和在该模型基础上进行形变的总和，在形变基础上进行PCA，得到刻画形状的低维参数——形状参数（shape）；同时，使用运动树表示人体的姿势，即运动树每个关节点和父节点的旋转关系，该关系可以表示为三维向量，最终每个关节点的局部旋转向量构成了smpl模型的姿势参数(pose)。</p></li><li><p>该模型中β和θ是其中的输入参数，其中β是ShapeBlendPose参数代表人体高矮胖瘦、头身比等比例的10个参数，θ是代表人体整体运动位姿和24个关节相对角度的75(24*3+3;每个关节点3个自由度，再加上3个根节点)个参数。然后通过LBS输出是6980个顶点N。</p></li><li><p>参数：</p><ul><li>M， : SMPL function</li><li>T，一个 6890 × 3 6890 的矩阵。这个矩阵是常数值，是TPose</li><li>J，从蒙皮预测骨架的函数，实现从顶点坐标获得关节坐标，24 × 3的矩阵，23个关节点+1个root orientation。这个矩阵是常数值</li><li>W，Skinning function蒙皮函数，一个 6890 × 24 的矩阵。注意，这个矩阵的值需要训练得到，每一个关节点的坐标变化对每一个mesh顶点坐标变化</li><li>B_p: 姿态混合蒙皮函数，Pose blendshapes function，输入姿态参数输出是3*N=3*6980的顶点坐标</li><li>p：所有207个姿势混合形状组成的矩阵</li><li>B_s: 形状混合蒙皮函数，Shape blendshapes function，输入形状参数输出是3*N=3*6980的顶点坐标</li><li>s：形状位移矩阵</li><li>β，影响人体mesh的形状（高矮胖瘦）</li><li>θ，影响人体mesh的姿态（动作姿势）</li><li>SMPL通过训练获得的参数：T，W，S，P，J</li></ul></li></ul><style>.rgyivpgynczi{}</style><img src="/2025/06/01/SMPL/image-20250602184708123.png" class="rgyivpgynczi" alt="image-20250602184708123"><ul><li><pre><code>J_regressor_prior (scipy.sparse.csc.csc_matrix)：表示一个稀疏矩阵，用于从形状参数推断关节位置的先验模型。f (numpy.ndarray)：通常表示面的数组，用于描述3D网格模型的拓扑结构。J_regressor (scipy.sparse.csc.csc_matrix)：另一个稀疏矩阵，用于从形状参数计算关节位置的回归器。kintree_table (numpy.ndarray)：表示关节层次结构的数组，用于描述骨骼的树状结构。J (numpy.ndarray)：关节位置的数组，表示模型中各关节在标准姿态下的位置。weights_prior (numpy.ndarray)：一个先验权重数组，用于姿态和形状的权重调整。weights (numpy.ndarray)：每个顶点的骨骼权重数组，表示各顶点受不同骨骼的影响程度。vert_sym_idxs (numpy.ndarray)：顶点对称索引数组，用于保持模型的左右对称性。posedirs (numpy.ndarray)：姿态方向数组，用于描述姿态变化对顶点位置的影响。pose_training_info (dict)：包含姿态训练信息的字典，可能包含训练数据或参数设置。bs_style (str)：绑定皮肤样式的字符串，描述模型的皮肤绑定方式。v_template (numpy.ndarray)：顶点模板数组，表示未变形的基础顶点位置。shapedirs (chumpy</code></pre></li><li><p>形状参数：</p><ul><li>0   代表整个人体的胖瘦和大小，初始为0的情况下，正数变瘦小，负数变大胖（±5）<br>1  侧面压缩拉伸，正数压缩<br>2  正数变胖大<br>3  负数肚子变大很多，人体缩小<br>4   代表 chest、hip、abdomen的大小，初始为0的情况下，正数变大，负数变小（±5）<br>5  负数表示大肚子+整体变瘦<br>6 正数表示肚子变得特别大的情况下，其他部位非常瘦小<br>7 正数表示身体被纵向挤压<br>8  正数表示横向表胖<br>9 正数表示肩膀变宽</li></ul></li><li><p>形状参数怎么影响顶点的：利用PCA原理控制mesh的胖瘦高矮(</p></li></ul><p>用LBS表示的线性混合蒙皮核用双cerian混合蒙皮dqb 都不如SMPL</p><p>SMPL是Tpose并对这个模型施加偏移量实现</p><p><img src="image-20250602174618282.png" alt="image-20250602174618282"></p><p><img src="image-20250602174707435.png" alt="image-20250602174707435"></p><h2 id="创新点：">创新点：</h2><ul><li>提出了一种学习人体形状和位置依赖的形状变化模型：模型的参数从数据中学习，包括静止姿势模板、混合权重、姿势相关的混合变形、身份相关的混合变形以及从顶点到关节位置的回归。</li><li>将pose blend shapes建模成rotation matrices（旋转矩阵）的线性函数。因为旋转矩阵的元素是有界的，所产生的变形也是有界的，有助于模型更好地推广。</li><li></li></ul><p><img src="image-20250602170036448.png" alt="image-20250602170036448"></p><style>.vcnolcndlays{zoom:150%;}</style><img src="/2025/06/01/SMPL/image-20250602170200797.png" class="vcnolcndlays" alt="image-20250602170200797"><h1>评估指标指标含义。</h1><ul><li><strong>MPJPE</strong>。(Mean Per Joint Position Error：首先对齐骨盆(pelvis，关节0)处的预测和GT真实3D关节，然后计算它们的距离，综合评价预测的姿态和形状，包括全局旋转。</li><li><strong>PA-MPJPE</strong>。(Procrustes-Aligned Mean Per Joint Position Error, or reconstruction error)：在计算 MPJPE 之前执行 Procrustes 对齐，即刚性对齐(rigid alignment)，这主要测量关节姿势，消除了尺度和全局旋转的差异。</li><li><strong>MPVE</strong>。(Mean Per-vertex Error or Vertex-to-Vertex ,V2V)：被定义为预测网格顶点与地面GT网格顶点之间的平均点对点欧几里得距离。</li><li><strong>PA-MPVE</strong>。(Procrustes-Aligned Mean Per-vertex Error  )：Procrustes对齐后的MPVE。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;SMPL&lt;/h1&gt;
&lt;h1&gt;基础知识&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;顶点（vertex）：动画模型可以看成多个小三角形组成，每三个顶点组成一个三角形，每个小三角形就可以看成一个顶点。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;骨骼点：骨架关节，比如膝盖&lt;/p&gt;
&lt;/li&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>20250329陕师大讲解mindnlp模型微调</title>
    <link href="http://outbreak-sen.github.io/2025/03/24/20250329%E9%99%95%E5%B8%88%E5%A4%A7%E8%AE%B2%E8%A7%A3mindnlp%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/"/>
    <id>http://outbreak-sen.github.io/2025/03/24/20250329%E9%99%95%E5%B8%88%E5%A4%A7%E8%AE%B2%E8%A7%A3mindnlp%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</id>
    <published>2025-03-24T05:38:37.000Z</published>
    <updated>2025-03-25T14:03:10.353Z</updated>
    
    <content type="html"><![CDATA[<h1>讲解mindnlp模型微调20250329</h1><p>先给大家说自己是三维视觉领域</p><p>首先讲解mindnlp的组成，训练方法</p><p>然后讲解模型任务，模型分为四种</p><p>然后讲解微调数据集的选择，微调的两种方法</p><p>然后讲解微调数据集的格式</p><p>然后讲解PEFT</p><p>然后讲解Ascend 910B</p><p>看效果</p><h1>MindNLP</h1><p>MindNLP是基于MindSpore的一个自然语言处理（NLP）开源库，其中包含了许多自然语言处理的常用方法，提供了一个易用的NLP平台，旨在帮助研究人员和开发者更高效地构建和训练模型。</p><ul><li><strong>丰富完备的数据预处理模块</strong><ul><li>提供丰富的NLP数据处理模块，可灵活高效地完成数据预处理，包括Multi30k、SQuAD、CoNLL等经典数据集。</li></ul></li><li><strong>用户友好的NLP模型工具集</strong><ul><li>提供多种可配置神经网络组件，易于用户自定义模型，可大幅提升NLP任务建模的效率。</li></ul></li><li><strong>灵活易用的训练与推理引擎</strong><ul><li>简化了MindSpore中复杂的训练过程，提供了灵活易用的 <code>Trainer</code> 和 <code>Evaluator</code> ，可以轻松地训练和评估模型</li></ul></li></ul><p><strong>MindNLP</strong></p><ul><li>华为昇思MindSpore生态下的开源NLP库</li><li>专为MindSpore框架优化设计</li><li>充分利用其自动并行、动静态图统一等特性</li><li>特别针对华为昇腾芯片优化</li><li>全面适配Hugging Face主要开发库(如Transformers、Peft、Trl)，可直接使用datasets库，保持与Hugging Face生态的一致性</li></ul><p><strong>Transformers</strong></p><ul><li><p>由Hugging Face开发维护，已成为全球最流行的NLP开源库之一。</p></li><li><p>支持PyTorch、TensorFlow和JAX等多种深度学习框架</p></li><li><p>框架无关设计，原生支持PyTorch、TensorFlow，通过适配层也可支持其他框架如JAX</p></li><li><p>它提供了数万个预训练模型</p></li><li><p>作为Hugging Face生态核心，与Model Hub、Datasets、Evaluate、Gradio等工具无缝集成</p></li></ul><h2 id="模型">模型</h2><p><a href="https://mindnlp-ai.readthedocs.io/en/latest/zh/api/transformers/models/blenderbot_small/#mindnlp.transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel">https://mindnlp-ai.readthedocs.io/en/latest/zh/api/transformers/models/blenderbot_small/#mindnlp.transformers.models.blenderbot_small.modeling_blenderbot_small.BlenderbotSmallModel</a></p><table><thead><tr><th>模型名称</th><th>架构类型</th><th>输入输出模式</th><th>主要用途</th><th>适用场景</th></tr></thead><tbody><tr><td><code>BlenderbotSmallModel</code></td><td>Encoder-Decoder</td><td>接受输入，返回未经语言模型头（LM Head）处理的原始隐藏状态</td><td>提取文本特征或自定义任务头</td><td>研究、特征提取</td></tr><tr><td><code>BlenderbotSmallForConditionalGeneration</code></td><td>Encoder-Decoder + LM Head</td><td>接受输入，直接生成文本序列（通过 <code>LM Head</code> 解码）</td><td>对话生成（如微调 <code>Synthetic-Persona-Chat</code>）。 文本摘要、翻译等需要双向上下文理解的任务。</td><td>对话生成、翻译、摘要</td></tr><tr><td><code>BlenderbotSmallForCausalLM</code></td><td>Decoder-only</td><td>从左到右生成文本（类似 GPT）</td><td>开放式文本生成（如故事续写）。 单轮问答、文本补全。</td><td>故事续写、文本补全</td></tr></tbody></table><hr><ol><li>BlenderbotSmallModel</li></ol><ul><li><strong>定位</strong>: 基础模型，仅包含编码器（Encoder）和解码器（Decoder）骨架。</li><li><strong>输出</strong>: 返回未经语言模型头（LM Head）处理的原始隐藏状态。</li><li><strong>用途</strong>:<ul><li>研究模型内部机制。</li><li>自定义任务头（如分类、检索）。</li></ul></li></ul><ol start="2"><li><strong>BlenderbotSmallForConditionalGeneration</strong>:<ul><li><strong>适用场景</strong>: 对话生成任务，尤其是需要根据上下文生成连贯回复的场景。</li><li><strong>特点</strong>: 这是一个条件生成模型，适合处理输入输出对的任务，如对话生成、文本摘要等。</li><li><strong>推荐</strong>: 如果你的目标是生成对话回复，<code>BlenderbotSmallForConditionalGeneration</code> 是更好的选择。</li></ul></li><li><strong>BlenderbotSmallForCausalLM</strong>:<ul><li><strong>适用场景</strong>: 自回归文本生成任务，如故事生成或文本补全。</li><li><strong>特点</strong>: 这是一个因果语言模型，适合从左到右生成文本，不依赖特定输入条件。</li><li><strong>推荐</strong>: 如果你的任务是生成连续文本而非对话回复，可以选择 <code>BlenderbotSmallForCausalLM</code>。</li></ul></li></ol><h1>模型微调方法</h1><p>微调的一般策略。通常有两种方法：一种是全参数微调，另一种是部分冻结，只训练某些层。</p><p>ConditionalGeneration，所以接下来应该是具体的微调步骤。冻结层通常是为了防止过拟合，特别是在数据量较小的情况下，或者保留预训练模型的一些通用特征。Synthetic-Persona-Chat数据集可能是一个对话生成数据集，有足够的数据量。</p><p>接下来要考虑BlenderbotSmall的结构。BlenderbotSmall是基于Transformer的模型，包含编码器和解码器。对于生成任务，通常解码器部分更关键，尤其是在生成响应时。用户是否需要冻结前面的权重，可能指的是编码器部分，或者是底层的嵌入层？</p><p>一般来说，在微调时，如果数据量不大，冻结部分底层参数可以防止过拟合，同时让模型保留更多的通用语言理解能力。反之，如果数据足够，解冻所有层进行微调可能效果更好。需要询问用户的数据量吗？但用户没有提到，可能需要给出一般性的建议。</p><p>然后，关于如何用transformers库冻结参数。通常是通过设置参数的requires_grad为False。例如，遍历模型的参数，对某些层进行冻结。例如，冻结嵌入层、编码器的前几层等。</p><ul><li><strong>不冻结（默认推荐）</strong>:<ul><li><strong>适用场景</strong>: 数据集足够大（与预训练数据分布相似），或需要模型充分适应对话任务。</li><li><strong>优点</strong>: 模型的所有参数均可更新，能更好地学习对话任务的特性。</li><li><strong>缺点</strong>: 训练时间较长，显存占用高，可能在小数据集上过拟合。</li></ul></li><li><strong>冻结部分权重</strong>:<ul><li><strong>适用场景</strong>: 数据集较小，或希望快速微调、节省资源。</li><li><strong>优点</strong>: 减少显存占用、加速训练，防止过拟合。</li><li><strong>缺点</strong>: 可能限制模型对新任务的适应能力。</li><li><ol><li><strong>轻量级微调</strong>:<ul><li>冻结编码器，仅训练解码器。</li><li>适合资源有限或数据集较小的情况。</li></ul></li><li><strong>平衡策略</strong>:<ul><li>冻结前几层（如编码器的前 3 层），训练深层网络。</li><li>保留浅层通用特征，调整深层任务相关特征。</li></ul></li><li><strong>全参数微调</strong>:<ul><li>不冻结任何参数，直接微调。</li><li>适合数据集较大且与预训练任务差异较大的场景。</li></ul></li></ol></li></ul></li></ul><h1>数据集选择</h1><h3 id="针对-synthetic-persona-chat-的微调选择推荐模型-blenderbotsmallforconditionalgeneration">针对 <code>Synthetic-Persona-Chat</code> 的微调选择推荐模型:<code>BlenderbotSmallForConditionalGeneration</code></h3><h3 id="原因">原因:</h3><ol><li><strong>任务匹配</strong>: 该数据集是 <strong>多轮对话</strong>，需要模型根据上下文（用户输入 + 角色设定）生成回复，完美契合 Seq2Seq 架构。</li><li><strong>双向上下文</strong>: 编码器可捕捉输入对话的全局信息，解码器生成时结合编码器状态和已生成部分。</li><li><strong>性能优势</strong>: 在对话任务中，条件生成模型的流畅度和相关性显著优于纯解码器模型。</li></ol><h3 id="为什么不选其他模型？">为什么不选其他模型？</h3><ul><li><code>BlenderbotSmallModel</code>:<ul><li>缺少生成头（LM Head），需自行添加生成逻辑，不适合直接微调生成任务。</li></ul></li><li><code>BlenderbotSmallForCausalLM</code>:<ul><li>仅解码器架构无法有效利用双向上下文，对话生成效果受限。</li><li>更适合单轮文本续写，而非多轮交互式对话。</li></ul></li></ul><h3 id="如何调用">如何调用</h3><h2 id="如何处理数据集">如何处理数据集</h2><p>一个dataset库获得的数据集要用在mindnlp训练中，要经过以下三个步骤：</p><ol><li>将数据集转化为模型需要的格式，如序列生成模型需要对话对格式，序类分类模型需要序列+标签格式</li><li>进行tokenizer</li><li>转化为mindnlp可以直接使用的mindspore.dataset格式</li></ol><p>给一个huggingface的图</p><h3 id="数据集的目标结构：对话形式">数据集的目标结构：对话形式</h3><p>BlenderbotSmall 是专为 <strong>多轮对话生成</strong> 设计的模型，因此数据集需要以 <strong>对话对（Context-Response Pairs）</strong> 为核心。具体来说，每条样本应包含：</p><ul><li><p><strong>输入（Context）</strong>：对话历史（多轮上下文）。</p></li><li><p><strong>目标输出（Target Response）</strong>：模型需要生成的回复。</p></li><li><p><strong>单轮对话（简单场景）</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 输入（上下文）</span><br><span class="line">&quot;Hi, how are you?&quot;</span><br><span class="line"># 目标输出（回复）</span><br><span class="line">&quot;I&#x27;m good, thanks! How about you?&quot;</span><br></pre></td></tr></table></figure><h4 id="多轮对话-更常见"><strong>多轮对话（更常见）</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 输入（上下文）</span><br><span class="line">&quot;User: Hi, what&#x27;s your favorite movie? \nBot: I like science fiction. \nUser: Why?&quot;</span><br><span class="line"># 目标输出（回复）</span><br><span class="line">&quot;Because it explores futuristic ideas.&quot;</span><br></pre></td></tr></table></figure><h4 id="带有角色设定-persona-的对话"><strong>带有角色设定（Persona）的对话</strong></h4><p>如果数据集包含角色信息（如 <code>Synthetic-Persona-Chat</code>），需要在输入中显式添加角色描述：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 输入（上下文）</span><br><span class="line">&quot;Persona: I am a robot. I love reading books. \nUser: What do you do for fun?&quot;</span><br><span class="line"># 目标输出（回复）</span><br><span class="line">&quot;I enjoy reading books, especially science fiction.&quot;</span><br></pre></td></tr></table></figure></li></ul><p>print(“dataset:”, dataset)</p><p>dataset: DatasetDict({<br>train: Dataset({<br>features: [‘user 1 personas’, ‘user 2 personas’, ‘Best Generated Conversation’],<br>num_rows: 8938<br>})<br>validation: Dataset({<br>features: [‘user 1 personas’, ‘user 2 personas’, ‘Best Generated Conversation’],<br>num_rows: 1000<br>})<br>test: Dataset({<br>features: [‘user 1 personas’, ‘user 2 personas’, ‘Best Generated Conversation’],<br>num_rows: 968<br>})<br>})</p><p>dataset_train = dataset[“train”]</p><p>print(“dataset_train:”, dataset_train)</p><p>dataset_train: Dataset({<br>features: [‘user 1 personas’, ‘user 2 personas’, ‘Best Generated Conversation’],<br>num_rows: 8938<br>})</p><p>print(“dataset_train[‘Best Generated Conversation’][0]:\n”, dataset_train[“Best Generated Conversation”][0])</p><p>dataset_train[‘Best Generated Conversation’][0]:<br>User 1: Hi! I’m [user 1’s name].<br>User 2: Hi [user 1’s name], I’m [user 2’s name].<br>User 1: What do you do for fun?<br>User 2: I like to play video games, go to the beach, and read.<br>User 1: I like to play video games too! I’m not much of a reader, though.<br>User 2: What video games do you like to play?<br>User 1: I like to play a lot of different games, but I’m really into competitive online games right now.<br>User 2: I’m not really into competitive games, I like to play more relaxing games.<br>User 1: That’s cool. What kind of relaxing games do you like to play?<br>User 2: I like to play puzzle games, simulation games, and story-based games.<br>User 1: I’ve never been much of a puzzle game person, but I do like simulation games and story-based games.<br>User 2: Nice! What’s your favorite simulation game?<br>User 1: I like Stardew Valley a lot. It’s a farming game, but it’s also really relaxing and fun.<br>User 2: I’ve heard good things about that game. I might have to check it out.<br>User 1: You should! It’s a lot of fun.<br>User 2: Well, I’m glad we met. Maybe we can play some games together sometime.<br>User 1: That would be fun!<br>User 2: Great! I’ll send you my Steam name.<br>User 1: Ok, sounds good.</p><p>print(“dataset_train[‘user 1 personas’][0]:”, dataset_train[“user 1 personas”][0])</p><p>dataset_train[‘user 1 personas’][0]: I am 32.<br>I do not want a job.<br>I play video games all day.<br>I still live at home with my parents.</p><p>print(“dataset_train[‘user 2 personas’][0]:”, dataset_train[“user 2 personas”][0])</p><p>dataset_train[‘user 2 personas’][0]: My favorite drink is iced coffee.<br>I have a black belt in karate.<br>I m in a jazz band and play the saxophone.<br>I vacation along lake michigan every summer.</p><h4 id="tokenizer">tokenizer</h4><p>def tokenize_function(examples):</p><p>​        model_inputs = tokenizer(</p><p>​            examples[“input”],</p><p>​            max_length=128,</p><p>​            truncation=True,</p><p>​            padding=“max_length”,</p><p>​        )</p><p>​        with tokenizer.as_target_tokenizer():</p><p>​            labels = tokenizer(</p><p>​                examples[“target”],</p><p>​                max_length=128,</p><p>​                truncation=True,</p><p>​                padding=“max_length”,</p><p>​            )</p><p>​        model_inputs[“labels”] = labels[“input_ids”]#获得&quot;labels&quot; “input_ids” “attention_mask”。通过查阅forward函数，label要改名labels</p><p>​        return model_inputs</p><p>​    dataset_train_tokenized = dataset_train_tokenized.map(tokenize_function, batched=True)</p><h4 id="转化为mingnlp格式">转化为mingnlp格式</h4><p>import mindspore.dataset as ds</p><p>def data_generator(dataset):</p><p>​    for item in dataset:</p><p>​        yield item[“input_ids”], item[“attention_mask”], item[“labels”]</p><p>import mindspore.dataset as ds</p><p># 将训练集和验证集转换为 MindSpore 数据集，注意forward函数中label要改成labels</p><p>def create_mindspore_dataset(dataset, shuffle=True):</p><p>​    return ds.GeneratorDataset(</p><p>​        source=lambda: data_generator(dataset),  # 使用 lambda 包装生成器</p><p>​        column_names=[“input_ids”, “attention_mask”, “labels”],</p><p>​        shuffle=shuffle</p><p>​    )</p><p>dataset_train_tokenized = create_mindspore_dataset(dataset_train_tokenized, shuffle=True)</p><h3 id="二-环境准备与数据说明-10分钟"><strong>二、环境准备与数据说明（10分钟）</strong></h3><ol><li><p><strong>环境配置</strong></p><ul><li>MindNLP安装指南（结合昇腾NPU/GPU版本）</li><li>依赖库：mindspore, mindnlp, datasets, peft</li></ul></li><li><p><strong>数据准备</strong></p><ul><li><p>数据集示例：DailyDialog（日常对话）、Custom JSON格式数据</p></li><li><p>数据预处理：</p><p>python</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def format_dialogue(example):  </span><br><span class="line">    return &#123;&quot;text&quot;: f&quot;User: &#123;example[&#x27;input&#x27;]&#125;\nBot: &#123;example[&#x27;output&#x27;]&#125;&quot;&#125;  </span><br></pre></td></tr></table></figure></li><li><p>数据划分：8:1:1（训练/验证/测试）</p></li></ul></li></ol><hr><h3 id="三-模型加载与peft配置-15分钟"><strong>三、模型加载与PEFT配置（15分钟）</strong></h3><ol><li><p><strong>加载BlenderBot-Small预训练模型</strong></p><ul><li><p>MindNLP代码示例：</p><p>python</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from mindnlp.models import BlenderbotSmall  </span><br><span class="line">model = BlenderbotSmall.from_pretrained(&quot;blenderbot_small-90M&quot;)  </span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>PEFT参数注入（以LoRA为例）</strong></p><ul><li><p>关键参数解析（衔接前序PEFT知识）：</p><ul><li><code>target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;]</code>（选择注意力层）</li><li><code>r=8, lora_alpha=32</code>（平衡参数量与效果）</li><li><code>lora_dropout=0.1</code>（防止小数据过拟合）</li></ul></li><li><p>代码实现：</p><p>python</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from peft import LoraConfig, get_peft_model  </span><br><span class="line">peft_config = LoraConfig(  </span><br><span class="line">    r=8, lora_alpha=32,  </span><br><span class="line">    target_modules=[&quot;q_proj&quot;, &quot;v_proj&quot;],  </span><br><span class="line">    lora_dropout=0.1  </span><br><span class="line">)  </span><br><span class="line">model = get_peft_model(model, peft_config)  </span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>冻结原模型参数</strong></p><ul><li>验证参数冻结状态：<code>print_trainable_parameters(model)</code></li></ul></li></ol><hr><h3 id="四-训练流程与优化技巧-20分钟"><strong>四、训练流程与优化技巧（20分钟）</strong></h3><ol><li><p><strong>训练配置</strong></p><ul><li>优化器选择：<code>AdamWeightDecay</code>（MindSpore兼容）</li><li>学习率设置：<code>5e-4</code>（PEFT通常需更大学习率）</li><li>Batch Size与梯度累积：根据显存动态调整</li></ul></li><li><p><strong>训练代码核心片段</strong></p><p>python</p><p>复制</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from mindnlp.engine import Trainer, TrainingArguments  </span><br><span class="line">training_args = TrainingArguments(  </span><br><span class="line">    output_dir=&quot;./results&quot;,  </span><br><span class="line">    per_device_train_batch_size=4,  </span><br><span class="line">    num_train_epochs=3,  </span><br><span class="line">    learning_rate=5e-4,  </span><br><span class="line">    logging_steps=50  </span><br><span class="line">)  </span><br><span class="line">trainer = Trainer(  </span><br><span class="line">    model=model,  </span><br><span class="line">    args=training_args,  </span><br><span class="line">    train_dataset=train_dataset  </span><br><span class="line">)  </span><br><span class="line">trainer.train()  </span><br></pre></td></tr></table></figure></li><li><p><strong>性能监控与调试</strong></p><ul><li>使用MindInsight可视化训练曲线（Loss/Perplexity）</li><li>常见问题：<ul><li>显存不足：启用梯度检查点（<code>gradient_checkpointing=True</code>）</li><li>收敛慢：检查学习率与参数是否解冻</li></ul></li></ul></li></ol><hr><ol><li><ul><li></li></ul></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;讲解mindnlp模型微调20250329&lt;/h1&gt;
&lt;p&gt;先给大家说自己是三维视觉领域&lt;/p&gt;
&lt;p&gt;首先讲解mindnlp的组成，训练方法&lt;/p&gt;
&lt;p&gt;然后讲解模型任务，模型分为四种&lt;/p&gt;
&lt;p&gt;然后讲解微调数据集的选择，微调的两种方法&lt;/p&gt;
&lt;p&gt;然后讲解微调</summary>
      
    
    
    
    
  </entry>
  
</feed>
