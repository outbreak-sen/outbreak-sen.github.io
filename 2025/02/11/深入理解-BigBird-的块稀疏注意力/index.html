<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深入理解 BigBird 的块稀疏注意力 | This is a 部落格 of outbreak_sen</title><meta name="author" content="outbreak_sen"><meta name="copyright" content="outbreak_sen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深入理解 BigBird 的块稀疏注意力 文档来自 引言 基于 transformer 的模型已被证明对很多 NLP 任务都非常有用。然而，O(n2)O(n^2)O(n2) 的时间和内存复杂度 (其中 nnn 是序列长度) 使得在长序列 (n&gt;512n &gt; 512n&gt;512) 上应用它们变得非常昂贵，因而大大限制了其应用。最近的几篇论文，如 Longformer 、Perform">
<meta property="og:type" content="article">
<meta property="og:title" content="深入理解 BigBird 的块稀疏注意力">
<meta property="og:url" content="http://outbreak-sen.github.io/2025/02/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-BigBird-%E7%9A%84%E5%9D%97%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B/index.html">
<meta property="og:site_name" content="This is a 部落格 of outbreak_sen">
<meta property="og:description" content="深入理解 BigBird 的块稀疏注意力 文档来自 引言 基于 transformer 的模型已被证明对很多 NLP 任务都非常有用。然而，O(n2)O(n^2)O(n2) 的时间和内存复杂度 (其中 nnn 是序列长度) 使得在长序列 (n&gt;512n &gt; 512n&gt;512) 上应用它们变得非常昂贵，因而大大限制了其应用。最近的几篇论文，如 Longformer 、Perform">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://outbreak-sen.github.io/img/head.png">
<meta property="article:published_time" content="2025-02-11T01:51:43.000Z">
<meta property="article:modified_time" content="2025-03-19T09:10:27.313Z">
<meta property="article:author" content="outbreak_sen">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="mindspore实习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://outbreak-sen.github.io/img/head.png"><link rel="shortcut icon" href="/img/h_beautygirl.png"><link rel="canonical" href="http://outbreak-sen.github.io/2025/02/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-BigBird-%E7%9A%84%E5%9D%97%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":true},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深入理解 BigBird 的块稀疏注意力',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="This is a 部落格 of outbreak_sen" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg" style="background-image: url(/img/v_beautygirl0.jpeg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./img/head.png" onerror="onerror=null;src='./img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-camera-retro"></i><span> 树洞</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 作品与鉴赏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/h_beautygirl.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/h_beautygirl.png" alt="Logo"><span class="site-name">This is a 部落格 of outbreak_sen</span></a><a class="nav-page-title" href="/"><span class="site-name">深入理解 BigBird 的块稀疏注意力</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 时间轴</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-camera-retro"></i><span> 树洞</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 作品与鉴赏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深入理解 BigBird 的块稀疏注意力</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-11T01:51:43.000Z" title="发表于 2025-02-11 09:51:43">2025-02-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-03-19T09:10:27.313Z" title="更新于 2025-03-19 17:10:27">2025-03-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%9B%B8%E6%9C%BA%E9%A9%B1%E5%8A%A8%E5%BC%80%E5%8F%91/">相机驱动开发</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1>深入理解 BigBird 的块稀疏注意力</h1>
<p><a target="_blank" rel="noopener" href="https://github.com/huggingface/blog/blob/main/zh/big-bird.md">文档来自</a></p>
<h2 id="引言">引言</h2>
<p>基于 transformer 的模型已被证明对很多 NLP 任务都非常有用。然而，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>n</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> 的时间和内存复杂度 (其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span> 是序列长度) 使得在长序列 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>&gt;</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">n &gt; 512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span><span class="mord">1</span><span class="mord">2</span></span></span></span>) 上应用它们变得非常昂贵，因而大大限制了其应用。最近的几篇论文，如 <code>Longformer</code> 、<code>Performer</code> 、<code>Reformer</code> 、<code>簇状注意力</code> 都试图通过对完整注意力矩阵进行近似来解决这个问题。如果你不熟悉这些模型，可以查看 🤗 之前的 <a target="_blank" rel="noopener" href="https://huggingface.co/blog/zh/long-range-transformers">博文</a>。</p>
<p><code>BigBird</code> (由 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.14062">该论文</a> 引入) 是解决这个问题的最新模型之一。 <code>BigBird</code> 依赖于 <strong>块稀疏注意力</strong> 而不是普通注意力 ( <em>即</em> BERT 的注意力)，与 BERT 相比，这一新算法能以低得多的计算成本处理长达 <strong>4096</strong> 的序列。在涉及很长序列的各种任务上，该模型都实现了 SOTA，例如长文档摘要、长上下文问答。</p>
<p><strong>RoBERTa 架构的 BigBird</strong> 模型现已集成入 🤗 transformers 中。本文的目的是让读者 <strong>深入</strong> 了解 BigBird 的实现，并让读者能在 🤗 transformers 中轻松使用 BigBird。但是，在更深入之前，一定记住 <code>BigBird</code> 注意力只是 <code>BERT</code> 完全注意力的一个近似，因此我们并不纠结于让它比 <code>BERT</code> 完全注意力 <strong>更好</strong>，而是致力于让它更有效率。有了它，transformer 模型就可以作用于更长的序列，因为 BERT 的二次方内存需求很快会变得难以为继。简而言之，如果我们有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span> 计算和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∞</mi></mrow><annotation encoding="application/x-tex">\infty</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord">∞</span></span></span></span> 时间，那么用 BERT 注意力就好了，完全没必要用本文讨论的块稀疏注意力。</p>
<p>如果你想知道为什么在处理较长序列时需要更多计算，那么本文正合你意！</p>
<hr>
<p>在使用标准的 <code>BERT</code> 类注意力时可能会遇到以下几个主要问题:</p>
<ul>
<li>每个词元真的都必须关注所有其他词元吗？</li>
<li>为什么不只计算重要词元的注意力？</li>
<li>如何决定哪些词元重要？</li>
<li>如何以高效的方式处理少量词元？</li>
</ul>
<hr>
<p>本文，我们将尝试回答这些问题。</p>
<h3 id="应该关注哪些词元？">应该关注哪些词元？</h3>
<p>下面，我们将以句子 <code>BigBird is now available in HuggingFace for extractive Question Answering</code> 为例来说明注意力是如何工作的。在 <code>BERT</code> 这类的注意力机制中，每个词元都简单粗暴地关注所有其他词元。从数学上来讲，这意味着每个查询的词元 $ \text{query-token} \in {\text{BigBird},\text{is},\text{now},\text{available},\text{in},\text{HuggingFace},\text{for},\text{extractive},\text{question},\text{answering}} $,<br>
将关注每个键词元 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>key-tokens</mtext><mo>=</mo><mrow><mo fence="true">[</mo><mtext>BigBird</mtext><mo separator="true">,</mo><mtext>is</mtext><mo separator="true">,</mo><mtext>now</mtext><mo separator="true">,</mo><mtext>available</mtext><mo separator="true">,</mo><mtext>in</mtext><mo separator="true">,</mo><mtext>HuggingFace</mtext><mo separator="true">,</mo><mtext>for</mtext><mo separator="true">,</mo><mtext>extractive</mtext><mo separator="true">,</mo><mtext>question</mtext><mo separator="true">,</mo><mtext>answering</mtext><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{key-tokens} = \left[\text{BigBird},\text{is},\text{now},\text{available},\text{in},\text{HuggingFace},\text{for},\text{extractive},\text{question},\text{answering} \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord text"><span class="mord">key-tokens</span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;">[</span><span class="mord text"><span class="mord">BigBird</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">is</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">now</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">available</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">in</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">HuggingFace</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">for</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">extractive</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">question</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">answering</span></span><span class="mclose delimcenter" style="top:0em;">]</span></span></span></span></span>。</p>
<p>我们考虑一下 <code>每个查询词元应如何明智地选择它实际上应该关注的键词元</code> 这个问题，下面我们通过编写伪代码的方式来整理思考过程。</p>
<p>假设 <code>available</code> 是当前查询词元，我们来构建一个合理的、需要关注的键词元列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下面的句子为例</span></span><br><span class="line">example = [<span class="string">&#x27;BigBird&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;now&#x27;</span>, <span class="string">&#x27;available&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;HuggingFace&#x27;</span>, <span class="string">&#x27;for&#x27;</span>, <span class="string">&#x27;extractive&#x27;</span>, <span class="string">&#x27;question&#x27;</span>, <span class="string">&#x27;answering&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设当前需要计算 &#x27;available&#x27; 这个词的表征</span></span><br><span class="line">query_token = <span class="string">&#x27;available&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化一个空集合，用于放 &#x27;available&#x27; 这个词的键词元</span></span><br><span class="line">key_tokens = [] <span class="comment"># =&gt; 目前，&#x27;available&#x27; 词元不关注任何词元</span></span><br></pre></td></tr></table></figure>
<p>邻近词元当然很重要，因为在一个句子 (单词序列) 中，当前词高度依赖于前后的邻近词。<code>滑动注意力</code> 即基于该直觉。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 考虑滑动窗大小为 3, 即将 &#x27;available&#x27; 的左边一个词和右边一个词纳入考量</span></span><br><span class="line"><span class="comment"># 左词: &#x27;now&#x27;; 右词: &#x27;in&#x27;</span></span><br><span class="line">sliding_tokens = [<span class="string">&quot;now&quot;</span>, <span class="string">&quot;available&quot;</span>, <span class="string">&quot;in&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用以上词元更新集合</span></span><br><span class="line">key_tokens.append(sliding_tokens)</span><br></pre></td></tr></table></figure>
<p><strong>长程依赖关系:</strong> 对某些任务而言，捕获词元间的长程关系至关重要。 <em>例如</em> ，在问答类任务中，模型需要将上下文的每个词元与整个问题进行比较，以便能够找出上下文的哪一部分对正确答案有用。如果大多数上下文词元仅关注其他上下文词元，而不关注问题，那么模型从不太重要的上下文词元中过滤重要的上下文词元就会变得更加困难。</p>
<p><code>BigBird</code> 提出了两种允许长程注意力依赖的方法，这两种方法都能保证计算效率。</p>
<ul>
<li><strong>全局词元:</strong> 引入一些词元，这些词元将关注每个词元并且被每个词元关注。例如，对 <em>“HuggingFace is building nice libraries for easy NLP”</em> ，现在假设 <em>‘building’</em> 被定义为全局词元，而对某些任务而言，模型需要知道 <em>‘NLP’</em> 和 <em>‘HuggingFace’</em> 之间的关系 (注意: 这 2 个词元位于句子的两端); 现在让 <em>‘building’</em> 在全局范围内关注所有其他词元，会对模型将 <em>‘NLP’</em> 与 <em>‘HuggingFace’</em> 关联起来有帮助。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们假设第一个和最后一个词元是全局的，则有:</span></span><br><span class="line">global_tokens = [<span class="string">&quot;BigBird&quot;</span>, <span class="string">&quot;answering&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将全局词元加入到集合中</span></span><br><span class="line">key_tokens.append(global_tokens)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>随机词元:</strong> 随机选择一些词元，这些词元将通过关注其他词元来传输信息，而那些词元又可以传输信息到其他词元。这可以降低直接从一个词元到另一个词元的信息传输成本。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 现在，我们可以从句子中随机选择 `r` 个词元。这里，假设 `r` 为 1， 选择了 `is` 这个词元</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>random_tokens = [<span class="string">&quot;is&quot;</span>] <span class="comment"># 注意: 这个是完全随机选择的，因此可以是任意词元。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将随机词元加入到集合中</span></span><br><span class="line">key_tokens.append(random_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在看下 `key_tokens` 集合中有哪些词元</span></span><br><span class="line">key_tokens</span><br><span class="line">&#123;<span class="string">&#x27;now&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;in&#x27;</span>, <span class="string">&#x27;answering&#x27;</span>, <span class="string">&#x27;available&#x27;</span>, <span class="string">&#x27;BigBird&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 至此，查询词 &#x27;available&#x27; 仅关注集合中的这些词元，而不用关心全部</span></span><br></pre></td></tr></table></figure>
<p>这样，查询词元仅关注所有词元的一个子集，该子集能够产生完全注意力值的一个不错的近似。相同的方法将用于所有其他查询词元。但请记住，这里的重点是尽可能有效地接近 <code>BERT</code> 的完全注意力。BERT 那种简单地让每个查询词元关注所有键词元的做法可以建模为一系列矩阵乘法，从而在现代硬件 (如 GPU) 上进行高效计算。然而，滑动、全局和随机注意力的组合似乎意味着稀疏矩阵乘法，这在现代硬件上很难高效实现。<code>BigBird</code> 的主要贡献之一是提出了 <code>块稀疏</code> 注意力机制，该机制可以高效计算滑动、全局和随机注意力。我们来看看吧！</p>
<h3 id="图解全局-滑动-随机注意力的概念">图解全局、滑动、随机注意力的概念</h3>
<p>首先，我们借助图来帮助理解“全局”、“滑动”和“随机”注意力，并尝试理解这三种注意力机制的组合是如何较好地近似标准 BERT 类注意力的。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/global.png" width=250 height=250>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/sliding.png" width=250 height=250>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/random.png" width=250 height=250> <br>
<p><em>上图分别把“全局”(左) 、“滑动”(中) 和“随机”(右) 连接建模成一个图。每个节点对应一个词元，每条边代表一个注意力分数。如果 2 个词元之间没有边连接，则其注意力分数为 0。</em></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/graph.gif" alt=""></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/full.png" width=230 height=230>
<p><strong>BigBird 块稀疏注意力</strong> 是滑动连接、全局连接和随机连接 (总共 10 个连接) 的组合，如上图左侧动图所示。而 <strong>完全注意力</strong> 图 (右侧) 则是有全部 15 个连接 (注意: 总共有 6 个节点)。你可以简单地将完全注意力视为所有词元都是全局词元 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">{}^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span>。</p>
<p><strong>完全注意力:</strong> 模型可以直接在单个层中将信息从一个词元传输到另一个词元，因为每个词元都会对每个其他词元进行查询，并且受到其他每个词元的关注。我们考虑一个与上图类似的例子，如果模型需要将 <em>‘going’</em> 与 <em>‘now’</em> 关联起来，它可以简单地在单层中执行此操作，因为它们两个是有直接连接的。</p>
<p><strong>块稀疏注意力:</strong> 如果模型需要在两个节点 (或词元) 之间共享信息，则对于某些词元，信息将必须经过路径中的各个其他节点; 因为不是所有节点都有直接连接的。<br>
<em>例如</em> ，假设模型需要将 <code>going</code> 与 <code>now</code> 关联起来，那么如果仅存在滑动注意力，则这两个词元之间的信息流由路径 <code>going -&gt; am -&gt; i -&gt; now</code> 来定义，也就是说它必须经过 2 个其他词元。因此，我们可能需要多个层来捕获序列的全部信息，而正常的注意力可以在单层中捕捉到这一点。在极端情况下，这可能意味着需要与输入词元一样多的层。然而，如果我们引入一些全局词元，信息可以通过以下路径传播 <code>going -&gt; i -&gt; now</code> ，这可以帮助缩短路径。如果我们再另外引入随机连接，它就可以通过 <code>going -&gt; am -&gt; now</code> 传播。借助随机连接和全局连接，信息可以非常快速地 (只需几层) 从一个词元传输到下一个词元。</p>
<p>如果我们有很多全局词元，那么我们可能不需要随机连接，因为信息可以通过多个短路径传播。这就是在使用 BigBird 的变体 (称为 ETC) 时设置 <code>num_random_tokens = 0</code> 的动机 (稍后部分将会详细介绍)。</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">{}^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord"></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 在这些图中，我们假设注意力矩阵是对称的 <strong>即</strong> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">A</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><msub><mi mathvariant="bold">A</mi><mrow><mi>j</mi><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mathbf{A} _{ij} = \mathbf{A}_ {ji}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.972218em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">A</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.972218em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">A</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> 因为在图中如果某个词元 <strong>A</strong> 关注 <strong>B</strong>，那么 <strong>B</strong> 也会关注 <strong>A</strong>。从下一节所示的注意力矩阵图中可以看出，这个假设对于 BigBird 中的大多数词元都成立。</p>
<table>
<thead>
<tr>
<th>注意力类型</th>
<th>全局词元</th>
<th>滑动词元</th>
<th>随机词元</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始完全注意力</td>
<td><code>n</code></td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>块稀疏注意力</td>
<td>2 x <code>block_size</code></td>
<td>3 x <code>block_size</code></td>
<td><code>num_random_blocks</code> x <code>block_size</code></td>
</tr>
</tbody>
</table>
<p>原始完全注意力即 <code>BERT</code> 的注意力，而块稀疏注意力则是 <code>BigBird</code> 的注意力。想知道 <code>block_size</code> 是什么？请继续阅读下文。<em>现在，为简单起见，将其视为 1。</em></p>
<h2 id="bigbird-块稀疏注意力">BigBird 块稀疏注意力</h2>
<p>BigBird 块稀疏注意力是我们上文讨论的内容的高效实现。每个词元都关注某些 <strong>全局词元</strong> 、 <strong>滑动词元</strong> 和 <strong>随机词元</strong>，而不管其他 <strong>所有</strong> 词元。作者分别实现了每类查询注意力矩阵，并使用了一个很酷的技巧来加速 GPU 和 TPU 上的训练/推理。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/attn.png" alt="BigBird 块稀疏注意力"></p>
<p><em>注意: 在上图的顶部有 2 个额外的句子。正如你所注意到的，两个句子中的每个词元都只是交换了一个位置。这就是滑动注意力的实现方式。当 <code>q[i]</code> 与 <code>k[i,0:3]</code> 相乘时，我们会得到 <code>q[i]</code> 的滑动注意力分数 (其中<code>i</code> 是序列中元素的索引)。</em></p>
<p>你可以在 <a target="_blank" rel="noopener" href="https://github.com/vasudevgupta7/transformers/blob/5f2d6a0c93ca2017961199aa04a344b9b779d454/src/transformers/models/big_bird/modeling_big_bird.py#L513">这儿</a> 找到 <code>block_sparse</code> 注意力的具体实现。现在看起来可能非常可怕😨😨，但这篇文章肯定会让你轻松理解它。</p>
<h3 id="全局注意力">全局注意力</h3>
<p>对于全局注意力而言，每个查询词元关注序列中的所有其他词元，并且被其他每个词元关注。我们假设 <code>Vasudev</code> (第一个词元) 和 <code>them</code> (最后一个词元) 是全局的 (如上图所示)。你可以看到这些词元直接连接到所有其他词元 (蓝色框)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 伪代码</span></span><br><span class="line"></span><br><span class="line">Q -&gt; Query martix (seq_length, head_dim)</span><br><span class="line">K -&gt; Key matrix (seq_length, head_dim)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个和最后一个词元关注所有其他词元</span></span><br><span class="line">Q[<span class="number">0</span>] x [K[<span class="number">0</span>], K[<span class="number">1</span>], K[<span class="number">2</span>], ......, K[n-<span class="number">1</span>]]</span><br><span class="line">Q[n-<span class="number">1</span>] x [K[<span class="number">0</span>], K[<span class="number">1</span>], K[<span class="number">2</span>], ......, K[n-<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个和最后一个词元也被其他所有词元关注</span></span><br><span class="line">K[<span class="number">0</span>] x [Q[<span class="number">0</span>], Q[<span class="number">1</span>], Q[<span class="number">2</span>], ......, Q[n-<span class="number">1</span>]]</span><br><span class="line">K[n-<span class="number">1</span>] x [Q[<span class="number">0</span>], Q[<span class="number">1</span>], Q[<span class="number">2</span>], ......, Q[n-<span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<h3 id="滑动注意力">滑动注意力</h3>
<p>键词元序列被复制两次，其中一份每个词元向右移动一步，另一份每个词元向左移动一步。现在，如果我们将查询序列向量乘以这 3 个序列向量，我们将覆盖所有滑动词元。计算复杂度就是 <code>O(3n) = O(n)</code> 。参考上图，橙色框代表滑动注意力。你可以在图的顶部看到 3 个序列，其中 2 个序列各移动了一个词元 (1 个向左，1 个向右)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们想做的</span></span><br><span class="line">Q[i] x [K[i-<span class="number">1</span>], K[i], K[i+<span class="number">1</span>]] <span class="keyword">for</span> i = <span class="number">1</span>:-<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 高效的代码实现 (👇 乘法为点乘)</span></span><br><span class="line">[Q[<span class="number">0</span>], Q[<span class="number">1</span>], Q[<span class="number">2</span>], ......, Q[n-<span class="number">2</span>], Q[n-<span class="number">1</span>]] x [K[<span class="number">1</span>], K[<span class="number">2</span>], K[<span class="number">3</span>], ......, K[n-<span class="number">1</span>], K[<span class="number">0</span>]]</span><br><span class="line">[Q[<span class="number">0</span>], Q[<span class="number">1</span>], Q[<span class="number">2</span>], ......, Q[n-<span class="number">1</span>]] x [K[n-<span class="number">1</span>], K[<span class="number">0</span>], K[<span class="number">1</span>], ......, K[n-<span class="number">2</span>]]</span><br><span class="line">[Q[<span class="number">0</span>], Q[<span class="number">1</span>], Q[<span class="number">2</span>], ......, Q[n-<span class="number">1</span>]] x [K[<span class="number">0</span>], K[<span class="number">1</span>], K[<span class="number">2</span>], ......, K[n-<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个序列被乘 3 词， 即 `window_size = 3`。为示意，仅列出主要计算，省略了一些计算。</span></span><br></pre></td></tr></table></figure>
<h3 id="随机注意力">随机注意力</h3>
<p>随机注意力确保每个查询词元也会关注一些随机词元。对实现而言，这意味着模型随机选取一些词元并计算它们的注意力分数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># r1, r2, r 为随机索引; 注意 r1, r2, r 每行取值不同 👇</span></span><br><span class="line">Q[<span class="number">1</span>] x [Q[r1], Q[r2], ......, Q[r]]</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">.</span><br><span class="line">Q[n-<span class="number">2</span>] x [Q[r1], Q[r2], ......, Q[r]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不用管第 0 个和第 n-1 个词元，因为它们已经是全局词元了。</span></span><br></pre></td></tr></table></figure>
<p><strong>注意:</strong> 当前的实现进一步将序列划分为块，并且每个符号都依块而定义而非依词元而定义。我们在下一节中会更详细地讨论这个问题。</p>
<h3 id="实现">实现</h3>
<p><strong>回顾:</strong> 在常规 BERT 注意力中，一系列词元，即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>…</mo><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">X = x_1, x_2, …., x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 通过线性层投影到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mtext>，</mtext><mi>K</mi><mtext>，</mtext><mi>V</mi></mrow><annotation encoding="application/x-tex">Q，K，V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">Q</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.22222em;">V</span></span></span></span>，并基于它们计算注意力分数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">Z</span></span></span></span>，公式为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Z=Softmax(QK^T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">Z</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord mathdefault">Q</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。使用 BigBird 块稀疏注意力时，我们使用相同的算法，但仅针对一些选定的查询和键向量进行计算。</p>
<p>我们来看看 BigBird 块稀疏注意力是如何实现的。首先，我们用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mtext>、</mtext><mi>r</mi><mtext>、</mtext><mi>s</mi><mtext>、</mtext><mi>g</mi></mrow><annotation encoding="application/x-tex">b、r、s、g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">b</span><span class="mord cjk_fallback">、</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord cjk_fallback">、</span><span class="mord mathdefault">s</span><span class="mord cjk_fallback">、</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span> 分别代表 <code>block_size</code> 、<code>num_random_blocks</code> 、<code>num_sliding_blocks</code> 、<code>num_global_blocks</code> 。我们以 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mn>4</mn><mtext>，</mtext><mi>r</mi><mo>=</mo><mn>1</mn><mtext>，</mtext><mi>g</mi><mo>=</mo><mn>2</mn><mtext>，</mtext><mi>s</mi><mo>=</mo><mn>3</mn><mtext>，</mtext><mi>d</mi><mo>=</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">b=4，r=1，g=2，s=3，d=5</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">2</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord">3</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">5</span></span></span></span> 为例来说明 BigBird 块稀疏注意力的机制部分，如下所示:</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/intro.png" width=500 height=250>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub><mtext>、</mtext><msub><mi>q</mi><mn>2</mn></msub><mtext>、</mtext><msub><mi>q</mi><mrow><mn>3</mn><mo>:</mo><mi>n</mi><mo>−</mo><mn>2</mn></mrow></msub><mtext>、</mtext><msub><mi>q</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mtext>、</mtext><msub><mi>q</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">{q} _{1}、{q}_ {2}、{q} _{3:n-2}、{q}_ {n-1}、{q}_{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mrel mtight">:</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">、</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的注意力分数分别计算如下:</p>
<hr>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_{1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的注意力分数由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">a_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>1</mn></msub><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mn>1</mn></msub><mo>∗</mo><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a_1=Softmax(q_1 * K^T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，即为第一块中的所有词元与序列中的所有其他词元之间的注意力分数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/q1.png" alt="BigBird 块稀疏注意力"></p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">q_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示第 1 块，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">g_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示第 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span> 块。我们仅在 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">q_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和  <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span></span></span></span> (即所有键) 之间执行正常的注意力操作。</p>
<hr>
<p>为了计算第二块中词元的注意力分数，我们收集前三块、最后一块和第五块。然后我们可以计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mn>2</mn></msub><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mn>2</mn></msub><mo>∗</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>7</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a_2 = Softmax(q_2 * concat(k_1, k_2, k_3, k_5, k_7))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span>。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/q2.png" alt="BigBird 块稀疏注意力"></p>
<p><em>这里，我用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mtext>，</mtext><mi>r</mi><mtext>，</mtext><mi>s</mi></mrow><annotation encoding="application/x-tex">g，r，s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mord cjk_fallback">，</span><span class="mord mathdefault">s</span></span></span></span> 表示词元只是为了明确地表示它们的性质 (即是全局、随机还是滑动词元)，只用 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span></span> 无法表示他们各自的性质。</em></p>
<hr>
<p>为了计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mrow><mn>3</mn><mo>:</mo><mi>n</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">{q} _{3:n-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mrel mtight">:</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 的注意力分数，我们先收集相应的全局、滑动、随机键向量，并基于它们正常计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mrow><mn>3</mn><mo>:</mo><mi>n</mi><mo>−</mo><mn>2</mn></mrow></msub></mrow><annotation encoding="application/x-tex">{q}_ {3:n-2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mrel mtight">:</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 上的注意力。请注意，正如前面滑动注意力部分所讨论的，滑动键是使用特殊的移位技巧来收集的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/q_middle.png" alt="BigBird 块稀疏注意力"></p>
<hr>
<p>为了计算倒数第二块 (即 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">{q} _{n-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span>) 中词元的注意力分数，我们收集第一块、最后三块和第三块的键向量。然后我们用公式 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>∗</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>c</mi><mi>a</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>k</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>3</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>5</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>6</mn></msub><mo separator="true">,</mo><msub><mi>k</mi><mn>7</mn></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{a}_ {n-1} = Softmax({q}_{n-1} * concat(k_1, k_3, k_5, k_6, k_7))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.638891em;vertical-align:-0.208331em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">a</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">c</span><span class="mord mathdefault">o</span><span class="mord mathdefault">n</span><span class="mord mathdefault">c</span><span class="mord mathdefault">a</span><span class="mord mathdefault">t</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03148em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">7</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> 进行计算。这和计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">q_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 非常相似。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/qlast_sec.png" alt="BigBird 块稀疏注意力"></p>
<hr>
<p>最后一块 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">q</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">\mathbf{q}_{n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.63888em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord"><span class="mord mathbf">q</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的注意力分数由 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">a_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 表示，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>a</mi><mi>n</mi></msub><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><msub><mi>q</mi><mi>n</mi></msub><mo>∗</mo><msup><mi>K</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">a_n=Softmax(q_n * K^T)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">a</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，只不过是最后一块中的所有词元与序列中的所有其他词元之间的注意力分数。这与我们对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>q</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">q_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 所做的非常相似。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/qlast.png" alt="BigBird 块稀疏注意力"></p>
<hr>
<p>我们将上面的矩阵组合起来得到最终的注意力矩阵。该注意力矩阵可用于获取所有词元的表征。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/blog/assets/18_big_bird/block-sparse-attn.gif" alt="BigBird 块稀疏注意力"></p>
<p><em>上图中 <code>蓝色 -&gt; 全局块</code> 、<code>红色 -&gt; 随机块</code> 、<code>橙色 -&gt; 滑动块</code> 。在前向传播过程中，我们不存储“白色”块，而是直接为每个单独的部分计算加权值矩阵 (即每个词元的表示)，如上所述。</em></p>
<p>现在，我们已经介绍了块稀疏注意力最难的部分，即它的实现。希望对你更好地理解实际代码有帮助。现在你可以深入研究代码了，在此过程中你可以将代码的每个部分与上面的某个部分联系起来以助于理解。</p>
<h2 id="时间和内存复杂度">时间和内存复杂度</h2>
<table>
<thead>
<tr>
<th>注意力类型</th>
<th>序列长度</th>
<th>时间和内存复杂度</th>
</tr>
</thead>
<tbody>
<tr>
<td>原始完全注意力</td>
<td>512</td>
<td><code>T</code></td>
</tr>
<tr>
<td></td>
<td>1024</td>
<td>4 x <code>T</code></td>
</tr>
<tr>
<td></td>
<td>4096</td>
<td>64 x <code>T</code></td>
</tr>
<tr>
<td>块稀疏注意力</td>
<td>1024</td>
<td>2 x <code>T</code></td>
</tr>
<tr>
<td></td>
<td>4096</td>
<td>8 x <code>T</code></td>
</tr>
</tbody>
</table>
<p><em>BERT 注意力和 BigBird 块稀疏注意力的时间和空间复杂度之比较。</em></p>
<details>
<summary> 展开以了解复杂度的计算过程。</summary>
<figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">BigBird 时间复杂度 = O(w x n + r x n + g x n)</span><br><span class="line">BERT 时间复杂度 = O(n^2)</span><br><span class="line"></span><br><span class="line">假设:</span><br><span class="line"><span class="code">    w = 3 x 64</span></span><br><span class="line"><span class="code">    r = 3 x 64</span></span><br><span class="line"><span class="code">    g = 2 x 64</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">当序列长度为 512 时</span><br><span class="line">=&gt; <span class="strong">**BERT 时间复杂度 = 512^2**</span></span><br><span class="line"></span><br><span class="line">当序列长度为 1024 时</span><br><span class="line">=&gt; BERT 时间复杂度 = (2 x 512)^2</span><br><span class="line">=&gt; <span class="strong">**BERT 时间复杂度 = 4 x 512^2**</span></span><br><span class="line"></span><br><span class="line">=&gt; BigBird 时间复杂度 = (8 x 64) x (2 x 512)</span><br><span class="line">=&gt; <span class="strong">**BigBird 时间复杂度 = 2 x 512^2**</span></span><br><span class="line"></span><br><span class="line">当序列长度为 4096 时</span><br><span class="line">=&gt; BERT 时间复杂度 = (8 x 512)^2</span><br><span class="line">=&gt; <span class="strong">**BERT 时间复杂度 = 64 x 512^2**</span></span><br><span class="line"></span><br><span class="line">=&gt; BigBird 时间复杂度 = (8 x 64) x (8 x 512)</span><br><span class="line">=&gt; BigBird 时间复杂度 = 8 x (512 x 512)</span><br><span class="line">=&gt; <span class="strong">**BigBird 时间复杂度 = 8 x 512^2**</span></span><br></pre></td></tr></table></figure>
</details>
<h2 id="itc-与-etc">ITC 与 ETC</h2>
<p>BigBird 模型可以使用 2 种不同的策略进行训练: <strong>ITC</strong> 和 <strong>ETC</strong>。 ITC (internal transformer construction，内部 transformer 构建) 就是我们上面讨论的。在 ETC (extended transformer construction，扩展 transformer 构建) 中，会有更多的全局词元，以便它们关注所有词元或者被所有词元关注。</p>
<p>ITC 需要的计算量较小，因为很少有词元是全局的，同时模型可以捕获足够的全局信息 (也可以借助随机注意力)。而 ETC 对于需要大量全局词元的任务非常有帮助，例如对 <strong>问答</strong> 类任务而言，整个问题应该被所有上下文关注，以便能够将上下文正确地与问题相关联。</p>
<p><em><strong>注意:</strong> BigBird 论文显示，在很多 ETC 实验中，随机块的数量设置为 0。考虑到我们上文图解部分的讨论，这是合理的。</em></p>
<p>下表总结了 ITC 和 ETC:</p>
<table>
<thead>
<tr>
<th></th>
<th>ITC</th>
<th>ETC</th>
</tr>
</thead>
<tbody>
<tr>
<td>全局注意力的注意力矩阵</td>
<td>\( A = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \ 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \)</td>
<td>\( B = \begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; &amp; &amp; &amp; &amp; &amp; 1 \ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 \end{bmatrix} \)</td>
</tr>
<tr>
<td>全局词元</td>
<td>2 x <code>block_size</code></td>
<td><code>extra_tokens</code> + 2 x <code>block_size</code></td>
</tr>
<tr>
<td>随机词元</td>
<td><code>num_random_blocks</code> x <code>block_size</code></td>
<td><code>num_random_blocks</code> x <code>block_size</code></td>
</tr>
<tr>
<td>滑动词元</td>
<td>3 x <code>block_size</code></td>
<td>3 x <code>block_size</code></td>
</tr>
</tbody>
</table>
<h2 id="在-🤗transformers-中使用-bigbird">在  🤗Transformers 中使用 BigBird</h2>
<p>你可以像使用任何其他 🤗 模型一样使用 <code>BigBirdModel</code> 。我们看一下代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdModel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从预训练 checkpoint 中加载 bigbird 模型</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/bigbird-roberta-base&quot;</span>)</span><br><span class="line"><span class="comment"># 使用默认配置初始化模型，如 attention_type = &quot;block_sparse&quot;，num_random_blocks = 3，block_size = 64</span></span><br><span class="line"><span class="comment"># 你也可以按照自己的需要改变这些参数。这 3 个参数只改变每个查询词元关注的词元数。</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/bigbird-roberta-base&quot;</span>, num_random_blocks=<span class="number">2</span>, block_size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过把 attention_type 设成 `original_full`，BigBird 就会用复杂度为 n^2 的完全注意力。此时，BigBird 与 BERT 相似度为 99.9%。</span></span><br><span class="line">model = BigBirdModel.from_pretrained(<span class="string">&quot;google/bigbird-roberta-base&quot;</span>, attention_type=<span class="string">&quot;original_full&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>截至现在， <strong>🤗 Hub</strong> 中总共有 <strong>3 个 BigBird checkpoint</strong>: <a target="_blank" rel="noopener" href="https://huggingface.co/google/bigbird-roberta-base"><code>bigbird-roberta-base</code></a>，<a target="_blank" rel="noopener" href="https://huggingface.co/google/bigbird-roberta-large"><code>bigbird-roberta-large</code></a> 以及 <a target="_blank" rel="noopener" href="https://huggingface.co/google/bigbird-base-trivia-itc"><code>bigbird-base-trivia-itc</code></a>。前两个检查点是使用 <code>masked_lm 损失</code> 预训练 <code>BigBirdForPretraining</code> 而得; 而最后一个是在 <code>trivia-qa</code> 数据集上微调 <code>BigBirdForQuestionAnswering</code> 而得。</p>
<p>让我们看一下如果用你自己喜欢的 PyTorch 训练器，最少需要多少代码就可以使用 🤗 的 BigBird 模型来微调你自己的任务。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以问答任务为例</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BigBirdForQuestionAnswering, BigBirdTokenizer</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们用预训练权重初始化 bigbird 模型，并随机初始化其头分类器</span></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;google/bigbird-roberta-base&quot;</span>, block_size=<span class="number">64</span>, num_random_blocks=<span class="number">3</span>)</span><br><span class="line">tokenizer = BigBirdTokenizer.from_pretrained(<span class="string">&quot;google/bigbird-roberta-base&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line">dataset = <span class="string">&quot;torch.utils.data.DataLoader object&quot;</span></span><br><span class="line">optimizer = <span class="string">&quot;torch.optim object&quot;</span></span><br><span class="line">epochs = ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最简训练循环</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataset:</span><br><span class="line">        model.train()</span><br><span class="line">        batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向</span></span><br><span class="line">        output = model(**batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后向</span></span><br><span class="line">        output[<span class="string">&quot;loss&quot;</span>].backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将最终权重存至本地目录</span></span><br><span class="line">model.save_pretrained(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将权重推到 🤗 Hub 中</span></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> ModelHubMixin</span><br><span class="line">ModelHubMixin.push_to_hub(<span class="string">&quot;&lt;YOUR-WEIGHTS-DIR&gt;&quot;</span>, model_id=<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用微调后的模型，以用于推理</span></span><br><span class="line">question = [<span class="string">&quot;How are you doing?&quot;</span>, <span class="string">&quot;How is life going?&quot;</span>]</span><br><span class="line">context = [<span class="string">&quot;&lt;some big context having ans-1&gt;&quot;</span>, <span class="string">&quot;&lt;some big context having ans-2&gt;&quot;</span>]</span><br><span class="line">batch = tokenizer(question, context, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">batch = &#123;k: batch[k].to(device) <span class="keyword">for</span> k <span class="keyword">in</span> batch&#125;</span><br><span class="line"></span><br><span class="line">model = BigBirdForQuestionAnswering.from_pretrained(<span class="string">&quot;&lt;YOUR-FINETUNED-ID&gt;&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    start_logits, end_logits = model(**batch).to_tuple()</span><br><span class="line">    <span class="comment"># 这里，你可以使用自己的策略对 start_logits，end_logits 进行解码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意:</span></span><br><span class="line"><span class="comment"># 该代码段仅用于展示即使你想用自己的 PyTorch 训练器微调 BigBrid，这也是相当容易的。</span></span><br><span class="line"><span class="comment"># 我会建议使用 🤗 Trainer，它更简单，功能也更多。</span></span><br></pre></td></tr></table></figure>
<p>使用 BigBird 时，需要记住以下几点:</p>
<ul>
<li>序列长度必须是块大小的倍数，即 <code>seqlen % block_size = 0</code> 。你不必担心，因为如果 batch 的序列长度不是 <code>block_size</code> 的倍数，🤗 transformers 会自动填充至最近的整数倍。</li>
<li>目前，Hugging Face 的实现 <strong>尚不支持 ETC</strong>，因此只有第一个和最后一个块是全局的。</li>
<li>当前实现不支持 <code>num_random_blocks = 0</code> 。</li>
<li>论文作者建议当序列长度 &lt; 1024 时设置 <code>attention_type = &quot;original_full&quot;</code> 。</li>
<li>必须满足: <code>seq_length &gt; global_token + random_tokens + moving_tokens + buffer_tokens</code> ，其中 <code>global_tokens = 2 x block_size</code> 、 <code>sliding_tokens = 3 x block_size</code> 、 <code>random_tokens = num_random_blocks x block_size</code> 且 <code>buffer_tokens = num_random_blocks x block_size</code> 。如果你不能满足这一点，🤗 transformers 会自动将 <code>attention_type</code> 切换为 <code>original_full</code> 并告警。</li>
<li>当使用 BigBird 作为解码器 (或使用 <code>BigBirdForCasualLM</code> ) 时， <code>attention_type</code> 应该是 <code>original_full</code> 。但你不用担心，🤗 transformers 会自动将 <code>attention_type</code> 切换为 <code>original_full</code> ，以防你忘记这样做。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://outbreak-sen.github.io">outbreak_sen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://outbreak-sen.github.io/2025/02/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-BigBird-%E7%9A%84%E5%9D%97%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B/">http://outbreak-sen.github.io/2025/02/11/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-BigBird-%E7%9A%84%E5%9D%97%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://outbreak-sen.github.io" target="_blank">This is a 部落格 of outbreak_sen</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/mindspore%E5%AE%9E%E4%B9%A0/">mindspore实习</a></div><div class="post-share"><div class="social-share" data-image="/./img/head.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赞助</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/02/11/%E6%98%93%E7%94%A8%E6%80%A7mint%E7%AE%97%E5%AD%90%E6%B5%8B%E8%AF%95%E6%97%A5%E8%AE%B0/" title="mindspore.mint接口测试任务"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">mindspore.mint接口测试任务</div></div><div class="info-2"><div class="info-item-1">mindspore.mint接口测试任务 任务 任务背景 mindspore.mint提供了大量的functional、nn、优化器接口，API用法及功能等与业界主流用法一致，方便用户参考使用。 mint接口当前是实验性接口，在图编译模式为O0和PyNative模式下性能比ops更优。当前暂不支持图下沉模式及CPU、GPU后端，后续会逐步完善。 mindspore.mint.div逐元素计算 input 除以 other 的商。 mindspore.mint.divide mindspore.mint.erf mindspore.mint.erfc mindspore.mint.erfinv 需求描述  对应Pytorch 的相应接口进行测试： a) 测试random输入不同dtype，对比两个框架的支持度 b) 测试固定dtype，random输入值，对比两个框架输出是否相等（误差范围为小于1e-3） c) 测试固定shape，固定输入值，不同输入参数（string\bool等类型），两个框架的支持度 d)...</div></div></div></a><a class="pagination-related" href="/2025/02/10/Mindspore%E5%AE%9E%E4%B9%A0-AKG%20SIG%E7%AE%97%E5%AD%90addlayernorm%E7%BC%96%E8%BE%91%E5%92%8C%E5%90%88%E5%B9%B6/" title="Mindspore实习-AKG SIG算子addlayernorm编辑和合并"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Mindspore实习-AKG SIG算子addlayernorm编辑和合并</div></div><div class="info-2"><div class="info-item-1">Mindspore实习-AKG SIG算子addlayernorm编辑和合并 什么是算子 计算图和算子在计算本质上是一致的。算子是打包后的计算图，计算图是拆包后的算子 比如sigmod复合算子可以看作一个计算图，由基础算子Exp，Add，Reciprocal等基础算子组成，用小规模的“基本算子”集合就可以表达任意现有计算图。 计算图可以完全由基础算子组成，但是还是得定义复合算子，比如sigmod这种，因为对于基本算子计算图来说，相邻算子之间只能通过全局内存（或显存）进行数据传递。而对于复合算子来说，相邻的基本计算之间则可以通过局部内存或者寄存器进行数据传递。除了性能之外，在一些场景下，通过算子融合也能有效减少对全局内存的的实际占用。 mindspore的算子融合方案是什么 在TVM、XLA等自动算子编译技术出现之前，AI框架主流采用手工融合的方式解决如上问题。主要思路是：   手工融合: 1）识别常见的热点算子组合子图，比如： Add(Mul(x, y))。然后针对该算子子图手工实现对应融合算子；...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/02/11/%E6%98%93%E7%94%A8%E6%80%A7mint%E7%AE%97%E5%AD%90%E6%B5%8B%E8%AF%95%E6%97%A5%E8%AE%B0/" title="mindspore.mint接口测试任务"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">mindspore.mint接口测试任务</div></div><div class="info-2"><div class="info-item-1">mindspore.mint接口测试任务 任务 任务背景 mindspore.mint提供了大量的functional、nn、优化器接口，API用法及功能等与业界主流用法一致，方便用户参考使用。 mint接口当前是实验性接口，在图编译模式为O0和PyNative模式下性能比ops更优。当前暂不支持图下沉模式及CPU、GPU后端，后续会逐步完善。 mindspore.mint.div逐元素计算 input 除以 other 的商。 mindspore.mint.divide mindspore.mint.erf mindspore.mint.erfc mindspore.mint.erfinv 需求描述  对应Pytorch 的相应接口进行测试： a) 测试random输入不同dtype，对比两个框架的支持度 b) 测试固定dtype，random输入值，对比两个框架输出是否相等（误差范围为小于1e-3） c) 测试固定shape，固定输入值，不同输入参数（string\bool等类型），两个框架的支持度 d)...</div></div></div></a><a class="pagination-related" href="/2025/02/09/EffiMVS%E5%85%A8%E6%96%87%E7%BF%BB%E8%AF%91/" title="EffiMVS全文翻译"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-09</div><div class="info-item-2">EffiMVS全文翻译</div></div><div class="info-2"><div class="info-item-1">Effi MVS翻译 摘要 在本文中，我们提出了一种用于多视图立体的新颖的迭代动态成本量。 与其他作品相比，我们的成本量要轻得多，因此可以使用基于 2D 卷积的 GRU 进行处理。 值得注意的是，每一步GRU 的输出可进一步用于生成新的成本量。 这样，就构建了一个基于GRU的迭代优化器。 此外，我们提出了一种级联和分层细化架构来利用多尺度信息并加速收敛。 具体来说，利用轻量级 3D CNN 生成最粗糙的初始深度图，这对于启动 GRU 并保证快速收敛至关重要。 然后，深度图由作用于金字塔特征图的多级 GRU 进行细化。 对 DTU 和 Tanks &amp; Temples 基准测试的大量实验表明，我们的方法可以在准确性、速度和内存使用方面实现最先进的结果。 介绍 多视图立体 (MVS) 旨在基于一系列姿势图像和相应的相机参数重建密集的 3D 模型。 预测深度图，然后将深度图融合到点云模型中是 MVS 最常见的流程。 作为一个基本问题，MVS 在计算机视觉领域已经研究了几十年。 最近，我们见证了基于深度学习的 MVS 方法的快速发展。...</div></div></div></a><a class="pagination-related" href="/2025/02/26/MindNLP-bigbird_pegasus%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/" title="MindNLP模型微调超级笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-26</div><div class="info-item-2">MindNLP模型微调超级笔记</div></div><div class="info-2"><div class="info-item-1">MindNLP模型微调超级笔记 乱七八糟的学习参考 已完成的bloom模型微调 pull链接这里面只有训练的部分结果，然后讲的是实现了bloom-3b在databricks-dolly-15k数据集上面的lora微调，不知道这个pytorch的训练过程是从哪来的，然后发现lora微调是一种微调方法，然后这里能够找到他fork的仓库，在mindNLP仓库是看不到改动的，需要去他提交pullrequst的fork的仓库，然后在这里点击可以到他fork的仓库，fork的仓库，同时可以看到这里fork后的仓库重新创建的分支叫做intern，然后查看commit改冲在哪里，找到发现在这里mindnlp/llm/finetune/bloom/bloom-3b-qlora，也就是说任务是微调的话就把代码放在llm/finetune这个目录下，然后对比官方的mindspore的仓库发现确实这里pull...</div></div></div></a><a class="pagination-related" href="/2025/02/26/NLP%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="NLP任务微调笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-26</div><div class="info-item-2">NLP任务微调笔记</div></div><div class="info-2"><div class="info-item-1">NLP任务微调笔记 NLP数据集没有像CV一样大量标号的数据集，所以NLP一般是自监督的，有两种自监督的模型：   LM：语言模型，预测下一个词   MLM：带掩码的语言模型，完形填空   每种预训练模型基于不同的技术也分为两种：   Word embddings：  LSTM之类的传统的    Transformer based pretrained model：  Bert：基于encoder，针对Bert的微调，把最后一层重新设计随机初始化，做分类只需要拿出一个然后训练，做QA只需要输出答案的位置 GPT：基于decoder T5：基于encoder和decode    如何做分词 数据集一般是用Unicode进行编码，是对世界所有语言的一种编码，但是我们需要将这些文字变成数字编码的向量，就需要分词 1&quot;我是一个用于测试输出unicode编码的文字&quot;.encode(&#x27;utf-8&#x27;)#可以看到每个字对应的unicode编码...</div></div></div></a><a class="pagination-related" href="/2025/02/26/mindNLP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" title="mindNLP使用方法以及NLP模型的使用"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-26</div><div class="info-item-2">mindNLP使用方法以及NLP模型的使用</div></div><div class="info-2"><div class="info-item-1">mindNLP API reference 另一个API reference，这个全 介绍这是什么   MindNLP是基于MindSpore的开源NLP库。这个是个package，类似于huggingface的transformer库，有 250+ 预训练模型支持类似 huggingface transformers 的 API。您可以通过以下代码片段轻松使用，这样可以直接使用已经定义好的模型并且还有模型的权重：支持的模型 123import mindnlp.transformers import AutoModelmodel = AutoModel.from_pretrained（&#x27;bert-base-cased&#x27;）   全面的数据处理：将多个经典的 NLP 数据集打包成友好的模块，以便于使用，例如 Multi30k、SQuAD、CoNLL 等。   友好的 NLP 模型工具集：MindNLP 提供了各种可配置的组件。使用 MindNLP 自定义模型很友好。   易于使用的引擎：MindNLP 简化了 MindSpore 中复杂的训练过程。它支持...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/./img/head.png" onerror="this.onerror=null;this.src='/./img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">outbreak_sen</div><div class="author-info-description">an interesting man</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/outbreak-sen" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://blog.csdn.net/outbreakrmb" target="_blank" title="CSDN"><i class="fa fa-book-open"></i></a><a class="social-icon" href="/1023786231" target="_blank" title="QQ"><i class="fab fa-qq"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text">深入理解 BigBird 的块稀疏注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E8%AF%A5%E5%85%B3%E6%B3%A8%E5%93%AA%E4%BA%9B%E8%AF%8D%E5%85%83%EF%BC%9F"><span class="toc-text">应该关注哪些词元？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E8%A7%A3%E5%85%A8%E5%B1%80-%E6%BB%91%E5%8A%A8-%E9%9A%8F%E6%9C%BA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">图解全局、滑动、随机注意力的概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bigbird-%E5%9D%97%E7%A8%80%E7%96%8F%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">BigBird 块稀疏注意力</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E5%B1%80%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">全局注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BB%91%E5%8A%A8%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">滑动注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">随机注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-text">实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%B6%E9%97%B4%E5%92%8C%E5%86%85%E5%AD%98%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">时间和内存复杂度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#itc-%E4%B8%8E-etc"><span class="toc-text">ITC 与 ETC</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8-%F0%9F%A4%97transformers-%E4%B8%AD%E4%BD%BF%E7%94%A8-bigbird"><span class="toc-text">在  🤗Transformers 中使用 BigBird</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/12/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84/" title="证券投资">证券投资</a><time datetime="2025-03-12T03:54:34.000Z" title="发表于 2025-03-12 11:54:34">2025-03-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/26/MindNLP-bigbird_pegasus%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/" title="MindNLP模型微调超级笔记">MindNLP模型微调超级笔记</a><time datetime="2025-02-26T07:01:44.000Z" title="发表于 2025-02-26 15:01:44">2025-02-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/26/NLP%E4%BB%BB%E5%8A%A1%E5%BE%AE%E8%B0%83%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="NLP任务微调笔记">NLP任务微调笔记</a><time datetime="2025-02-26T07:01:44.000Z" title="发表于 2025-02-26 15:01:44">2025-02-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/26/mindNLP%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" title="mindNLP使用方法以及NLP模型的使用">mindNLP使用方法以及NLP模型的使用</a><time datetime="2025-02-26T02:53:47.000Z" title="发表于 2025-02-26 10:53:47">2025-02-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/02/17/%E5%A4%A7%E6%81%92%E7%9B%B8%E6%9C%BA%E7%9A%84%E5%A4%9A%E7%9B%B8%E6%9C%BA%E7%A1%AC%E8%A7%A6%E5%8F%91%E6%96%B9%E6%A1%88/" title="大恒相机的多相机硬触发方案">大恒相机的多相机硬触发方案</a><time datetime="2025-02-17T06:49:03.000Z" title="发表于 2025-02-17 14:49:03">2025-02-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By outbreak_sen</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>