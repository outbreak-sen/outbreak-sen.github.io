<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>This is a 部落格 of outbreak_sen</title>
  
  <subtitle>[object Object]</subtitle>
  <link href="http://outbreak-sen.github.io/atom.xml" rel="self"/>
  
  <link href="http://outbreak-sen.github.io/"/>
  <updated>2024-07-28T06:14:40.000Z</updated>
  <id>http://outbreak-sen.github.io/</id>
  
  <author>
    <name>outbreak_sen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/VSCode%20Python%20Debug%20%E6%95%99%E7%A8%8B/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/VSCode%20Python%20Debug%20%E6%95%99%E7%A8%8B/</id>
    <published>2025-12-03T05:57:28.299Z</published>
    <updated>2024-07-28T06:14:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1>VSCode Python Debug 教程</h1><p>我有以下几个目的：</p><ol><li>debug时候传入参数，就像运行python test -pram a -pram1 b这样子，这个如何实现</li><li>如何实现conda环境下运行</li><li>看每个变量的大小和值</li><li>步进运行每一行</li></ol><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="string">&quot;version&quot;</span>: <span class="string">&quot;0.2.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;configurations&quot;</span>: [</span><br><span class="line">        <span class="comment">// 基于内置的 Node.js 调试器</span></span><br><span class="line">        <span class="comment">// 可以创建多个调试器，然后起不同名字，选择对应的进行操作</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 这个是调试器配置的名称</span></span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;Python经典调试配置名称&quot;</span>,</span><br><span class="line">            <span class="comment">// 指定了使用Python调试器</span></span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;debugpy&quot;</span>,</span><br><span class="line">            <span class="comment">// 表示启动一个新的调试会话</span></span><br><span class="line">            <span class="string">&quot;request&quot;</span>: <span class="string">&quot;launch&quot;</span>,</span><br><span class="line">            <span class="comment">// 指定要调试的Python脚本， workspaceFolder/train.py&quot;:指定要调试的Python脚本</span></span><br><span class="line">            <span class="comment">// &#123;workspaceFolder&#125;是当前VS Code工作区目录的占位符，假设train.py位于工作区的根目录下。</span></span><br><span class="line">            <span class="string">&quot;program&quot;</span>: <span class="string">&quot;$&#123;file&#125;&quot;</span>,</span><br><span class="line">            <span class="comment">// 调试输出将显示在VS Code的集成终端中。</span></span><br><span class="line">            <span class="string">&quot;console&quot;</span>: <span class="string">&quot;integratedTerminal&quot;</span>,</span><br><span class="line">            <span class="comment">// 这样做可以让Python的输出在终端中实时显示，而不是被缓冲。</span></span><br><span class="line">            <span class="string">&quot;env&quot;</span>: &#123;<span class="string">&quot;PYTHONUNBUFFERED&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;,</span><br><span class="line">            <span class="comment">// &quot;args&quot;: &quot;$&#123;command:pickArgs&#125;&quot; ,</span></span><br><span class="line">            <span class="string">&quot;args&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;-batch&quot;</span>, <span class="string">&quot;64&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-dataset&quot;</span>, <span class="string">&quot;cifar_fs&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-gpu&quot;</span>, <span class="string">&quot;1&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-extra_dir&quot;</span>, <span class="string">&quot;your_run&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-temperature_attn&quot;</span>, <span class="string">&quot;5.0&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-lamb&quot;</span>, <span class="string">&quot;0.5&quot;</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// 这个是调试器配置的名称</span></span><br><span class="line">            <span class="string">&quot;name&quot;</span>: <span class="string">&quot;MotionBert调试&quot;</span>,</span><br><span class="line">            <span class="comment">// 指定了使用Python调试器</span></span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;debugpy&quot;</span>,</span><br><span class="line">            <span class="comment">// 表示启动一个新的调试会话</span></span><br><span class="line">            <span class="string">&quot;request&quot;</span>: <span class="string">&quot;launch&quot;</span>,</span><br><span class="line">            <span class="comment">// 指定要调试的Python脚本， workspaceFolder/train.py&quot;:指定要调试的Python脚本</span></span><br><span class="line">            <span class="comment">// &#123;workspaceFolder&#125;是当前VS Code工作区目录的占位符，假设train.py位于工作区的根目录下。</span></span><br><span class="line">            <span class="string">&quot;program&quot;</span>: <span class="string">&quot;$&#123;file&#125;&quot;</span>,</span><br><span class="line">            <span class="comment">// 调试输出将显示在VS Code的集成终端中。</span></span><br><span class="line">            <span class="string">&quot;console&quot;</span>: <span class="string">&quot;integratedTerminal&quot;</span>,</span><br><span class="line">            <span class="comment">// 这样做可以让Python的输出在终端中实时显示，而不是被缓冲。</span></span><br><span class="line">            <span class="string">&quot;env&quot;</span>: &#123;<span class="string">&quot;PYTHONUNBUFFERED&quot;</span>: <span class="string">&quot;1&quot;</span>&#125;,</span><br><span class="line">            <span class="comment">// &quot;args&quot;: &quot;$&#123;command:pickArgs&#125;&quot; ,</span></span><br><span class="line">            <span class="string">&quot;args&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;-batch&quot;</span>, <span class="string">&quot;64&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-dataset&quot;</span>, <span class="string">&quot;cifar_fs&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-gpu&quot;</span>, <span class="string">&quot;1&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-extra_dir&quot;</span>, <span class="string">&quot;your_run&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-temperature_attn&quot;</span>, <span class="string">&quot;5.0&quot;</span>,</span><br><span class="line">                <span class="string">&quot;-lamb&quot;</span>, <span class="string">&quot;0.5&quot;</span></span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;VSCode Python Debug 教程&lt;/h1&gt;
&lt;p&gt;我有以下几个目的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;debug时候传入参数，就像运行python test -pram a -pram1 b这样子，这个如何实现&lt;/li&gt;
&lt;li&gt;如何实现conda环境下运行&lt;/li</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/SFM/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/SFM/</id>
    <published>2025-12-03T05:57:28.288Z</published>
    <updated>2023-12-06T06:03:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1>SFM</h1><h2 id="sfm是什么？和mvs的对比？">SFM是什么？和MVS的对比？</h2><p>运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题</p><ul><li><p>SFM相当于获得由二维点获得三维点输出稀疏点云还有每张图片对应的相机参数</p></li><li><p>MVS相当于给三维点贴图输出稠密点云</p></li></ul><p>3D点阵可以通过MeshLab来重建稀疏的Mesh。也可以通过PMVS（Patch-based Multi-view Stereo）来重建Dense的Mesh</p><h2 id="sfm的阶段">SFM的阶段</h2><p>SfM 涉及三个主要阶段：</p><ol><li>提取图像中的特征（例如，兴趣点、线条等）并在图像之间匹配这些特征<ol><li>SIFT，SURF来提取并匹配，也可以用最新的AKAZE（SIFT的改进版，2010）来匹配。</li><li>误匹配会造成较大的Error，所以要对匹配进行筛选，目前流行的方法是RANSAC（Random Sample Consensus）。2D的误匹配点可以应用3D的Geometric特征来进行排除。</li></ol></li><li>相机运动估计–外参（使用提取的特征）</li><li>使用估计的外参特征恢复 3D 结构（通过最小化所谓的重投影误差）</li></ol><p>该技术旨在优化称为<strong>total reprojection error的成本函数</strong></p><h2 id="sfm分类">SfM分类</h2><ul><li><p>增量式（incremental/sequential）：</p><ul><li>优–对错误的匹配点有较强鲁棒性，总体精度更高</li><li>劣运行时间长，<strong>drift: error随着camera registration(相机校准)逐步积累</strong></li></ul></li><li><p>全局式（global）：</p><ul><li>优–避免了drift的问题（更反映了图像的全局性), 速度快（只需解决两个global synchronization(global SfM算法中的全局同步操作)+一次BA<br>(光束平差法)）</li><li>劣–对错误的匹配点鲁棒性较差，且错误难以修正 (error会沿着pipeline累积）</li></ul></li><li><p>混合式（hybrid）：</p><ul><li>全局估计摄像机旋转矩阵，增量估计摄像机中心</li></ul></li><li><p>层次式（hierarchical）：</p><ul><li>执行顺序上采用了层次式的聚类策略（clustering）。其先生成一棵聚类二叉树（binary cluster tree），然后算法自底向上进行处理:<strong>算法的每次迭代合并具有最小距离的两个clusters，每个cluster可以是一张图片，也可以是一个合并之后的cluster</strong>。</li></ul></li><li><p>基于语义的(Semantic)</p></li></ul><h2 id="2006-年-sfm-的-sequential-pipeline增量式">2006 年 SfM 的 sequential  pipeline增量式</h2><ol><li>测每个图像中的关键点。使用SIFT 描述符来比较跨图像的这些关键点</li><li>应用随机采样和一致性（RANSAC），以稳健地估计<strong>图像对之间的基本矩阵</strong>（用于相机相对运动）并丢弃异常匹配</li><li>从找到<strong>内部匹配数量最多的一对图像开始</strong>，然后一次贪婪地添加一个图像，反复求解束调整</li></ol><h2 id="cvpr-2016structure-from-motion-revisited提出colmap">CVPR 2016Structure-from-Motion Revisited提出colmap</h2><ul><li><a href="https://ieeexplore.ieee.org/document/7780814">https://ieeexplore.ieee.org/document/7780814</a></li><li>(论文“Structure-from-Motion Revisited” 对ISFM改进的理解)[<a href="https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235%5Ev38%5Epc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235^v38^pc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4</a>]</li><li>在实验部分对比的两个incremetal SfM框架：<a href="http://phototour.cs.washington.edu/Photo_Tourism.pdf">Bundler</a>、<a href="https://ieeexplore.ieee.org/document/6599068">VisualSFM</a>；两个global SfM 框架：<a href="https://ieeexplore.ieee.org/document/5995626">DISCO</a>、<a href="http://theia-sfm.org/">Theia</a>提出了一个当时“近乎理想”的incremental SfM并开源了代码[COLMAP]</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;SFM&lt;/h1&gt;
&lt;h2 id=&quot;sfm是什么？和mvs的对比？&quot;&gt;SFM是什么？和MVS的对比？&lt;/h2&gt;
&lt;p&gt;运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SFM相当于获得由二维点获得三</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/Multi%20View%20Stereo/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/Multi%20View%20Stereo/</id>
    <published>2025-12-03T05:57:28.276Z</published>
    <updated>2023-11-30T04:32:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Multi View Stereo</h1><p>[TOC]</p><h2 id="传统方法">传统方法</h2><p>SSD、NCC、SAD+三角测量重建</p><h2 id="开源工具">开源工具</h2><p>四种经典的三维重建技术Pipeline。分别为：<strong>1）传统方法（COLMAP）2）深度学习方法（COLMAP + MVSNet）3）传统方法（COLMAP + OpenMVS）4）深度学习方法（COLMAP + R-MVSNet）</strong></p><h2 id="论文部分">论文部分</h2><p><a href="https://zhuanlan.zhihu.com/p/148569782">有监督深度学习三维重建</a></p><p><a href="https://zhuanlan.zhihu.com/p/439210991">无监督深度学习三维重建</a></p><p>综述：Deep Learning for Multi-view Stereo via Plane Sweep: A Survey</p><p>MVSNet(2018年ECCV),RMVSNet(CVPR2019),PointMVSNet(ICCV2019),</p><p>P-MVSNet(ICCV2019),MVSCRF(ICCV2019),Cascade(CVPR2020),CVP-MVSNet(CVPR2020),</p><p>Fast-MVSNet(CVPR2020),UCSNet(CVPR2020),CIDER(AAAI2020),PVAMVSNet(ECCV2020),</p><p>*D*2HC-RMVSNet(ECCV2020),Vis-MVSNet(BMVC 2020),AA-RMVSNet(ICCV2021),</p><p>EPP-MVSNet(ICCV2021)</p><h2 id="mvsnet">MVSNet</h2><p>香港科技大学权龙yoyo和深圳Altizure公司的研究团队</p><p>被ECCV2018选为Oral</p><ol><li><p>通过<strong>可微单应矩阵（Differentiable Homography</strong>）将相机几何嵌入到网络，以帮助实现<strong>端到端</strong>的深度神经网络。</p></li><li><p>设计了<strong>基于方差的多视匹配代价计算准则</strong>，将任意数量的像素特征向量映射为一个匹配代价向量，以帮助网络适用于不定数量的图像输入</p></li><li><p>输入一张r<strong>eference image（为主）</strong> 和几张<strong>source images（辅助）</strong>；</p></li><li><p>分别用网络提取出下采样四分之一的32通道的特征图；</p></li><li><p>采用立体匹配（即双目深度估计）里提出的cost volume的概念，将几张source images的特征利用<strong>单应性变换( homography warping)<strong>转换到reference image，在转换的过程中，<strong>类似极线搜索</strong>，引入了深度信息。构建</strong>cost volume</strong>可以说是<strong>MVSNet的关键</strong></p></li><li><p>可微单应性变换（<a href="https://zhuanlan.zhihu.com/p/363830541%EF%BC%89">https://zhuanlan.zhihu.com/p/363830541）</a></p></li><li><p><strong>具体costvolume上一个点是所有图片在这个点和深度值上特征的方差，方差越小，说明在该深度上置信度越高。</strong></p></li><li><p>利用3D卷积操作cost volume，先输出每个深度的概率，然后求深度的加权平均得到预测的深度信息，用L1或smoothL1回归深度信息，是一个回归模型。</p></li><li><p>利用多张图片之间的重建约束（<em>photometric</em> and <em>geometric</em> consistencies）来选择预测正确的深度信息，重建成三维点云。</p></li></ol><p>过于耗费内存而难以应用到大尺度场景</p><p>Xiaoyang Guo 同学把原来MVSNet的tensorflow代码改成了pytorch，效果提升了很多</p><h2 id="r-mvsnet-recurrent循环-mvsnet">R-MVSNet（Recurrent循环 MVSNet）</h2><p>香港科技大学权龙团队yoyo和深圳Altizure公司的研究团队</p><p>被CVPR2019接收</p><ol><li><p>引入循环神经网络架构，可依序地在深度方向通过GRU单元正则化2D的代价图，较大程度地缓解了内存消耗</p></li><li><p>输入的多视影像首先经由2D的特征提取层提取特征，经由可微的单应矩阵变换到参考影像的相机视锥体的正面平行面上，然后在不同深度计算代价，并经由卷积的GRU单元进行正则化，使在深度方向获取几何和单向的语义信息成为可能。该网络将问题视为分类问题，以交叉熵作为损失函数。</p></li><li><p>3D卷积换成GRU的时序网络来降低模型大小，然后loss也改成了多分类的交叉熵损失，其他都一样，还是在四分之一的图上预测深度。模型变小了，但是其实精度也小有降低。</p></li><li><p>和MVSNet代码合到一起了</p></li></ol><h2 id="pointmvsnet">PointMVSNet</h2><p>ICCV2019</p><ol><li>测出深度depth信息然后和图片构成三维点云，再用3D点云的算法去优化depth的回归。</li></ol><h2 id="p-mvsnet">P-MVSNet</h2><p>ICCV2019</p><p>采用传统三维重建算法中Patch-wise</p><h2 id="mvscrf-learning-multi-view-stereo-with-conditional-random-fields">MVSCRF(Learning Multi-view Stereo with Conditional Random Fields )</h2><p>ICCV2019</p><p>接入了一个CRF模块</p><h2 id="cascade-mvsnet">cascade MVSNet</h2><p>CVPR2020</p><p>阿里，GitHub链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/alibaba/cascade-stereo">https://github.com/alibaba/cascade-stereo</a></p><p>把MVSNet的模型改成了层级的，先预测下采样四分之一的深度，然后用来缩小下采样二分之一的深度，再用其缩小原始图片大小的深度，这样层级的方式，可以采用大的深度间隔和少的深度区间，从而可以一次训练更多数据。先预测出深度信息然后用来缩小更大的图片的深度</p><p>cascade MVSNet也把cost volume用在了双目立体匹配中</p><h2 id="cvp-mvsnet">CVP-MVSNet</h2><p>CVPR2020</p><p>澳大利亚国立和英伟达，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/JiayuYANG/CVP-MVSNet">https://github.com/JiayuYANG/CVP-MVSNet</a></p><p>先预测出深度信息然后用来缩小更大的图片的深度，CVP-MVSNet相比cascade MVSNet也缩小了cost volume的范围。</p><h2 id="fast-mvsnet">Fast-MVSNet</h2><p>上海科技大学，github链接：<a href="https://link.zhihu.com/?target=https%3A//github.com/svip-lab/FastMVSNet">https://github.com/svip-lab/FastMVSNet</a></p><p>采用稀疏的cost volume以及Gauss-Newton layer，目的是提高MVSNet<strong>的速度</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Multi View Stereo&lt;/h1&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;传统方法&quot;&gt;传统方法&lt;/h2&gt;
&lt;p&gt;SSD、NCC、SAD+三角测量重建&lt;/p&gt;
&lt;h2 id=&quot;开源工具&quot;&gt;开源工具&lt;/h2&gt;
&lt;p&gt;四种经典的三维重建技术Pipeline。分别为</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/dive%20into%20deeplearning/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/dive%20into%20deeplearning/</id>
    <published>2025-12-03T05:57:28.264Z</published>
    <updated>2024-03-02T09:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1>dive into deeplearning</h1><h2 id="引言">引言</h2><h2 id="监督学习-supervised-learning"><em>监督学习</em>（supervised learning）</h2><p>擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个<em>样本</em>（example）。</p><p>回归问题：（regression）平方误差损失函数。</p><p>分类问题：（classification）<em>分类</em>问题希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class）），当有两个以上的类别时，我们把这个问题称为<em>多项分类</em>（multiclass classification）问题。 常见的例子包括手写字符识别 。 与解决回归问题不同，分类问题的常见损失函数被称为<em>交叉熵</em>（cross-entropy）</p><p>标记问题：学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification），一个样本点有多个标签，标记问题输出样本点上的所有标签，分类只有一个标签</p><p>搜索问题：搜索一个标签，输出多个结果，需要对输出多个结果进行相关度排序。</p><p>序列学习问题：输入是连续的，模型可能就需要拥有“记忆”功能</p><h3 id="无监督学习">无监督学习</h3><p>聚类（clustering）问题：没有标签的情况下，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片</p><p>主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。</p><p>因果关系（causality）和<em>概率图模型</em>（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</p><p>生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。</p><h3 id="强化学习">强化学习</h3><p><em>离线学习</em>：（offline learning）预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的。但是有些策略与环境互动，而不是离线的，比如强化学习</p><p>机器学习开发与环境交互并采取行动感兴趣，那么最终可能会专注于<em>强化学习</em>（reinforcement learning）</p><p>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得<em>奖励</em>（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。强化学习的目标是产生一个好的<em>策略</em>（policy）。</p><h2 id="历史">历史</h2><p>神经网络的研究从1995年左右开始停滞不前，首先，训练网络（在计算上）非常昂贵。其次，数据集相对较小，<em>核方法</em>（kernel method）、<em>决策树</em>（decision tree）和<em>图模型</em>（graph models）等强大的统计工具（在经验上）证明是更为优越的。神经网络的研究直到2005年才稍有起色</p><p>大约2010年开始，那些在计算上看起来不可行的神经网络算法变得热门起来，实际上是以下两点导致的： 其一，随着互联网的公司的出现，大规模数据集变得触手可及； 另外，廉价又高质量的传感器、廉价的数据存储（克莱德定律）以及廉价计算（摩尔定律）的普及，特别是GPU的普及，使大规模算力唾手可得。</p><p>这也造就了许多深度学习的中流砥柱，如多层感知机 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id106">McCulloch and Pitts, 1943</a>) 、卷积神经网络 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id90">LeCun <em>et al.</em>, 1998</a>) 、长短期记忆网络 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id51">Graves and Schmidhuber, 2005</a>) 和Q学习 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id179">Watkins and Dayan, 1992</a>)。</p><p>下面列举了帮助研究人员在过去十年中取得巨大进步的想法：</p><ul><li>新的容量控制方法，如<em>dropout</em> (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id155">Srivastava <em>et al.</em>, 2014</a>)，有助于减轻过拟合的危险。这是通过在整个神经网络中应用噪声注入 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id9">Bishop, 1995</a>) 来实现的，出于训练目的，用随机变量来代替权重。</li><li>注意力机制解决了困扰统计学一个多世纪的问题：如何在不增加可学习参数的情况下增加系统的记忆和复杂性。这大大提高了长序列的准确性，因为模型在开始生成新序列之前不再需要记住整个序列。</li><li>多阶段设计。例如，存储器网络 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id158">Sukhbaatar <em>et al.</em>, 2015</a>) 和神经编程器-解释器 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id136">Reed and De Freitas, 2015</a>)。它们允许统计建模者描述用于推理的迭代方法。这些工具允许重复修改深度神经网络的内部状态，从而执行推理链中的后续步骤，类似于处理器如何修改用于计算的存储器。</li><li>另一个关键的发展是生成对抗网络 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id48">Goodfellow <em>et al.</em>, 2014</a>) 的发明。</li><li>并行和分布式训练算法的能力有了显著提高。设计可伸缩算法的关键挑战之一是深度学习优化的主力——随机梯度下降，它依赖于相对较小的小批量数据来处理。同时，小批量限制了GPU的效率。因此，在1024个GPU上进行训练，例如每批32个图像的小批量大小相当于总计约32000个图像的小批量。最近的工作，首先是由 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id91">Li, 2017</a>) 完成的，随后是 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id192">You <em>et al.</em>, 2017</a>) 和 (<a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id79">Jia <em>et al.</em>, 2018</a>) ，将观察大小提高到64000个，将ResNet-50模型在ImageNet数据集上的训练时间减少到不到7分钟。作为比较——最初的训练时间是按天为单位的。</li><li>并行计算的能力也对强化学习的进步做出了相当关键的贡献。这导致了计算机在围棋、雅达里游戏、星际争霸和物理模拟（例如，使用MuJoCo）中实现超人性能的重大进步。</li><li>深度学习框架在传播思想方面发挥了至关重要的作用。允许轻松建模的第一代框架包括<a href="https://github.com/BVLC/caffe">Caffe</a>、<a href="https://github.com/torch">Torch</a>和<a href="https://github.com/Theano/Theano">Theano</a>。许多开创性的论文都是用这些工具写的。到目前为止，它们已经被<a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>（通常通过其高级API <a href="https://github.com/keras-team/keras">Keras</a>使用）、<a href="https://github.com/Microsoft/CNTK">CNTK</a>、<a href="https://github.com/caffe2/caffe2">Caffe 2</a>和<a href="https://github.com/apache/incubator-mxnet">Apache MXNet</a>所取代。第三代工具，即用于深度学习的命令式工具，可以说是由<a href="https://github.com/chainer/chainer">Chainer</a>率先推出的，它使用类似于Python NumPy的语法来描述模型。这个想法被<a href="https://github.com/pytorch/pytorch">PyTorch</a>、MXNet的<a href="https://github.com/apache/incubator-mxnet">Gluon API</a>和<a href="https://github.com/google/jax">Jax</a>都采纳了。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;dive into deeplearning&lt;/h1&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;h2 id=&quot;监督学习-supervised-learning&quot;&gt;&lt;em&gt;监督学习&lt;/em&gt;（supervised learning）&lt;/h2&gt;
&lt;p&gt;擅长在“给定输入特征”</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/Deep%20LearningOverview/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/Deep%20LearningOverview/</id>
    <published>2025-12-03T05:57:28.253Z</published>
    <updated>2023-11-24T15:20:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1>Deep Learning</h1><p>无监督和有监督</p><p><strong>无监督机器学习分类</strong></p><p>1.聚类是最常见的无监督学习方法之一。聚类的方法包括将未标记的数据组织成类似的组。此处的主要目标是发现数据点中的相似性，并将相似的数据点分组到一个聚类中。</p><p>2.异常检测是识别与大多数数据显著不同的特殊项、事件或观测值的方法。通常在数据中寻找异常或异常值的原因在于它们是可疑的。异常检测常用于银行欺诈和医疗差错检测。</p><h2 id="前馈神经网络-全连接神经网络-feedforward-neural-network-fnn-：">前馈神经网络/全连接神经网络(Feedforward Neural Network,FNN)：</h2><p>线性函数和激活函数叠好几个层最简单</p><h2 id="循环神经网络-recurrent-neural-network">循环神经网络(Recurrent Neural Network):</h2><p><a href="https://blog.csdn.net/bestrivern/article/details/90723524">RNN详解</a></p><p><a href="https://zhuanlan.zhihu.com/p/123211148">史上最详细循环神经网络讲解（RNN/LSTM/GRU）</a></p><p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p><p>层与层之间是全连接的，每层之间的节点也是连接的，每一步的参数共享</p><p>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息</p><h2 id="长短期记忆递归神经网络-long-short-term-memory-lstm">长短期记忆递归神经网络（Long-Short Term Memory，LSTM):</h2><p>LSTM是RNN的一种变体</p><p>LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；</p><p>引入了很多内容，导致参数变多，也使得训练难度加大了很多。</p><p>因此往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p><p>三个门：</p><ol><li>Input Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory Cell。</li><li>Output Gate：中文是输出门，每一时刻是否有信息从Memory Cell输出取决于这一道门。</li><li>Forget Gate：中文是遗忘门，每一时刻Memory Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory Cell里的值清除，也就是遗忘掉。</li></ol><h2 id="门控循环单元-gate-recurrent-unit-gru-2014提出循环神经网络的一种">门控循环单元（Gate Recurrent Unit，GRU）:2014提出循环神经网络的一种</h2><p>GRU模型中有两个门，重置门和更新门</p><p><strong>重置门决定了如何将新的输入信息与前面的记忆相结合</strong></p><p>更新门用于控制前一时刻的状态信息被带入到当前状态中的程度</p><h2 id="递归神经网络-recursive-neural-network-rnn">递归神经网络 (Recursive Neural Network, RNN)</h2><p>尽管<strong>递归神经网络</strong>具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，<strong>递归神经网络</strong>的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用<strong>循环神经网络</strong>处理句子，那么我们可以直接把句子作为输入。然而，如果我们用<strong>递归神经网络</strong>处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于<strong>递归神经网络</strong>能够带来的性能提升，这个投入是不太划算的。</p><h2 id="深度神经网络dnn：">深度神经网络DNN：</h2><p>有很多隐藏层的神经网络</p><h2 id="卷积神经网络cnn">卷积神经网络CNN</h2><h2 id="生成对抗网络gan">生成对抗网络GAN</h2><h2 id="自然语言处理nlp：">自然语言处理NLP：</h2><p>分类问题的常见损失函数：交叉熵</p><p>回归问题的常见损失函数：均方差</p><p>激活函数：<a href="https://zhuanlan.zhihu.com/p/364620596">https://zhuanlan.zhihu.com/p/364620596</a></p><h1></h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;p&gt;无监督和有监督&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;无监督机器学习分类&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.聚类是最常见的无监督学习方法之一。聚类的方法包括将未标记的数据组织成类似的组。此处的主要目标是发现数据点中的相似性，并将相似的</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/blender%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/blender%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2025-12-03T05:57:28.242Z</published>
    <updated>2024-02-29T12:50:10.000Z</updated>
    
    <content type="html"><![CDATA[<h1>BLENDER</h1><h2 id="基本操作">基本操作</h2><p>左上角有不同编辑器视图模式</p><p>3D模式右上角有着色方式‘</p><p>​cavity：边线</p><p>右下角是编辑器</p><p>​修改器（扳手）</p><p>​倒角：摁住shift拖动，更精确，小数点后三三位</p><p>SHIFT+中建 平移视角</p><p>变换工具：是移动缩放旋转的整合</p><p>游标：添加物体的位置，操作点</p><p>shift+a： 添加</p><p>x：删除</p><p>~：视图切换</p><p>shift+d：复制几何体并移动，假如再输入x则在x轴移动</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;BLENDER&lt;/h1&gt;
&lt;h2 id=&quot;基本操作&quot;&gt;基本操作&lt;/h2&gt;
&lt;p&gt;左上角有不同编辑器视图模式&lt;/p&gt;
&lt;p&gt;3D模式右上角有着色方式‘&lt;/p&gt;
&lt;p&gt;​	cavity：边线&lt;/p&gt;
&lt;p&gt;右下角是编辑器&lt;/p&gt;
&lt;p&gt;​	修改器（扳手）&lt;/p&gt;
&lt;p&gt;​		</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SFM/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_SFM/</id>
    <published>2025-12-03T05:57:28.231Z</published>
    <updated>2023-12-06T06:03:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1>SFM</h1><h2 id="sfm是什么？和mvs的对比？">SFM是什么？和MVS的对比？</h2><p>运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题</p><ul><li><p>SFM相当于获得由二维点获得三维点输出稀疏点云还有每张图片对应的相机参数</p></li><li><p>MVS相当于给三维点贴图输出稠密点云</p></li></ul><p>3D点阵可以通过MeshLab来重建稀疏的Mesh。也可以通过PMVS（Patch-based Multi-view Stereo）来重建Dense的Mesh</p><h2 id="sfm的阶段">SFM的阶段</h2><p>SfM 涉及三个主要阶段：</p><ol><li>提取图像中的特征（例如，兴趣点、线条等）并在图像之间匹配这些特征<ol><li>SIFT，SURF来提取并匹配，也可以用最新的AKAZE（SIFT的改进版，2010）来匹配。</li><li>误匹配会造成较大的Error，所以要对匹配进行筛选，目前流行的方法是RANSAC（Random Sample Consensus）。2D的误匹配点可以应用3D的Geometric特征来进行排除。</li></ol></li><li>相机运动估计–外参（使用提取的特征）</li><li>使用估计的外参特征恢复 3D 结构（通过最小化所谓的重投影误差）</li></ol><p>该技术旨在优化称为<strong>total reprojection error的成本函数</strong></p><h2 id="sfm分类">SfM分类</h2><ul><li><p>增量式（incremental/sequential）：</p><ul><li>优–对错误的匹配点有较强鲁棒性，总体精度更高</li><li>劣运行时间长，<strong>drift: error随着camera registration(相机校准)逐步积累</strong></li></ul></li><li><p>全局式（global）：</p><ul><li>优–避免了drift的问题（更反映了图像的全局性), 速度快（只需解决两个global synchronization(global SfM算法中的全局同步操作)+一次BA<br>(光束平差法)）</li><li>劣–对错误的匹配点鲁棒性较差，且错误难以修正 (error会沿着pipeline累积）</li></ul></li><li><p>混合式（hybrid）：</p><ul><li>全局估计摄像机旋转矩阵，增量估计摄像机中心</li></ul></li><li><p>层次式（hierarchical）：</p><ul><li>执行顺序上采用了层次式的聚类策略（clustering）。其先生成一棵聚类二叉树（binary cluster tree），然后算法自底向上进行处理:<strong>算法的每次迭代合并具有最小距离的两个clusters，每个cluster可以是一张图片，也可以是一个合并之后的cluster</strong>。</li></ul></li><li><p>基于语义的(Semantic)</p></li></ul><h2 id="2006-年-sfm-的-sequential-pipeline增量式">2006 年 SfM 的 sequential  pipeline增量式</h2><ol><li>测每个图像中的关键点。使用SIFT 描述符来比较跨图像的这些关键点</li><li>应用随机采样和一致性（RANSAC），以稳健地估计<strong>图像对之间的基本矩阵</strong>（用于相机相对运动）并丢弃异常匹配</li><li>从找到<strong>内部匹配数量最多的一对图像开始</strong>，然后一次贪婪地添加一个图像，反复求解束调整</li></ol><h2 id="cvpr-2016structure-from-motion-revisited提出colmap">CVPR 2016Structure-from-Motion Revisited提出colmap</h2><ul><li><a href="https://ieeexplore.ieee.org/document/7780814">https://ieeexplore.ieee.org/document/7780814</a></li><li>(论文“Structure-from-Motion Revisited” 对ISFM改进的理解)[<a href="https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235%5Ev38%5Epc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4">https://blog.csdn.net/Z5122/article/details/103287832?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-103287832-blog-124490400.235^v38^pc_relevant_sort&amp;spm=1001.2101.3001.4242.2&amp;utm_relevant_index=4</a>]</li><li>在实验部分对比的两个incremetal SfM框架：<a href="http://phototour.cs.washington.edu/Photo_Tourism.pdf">Bundler</a>、<a href="https://ieeexplore.ieee.org/document/6599068">VisualSFM</a>；两个global SfM 框架：<a href="https://ieeexplore.ieee.org/document/5995626">DISCO</a>、<a href="http://theia-sfm.org/">Theia</a>提出了一个当时“近乎理想”的incremental SfM并开源了代码[COLMAP]</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;SFM&lt;/h1&gt;
&lt;h2 id=&quot;sfm是什么？和mvs的对比？&quot;&gt;SFM是什么？和MVS的对比？&lt;/h2&gt;
&lt;p&gt;运动恢复结构（SfM）问题是从二维（2D）图像的集合中恢复静止场景的三维（3D）结构的问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SFM相当于获得由二维点获得三</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%96%87%E7%94%9F%E5%9B%BE%E4%B8%B2%E8%AE%B2/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%96%87%E7%94%9F%E5%9B%BE%E4%B8%B2%E8%AE%B2/</id>
    <published>2025-12-03T05:57:28.219Z</published>
    <updated>2025-08-28T15:03:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>目前我的理解有两个阵营</p><ol><li>Stable Diffusion=DiffusionUNet+VAE+CLIP</li><li>Dalle2=CLIP+像素级别DiffusionUNet</li><li>其他还有其他的文生图的网络，也用了diffusion，<ol><li>Imagen=T5+DiffusionTransformer不开源</li><li>Midjourney不开源</li></ol></li></ol><table><thead><tr><th>特性</th><th>DALL·E 2</th><th>Stable Diffusion</th></tr></thead><tbody><tr><td><strong>开发者</strong></td><td>OpenAI</td><td>Stability AI, CompVis, LMU</td></tr><tr><td><strong>开源状态</strong></td><td>❌ <strong>闭源</strong>，通过 API 或 Web 界面使用</td><td>✅ <strong>完全开源</strong>，可本地部署</td></tr><tr><td><strong>架构</strong></td><td>基于 CLIP + 级联扩散模型，也是一个diffusion的</td><td>基于潜在空间的扩散模型 (Latent Diffusion)</td></tr><tr><td><strong>访问方式</strong></td><td>订阅制（按生成次数付费）</td><td>免费开源，可自行运行</td></tr><tr><td>社区与定制</td><td>有限，无法微调模型</td><td>拥有庞大的社区，可训练 LoRA、Dreambooth 模型等</td></tr><tr><td>成风格</td><td>通常更“安全”、更偏向摄影/插画</td><td>风格极其多样，取决于微调模型</td></tr><tr><td><strong>生去噪空间</strong></td><td>扩散过程主要在<strong>像素空间</strong>或<strong>CLIP 的嵌入空间</strong>进行，而不是像 SD 那样在独立的 VAE 潜在空间。</td><td>核心创新——“<strong>潜在扩散</strong>”（Latent Diffusion），极大提升了效率。</td></tr></tbody></table><h2 id="从diffusion到stablediffusion">从Diffusion到StableDiffusion</h2><p>扩散模型（Diffusion Model）是一种<strong>生成式深度学习模型</strong><br>它的工作原理分为两个主要阶段：</p><ol><li>前向扩散过程 (Forward Diffusion Process)<ul><li>从一张真实的图像开始。</li><li>在训练过程中，模型会<strong>逐步、有规律地向图像中添加高斯噪声</strong>。</li><li>经过很多步（比如 1000 步）后，原始图像被完全破坏，变成了一幅纯噪声图像。</li><li>这个过程是固定的、可预测的。</li></ul></li><li>反向去噪过程 (Reverse Denoising Process)<ul><li>这是生成新图像的关键。</li><li>模型的目标是学习如何<strong>从纯噪声开始，一步步地去除噪声，最终还原出一张清晰的图像</strong>。</li><li>在训练时，模型学习预测每一步被添加的噪声是什么。</li><li>在生成时（推理阶段），模型从一个随机噪声开始，利用学到的知识，一步一步地“去噪”，最终生成一张新的、逼真的图像。<br>Stable Diffusion 是由 <strong>Stability AI</strong> 等机构在 2022 年发布的一个<strong>具体的、开源的文本生成图像模型</strong>。核心创新：引入了 <strong>VAE（变分自编码器）在潜在空间（Latent Space）中进行扩散</strong><br>它的工作原理分为三个主要阶段：</li></ul></li><li><strong>编码器</strong>：先将原始图像压缩到一个低维的<strong>潜在空间</strong>（例如 64x64x4）。</li><li><strong>扩散过程</strong>：扩散模型（U-Net）在这个<strong>低维的潜在空间</strong>中进行去噪和生成。这大大降低了计算复杂度，提升了速度。</li><li><strong>解码器</strong>：生成完成后，再用 VAE 的<strong>解码器</strong>将低维的潜在表示“解压”回高维的像素图像。</li></ol><p>Stable Diffusion (通常指 <strong>v1.x</strong> 系列) 和 <strong>Stable Diffusion v2</strong> 是由 Stability AI 发布的同一模型系列的两个主要版本，Stable Diffusion v2的数据集更好，另外提供 <strong>upscaling diffusion model</strong>，可将低分辨率图像提高分辨率，还能利用 MiDaS 模型估计深度。</p><ol><li>Stable Diffusion v1.x 系列：使用 OpenAI 的 CLIP。Stable Diffusion v2.0：切换为 OpenCLIP</li></ol><h2 id="vae-variational-autoencoder-变分自编码器-在stablediffusion中的作用">VAE (Variational Autoencoder - 变分自编码器)在StableDiffusion中的作用</h2><p>VAE 在文生图流程中主要负责<strong>图像的编码与解码</strong>，其作用是连接像素空间和潜在空间（latent space）。</p><ul><li><strong>编码器 (Encoder) 的作用：</strong><ul><li>在训练阶段，VAE 的编码器将真实图像从高维的<strong>像素空间</strong>压缩到一个低维的、结构化的<strong>潜在空间</strong>（latent space）。</li><li>这个潜在空间的向量（通常称为 latent code 或 latent representation）包含了原始图像的关键视觉信息，但维度远低于原始像素，从而大大降低了后续生成模型（如扩散模型）的计算复杂度。</li><li>例如，在 Stable Diffusion 中，一张 512x512x3 的图像被编码成一个 64x64x4 的潜在向量。</li></ul></li><li><strong>解码器 (Decoder) 的作用：</strong><ul><li>在生成阶段，生成模型（如扩散模型）在潜在空间中逐步“去噪”或构建出一个代表目标图像的潜在向量。</li><li>VAE 的解码器则负责将这个最终的潜在向量<strong>解码回像素空间</strong>，生成我们最终看到的、可视化的图像。</li></ul></li></ul><h2 id="clip-contrastive-language-image-pre-training-对比语言-图像预训练模型-在stablediffusion中的作用">CLIP (Contrastive Language–Image Pre-training - 对比语言-图像预训练模型)在StableDiffusion中的作用</h2><p>CLIP 的核心作用是<strong>理解文本语义并将其与视觉概念对齐</strong>，充当文本和图像之间的“翻译器”或“桥梁”。</p><ul><li><strong>文本编码器 (Text Encoder)：</strong><ul><li>将输入的文本描述（如“一只戴着墨镜的猫在太空漫步”）转换成一个高维的<strong>文本嵌入向量</strong>（text embedding）。这个向量捕捉了文本的语义信息。</li></ul></li><li><strong>图像编码器 (Image Encoder)：</strong><ul><li>将图像（或图像的潜在表示）也转换成一个<strong>图像嵌入向量</strong>。</li></ul></li><li><strong>对齐机制：</strong><ul><li>CLIP 是在海量的“图像-文本对”数据上进行对比学习训练的。它的目标是让匹配的图像和文本的嵌入向量在向量空间中尽可能接近，而不匹配的则尽可能远离。</li><li>这种训练方式使得 CLIP 能够深刻理解不同概念之间的关联，例如知道“猫”这个词对应的视觉特征是什么。</li></ul></li><li><strong>在文生图中的应用：</strong><ul><li><strong>文本条件输入：</strong> 在生成过程中，CLIP 的文本编码器将用户的文本提示（prompt）编码成一个文本嵌入向量。这个向量作为<strong>条件信息</strong>，指导生成模型（如扩散模型中的 U-Net）去生成符合该描述的图像。</li><li><strong>引导生成：</strong> 生成模型利用这个文本嵌入向量来调整其在潜在空间中的生成过程，确保生成的潜在表示与文本描述在语义上保持一致。</li><li><strong>评估与排序（可选）：</strong> 有时 CLIP 也可以用来评估生成图像与文本描述的匹配程度。</li></ul></li></ul><h2 id="dall-e-2">DALL·E 2</h2><ul><li><strong>输入</strong>：一段文字描述，例如：“一只戴着贝雷帽、在月球上用萨克斯演奏爵士乐的柴犬，赛博朋克风格，8K 分辨率”。</li><li><strong>输出</strong>：生成一张或多张与该描述高度匹配的、高质量的图像。<br>工作原理简述有两个阶段</li></ul><ol><li>文本理解：首先，DALL·E 2 利用了 OpenAI 自家的 <strong>CLIP 模型</strong>。将输入的文本描述编码成一个高维的语义向量。</li><li>图像生成：一个级联的扩散模型开始工作：<ul><li><strong>第一阶段（低分辨率生成）</strong>：一个扩散模型从纯噪声开始，根据文本语义向量的指导，逐步去噪，生成一张低分辨率（如 256x256）的图像。</li><li><strong>第二阶段（超分辨率）</strong>：另一组扩散模型（超分辨率模型）将低分辨率图像作为输入，逐步添加细节，提升到更高的分辨率（如 1024x1024），使图像更清晰、细节更丰富。</li><li>还具备强大的编辑功能：局部编辑（Inpainting），图像到图像（Image-to-Image），外绘（Outpainting）</li></ul></li></ol><h2 id="为什么用unet做diffusion？">为什么用UNet做diffusion？</h2><p>既要理解全局语义，又要保留精确的空间位置信息。Unet的跳跃连接刚好能做到<br><strong>核心去噪网络都是基于 U-Net 或其变体</strong>（如 DiT）</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;目前我的理解有两个阵营&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Stable Diffusion=DiffusionUNet+VAE+CLIP&lt;/li&gt;
&lt;li&gt;Dalle2=CLIP+像素级别DiffusionUNet&lt;/li&gt;
&lt;li&gt;其他还有其他的文生图的网络，也用了diffusi</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/</id>
    <published>2025-12-03T05:57:28.218Z</published>
    <updated>2025-08-29T15:51:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>给输入序列注入“位置信息”</strong>，让模型知道“每个元素在什么位置”。<br>位置编码的两大类型</p><table><thead><tr><th>类型</th><th>特点</th><th>代表</th></tr></thead><tbody><tr><td>**固定位置编码 **（Fixed）</td><td>位置编码是预定义的，不可学习</td><td>原始 Transformer 的正弦编码</td></tr><tr><td>**可学习位置编码 **（Learned）</td><td>位置编码是可训练的参数，<strong>就是字典啦，tokenizer把文本变成数字编码之后做的</strong></td><td>BERT、ViT 的 <code>position embedding</code></td></tr></tbody></table><h2 id="正弦位置编码-sinusoidal-positional-encoding">正弦位置编码（Sinusoidal Positional Encoding）</h2><p>这是 <strong>原始 Transformer 论文</strong>（“Attention is All You Need”, 2017）中提出的方法。<br>核心思想：使用<strong>正弦和余弦函数</strong>生成位置编码。编码是<strong>确定性的、固定的</strong>，不参与训练。可以表示<strong>任意长度的位置</strong>，外推性好</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SinusoidalPositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)  <span class="comment"># (1, max_len, d_model)</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (B, seq_len, d_model)</span></span><br><span class="line">        x = x + <span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)]</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p><strong>编码公式</strong></p><p><img src="%E9%99%84%E4%BB%B6/Pasted%20image%2020250829234857.png" alt=""></p><h2 id="可学习位置编码-learned-positional-embedding">可学习位置编码（Learned Positional Embedding）</h2><p>这是 <strong>BERT、ViT、GPT 等模型</strong> 采用的方法。位置编码是<strong>可学习的参数矩阵</strong>。每个位置对应一个向量（类似词嵌入）。在大多数任务上优于正弦编码，超出训练长度时性能急剧下降</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LearnedPositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.pos_embedding = nn.Embedding(max_len, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: (B, seq_len, d_model)</span></span><br><span class="line">        positions = torch.arange(x.size(<span class="number">1</span>), device=x.device).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pos_embedding(positions)</span><br></pre></td></tr></table></figure><h3 id="旋转位置编码-rotary-position-embedding-rope">旋转位置编码 **（Rotary Position Embedding, RoPE）</h3><p>将位置信息编码为<strong>旋转矩阵</strong>，通过旋转向量来体现位置差异。代表：<strong>LLaMA、ChatGLM、PaLM</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;strong&gt;给输入序列注入“位置信息”&lt;/strong&gt;，让模型知道“每个元素在什么位置”。&lt;br&gt;
位置编码的两大类型&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;类型&lt;/th&gt;
&lt;th&gt;特点&lt;/th&gt;
&lt;th&gt;代表&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%87%A0%E4%B8%AA%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%87%A0%E4%B8%AA%E5%B8%B8%E8%A7%81%E7%9A%84%E6%A6%82%E5%BF%B5/</id>
    <published>2025-12-03T05:57:28.217Z</published>
    <updated>2025-08-28T14:32:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1>无监督和有监督学习</h1><h2 id="监督学习-supervised-learning">监督学习（supervised learning）</h2><p>擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个<em>样本</em>（example）。</p><p>回归问题：（regression）平方误差损失函数。</p><p>分类问题：（classification）<em>分类</em>问题希望模型能够预测样本属于哪个<em>类别</em>（category，正式称为<em>类</em>（class）），当有两个以上的类别时，我们把这个问题称为<em>多项分类</em>（multiclass classification）问题。 常见的例子包括手写字符识别 。 与解决回归问题不同，分类问题的常见损失函数被称为<em>交叉熵</em>（cross-entropy）</p><p>标记问题：学习预测不相互排斥的类别的问题称为<em>多标签分类</em>（multi-label classification），一个样本点有多个标签，标记问题输出样本点上的所有标签，分类只有一个标签</p><p>搜索问题：搜索一个标签，输出多个结果，需要对输出多个结果进行相关度排序。</p><p>序列学习问题：输入是连续的，模型可能就需要拥有“记忆”功能</p><h3 id="无监督学习">无监督学习</h3><p>聚类（clustering）问题：没有标签的情况下，给定一组照片，我们能把它们分成风景照片、狗、婴儿、猫和山峰的照片</p><p>主成分分析（principal component analysis）问题：我们能否找到少量的参数来准确地捕捉数据的线性相关属性？比如，一个球的运动轨迹可以用球的速度、直径和质量来描述。</p><p>因果关系（causality）和<em>概率图模型</em>（probabilistic graphical models）问题：我们能否描述观察到的许多数据的根本原因？例如，如果我们有关于房价、污染、犯罪、地理位置、教育和工资的人口统计数据，我们能否简单地根据经验数据发现它们之间的关系？</p><p>生成对抗性网络（generative adversarial networks）：为我们提供一种合成数据的方法，甚至像图像和音频这样复杂的非结构化数据。潜在的统计机制是检查真实和虚假数据是否相同的测试，它是无监督学习的另一个重要而令人兴奋的领域。<br>1.聚类是最常见的无监督学习方法之一。聚类的方法包括将未标记的数据组织成类似的组。此处的主要目标是发现数据点中的相似性，并将相似的数据点分组到一个聚类中。</p><p>2.异常检测是识别与大多数数据显著不同的特殊项、事件或观测值的方法。通常在数据中寻找异常或异常值的原因在于它们是可疑的。异常检测常用于银行欺诈和医疗差错检测。</p><h3 id="强化学习">强化学习</h3><p><em>离线学习</em>：（offline learning）预先获取大量数据，然后启动模型，不再与环境交互。 这里所有学习都是在算法与环境断开后进行的。但是有些策略与环境互动，而不是离线的，比如强化学习</p><p>机器学习开发与环境交互并采取行动感兴趣，那么最终可能会专注于<em>强化学习</em>（reinforcement learning）</p><p>在强化学习问题中，智能体（agent）在一系列的时间步骤上与环境交互。 在每个特定时间点，智能体从环境接收一些<em>观察</em>（observation），并且必须选择一个<em>动作</em>（action），然后通过某种机制（有时称为执行器）将其传输回环境，最后智能体从环境中获得<em>奖励</em>（reward）。 此后新一轮循环开始，智能体接收后续观察，并选择后续操作，依此类推。强化学习的目标是产生一个好的<em>策略</em>（policy）。</p><h2 id="前馈神经网络-全连接神经网络-feedforward-neural-network-fnn-：">前馈神经网络/全连接神经网络(Feedforward Neural Network,FNN)：</h2><p>线性函数和激活函数叠好几个层最简单</p><h2 id="循环神经网络-recurrent-neural-network">循环神经网络(Recurrent Neural Network):</h2><p><a href="https://blog.csdn.net/bestrivern/article/details/90723524">RNN详解</a></p><p><a href="https://zhuanlan.zhihu.com/p/123211148">史上最详细循环神经网络讲解（RNN/LSTM/GRU）</a></p><p><a href="https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p><p>层与层之间是全连接的，每层之间的节点也是连接的，每一步的参数共享</p><p>RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息</p><h2 id="长短期记忆递归神经网络-long-short-term-memory-lstm">长短期记忆递归神经网络（Long-Short Term Memory，LSTM):</h2><p>LSTM是RNN的一种变体</p><p>LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；</p><p>引入了很多内容，导致参数变多，也使得训练难度加大了很多。</p><p>因此往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p><p>三个门：</p><ol><li>Input Gate：中文是输入门，在每一时刻从输入层输入的信息会首先经过输入门，输入门的开关会决定这一时刻是否会有信息输入到Memory Cell。</li><li>Output Gate：中文是输出门，每一时刻是否有信息从Memory Cell输出取决于这一道门。</li><li>Forget Gate：中文是遗忘门，每一时刻Memory Cell里的值都会经历一个是否被遗忘的过程，就是由该门控制的，如果打卡，那么将会把Memory Cell里的值清除，也就是遗忘掉。</li></ol><h2 id="门控循环单元-gate-recurrent-unit-gru-2014提出循环神经网络的一种">门控循环单元（Gate Recurrent Unit，GRU）:2014提出循环神经网络的一种</h2><p>GRU模型中有两个门，重置门和更新门</p><p><strong>重置门决定了如何将新的输入信息与前面的记忆相结合</strong></p><p>更新门用于控制前一时刻的状态信息被带入到当前状态中的程度</p><h2 id="递归神经网络-recursive-neural-network-rnn">递归神经网络 (Recursive Neural Network, RNN)</h2><p>尽管<strong>递归神经网络</strong>具有更为强大的表示能力，但是在实际应用中并不太流行。其中一个主要原因是，<strong>递归神经网络</strong>的输入是树/图结构，而这种结构需要花费很多人工去标注。想象一下，如果我们用<strong>循环神经网络</strong>处理句子，那么我们可以直接把句子作为输入。然而，如果我们用<strong>递归神经网络</strong>处理句子，我们就必须把每个句子标注为语法解析树的形式，这无疑要花费非常大的精力。很多时候，相对于<strong>递归神经网络</strong>能够带来的性能提升，这个投入是不太划算的。</p><h2 id="深度神经网络dnn：">深度神经网络DNN：</h2><p>有很多隐藏层的神经网络</p><h2 id="卷积神经网络cnn">卷积神经网络CNN</h2><h2 id="生成对抗网络gan">生成对抗网络GAN</h2><h2 id="自然语言处理nlp">自然语言处理NLP</h2><p>分类问题的常见损失函数：交叉熵</p><p>回归问题的常见损失函数：均方差</p><p>激活函数：<a href="https://zhuanlan.zhihu.com/p/364620596">https://zhuanlan.zhihu.com/p/364620596</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;无监督和有监督学习&lt;/h1&gt;
&lt;h2 id=&quot;监督学习-supervised-learning&quot;&gt;监督学习（supervised learning）&lt;/h2&gt;
&lt;p&gt;擅长在“给定输入特征”的情况下预测标签。 每个“特征-标签”对都称为一个&lt;em&gt;样本&lt;/em&gt;（examp</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_YOLOX%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_YOLOX%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/</id>
    <published>2025-12-03T05:57:28.206Z</published>
    <updated>2025-08-25T12:26:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1>yolox微调训练日记</h1><p>20250516</p><p>今天在测试视频的时候发现在某些角度，Alphapose不能识别运动员的位姿，思来想去应该是yolo的问题，alphapose是一个先识别人体再使用hrnet或者resnet进行识别的一个网络，所以如果连人的框都没有就不用提能不能识别位姿了，yolov3训练是势在必行了，但是alphapose里面的<a href="https://github.com/ayooshkathuria/pytorch-yolo-v3/tree/master">yolov3</a>是一个印度人写的，并不是纯正血统的<a href="https://github.com/ultralytics/yolov3">yolo3</a>，并且没有训练教程，并且还是使用torch编写的darknet进行部署，darknet的部署是用.weight文件，而当前官方使用的是.pt文件，把我恶心坏了。</p><p>正当我想今晚大干要训练一下这个yolox的时候，天无绝人之路，我突然想到alphapose是支持<a href="https://github.com/Megvii-BaseDetection/YOLOX?tab=readme-ov-file">yolox</a>的，并且是纯正血统的yolox，测试发现本身的yolo识别运动员的概率还更高，所以目前的结论是：<strong>我得微调训练一个yolox的权重</strong>。</p><p>另外今晚发现一个事情，我带着我的没有预训练的权重到北京测试那里的视频，发现竟然没有识别，我以为是放在.cache文件里面的权重也会进行训练而我并没有复制过来导致的，今天通过计算哈希的方法发现经过训练之后的在cache中的权重并没有变化，然后又测试了在北京测试的没有识别成功的视频，发现在家是识别不了这些视频的，所以结论是：<strong>我的不是通过微调训练的，而是重新完全训练的权重,在训练数据集上过拟合了，而且过拟合的程度非常高，即使同样是运动员，换了新的视频就无法识别了,所以我之后也只能微调训练</strong></p><h1>建立COCO数据集</h1><p>这里不讲。我这里直接用自己建立好的，里面只有person一种类别</p><h2 id="微调yolox-l模型">微调YOLOX-l模型</h2><p>因为发现yolox-l性能最适合，所以微调这个模型，微调过程中只需要注意exp这个文件夹，exp/default中是默认的yolox参数，这里是一个官方的参考参数，以及部署时候应该使用的参数。</p><p>注意的是，每个yolox_*.py中的self.depth和self.width是给每个模型定义的参数，比如如果你想训练或者使用yolo-s，那你的参数为self.depth = 0.33 self.width = 0.50，如果你想训练或者使用yolo-l，那你的参数为self.depth = 1.0 self.width = 1.0。</p><p>保持这里的default文件夹不要动，我们重新在exps/example/custom里建立我们训练和部署用的参数。比如复制yolox_l.py到exps/example/custom中并重命名为yolox_l_pose.py。编写文件如下，修改其中的训练数据位置和训练批次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">MyExp</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Exp, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.depth = <span class="number">1.0</span></span><br><span class="line">        <span class="variable language_">self</span>.width = <span class="number">1.0</span></span><br><span class="line">        <span class="variable language_">self</span>.exp_name = os.path.split(os.path.realpath(__file__))[<span class="number">1</span>].split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Define yourself dataset path</span></span><br><span class="line">        <span class="variable language_">self</span>.data_dir = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.train_ann = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco/annotations/person_keypoints_train2017.json&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.val_ann = <span class="string">&quot;/home/houbosen/AlphaPose/data/coco/annotations/person_keypoints_val2017.json&quot;</span></span><br><span class="line"></span><br><span class="line">         <span class="variable language_">self</span>.num_classes = <span class="number">80</span></span><br><span class="line">        <span class="comment"># self.num_classes = 1</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.max_epoch = <span class="number">100</span></span><br><span class="line">        <span class="variable language_">self</span>.data_num_workers = <span class="number">4</span></span><br><span class="line">        <span class="variable language_">self</span>.eval_interval = <span class="number">20</span></span><br></pre></td></tr></table></figure><h4 id="修改训练类别数">修改训练类别数</h4><p>注意如果你想修改训练类别，那设置yolox_l_pose.py文件中的num_classes为需要的类别，注意默认是80而不是71</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">self</span>.num_classes = <span class="number">1</span></span><br></pre></td></tr></table></figure><p>然后在<code>YOLOX/yolox/data/datasets/coco_classes.py</code>修改对应的具体类别名称，我这里只有一个person类别。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">COCO_CLASSES = (</span><br><span class="line">    <span class="string">&quot;person&quot;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>然后进行训练，注意加载预训练模型的时候会警告说预训练权重是输出80个类的，但是这里只需要1个类，没关系</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应该是类似的输出，就是说后面的head层size不一致</span></span><br><span class="line">size mismatch <span class="keyword">for</span> head.cls_preds.0.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.1.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.1.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.2.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80, 128, 1, 1]).</span><br><span class="line">        size mismatch <span class="keyword">for</span> head.cls_preds.2.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape <span class="keyword">in</span> current model is torch.Size([80]).</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0,1,2,3 python -m yolox.tools.train \</span><br><span class="line">-f exps/example/custom/yolox_l_pose1.py \</span><br><span class="line">-d 4 \ <span class="comment"># 用4张卡</span></span><br><span class="line">-b 32 \ <span class="comment"># 一般是卡数*8</span></span><br><span class="line">--fp16 \ <span class="comment">#混合精度</span></span><br><span class="line">-c yolox_l.pth <span class="comment">#下载官方的预训练权重，不用预训练的权重过拟合很严重</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=3 python -m yolox.tools.train \</span><br><span class="line">-f exps/example/custom/yolox_s_pose1.py \</span><br><span class="line">-d 1 \          </span><br><span class="line">-b 8 \               </span><br><span class="line">--fp16 \          </span><br><span class="line">-c ./YOLOX_outputs/yolox_s_coco2_num1epoth100BestHuang/best_ckpt.pth  </span><br></pre></td></tr></table></figure><p>注意一点，如果你想要推理的时候，就不要加载默认的参数文件了，否则会报错说网络框架应该输出80个类别但是训练之后的权重只有一个类别输出，推理的时候加在的参数文件还得是yolox_l_pose.py</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">python tools/demo.py image </span><br><span class="line">-f exps/example/custom/yolox_l_pose.py </span><br><span class="line">-c /path/to/your/yolox_s.pth </span><br><span class="line">--path assets/dog.jpg </span><br><span class="line">--conf 0.25 </span><br><span class="line">--nms 0.45 </span><br><span class="line">--tsize 640 </span><br><span class="line">--save_result </span><br><span class="line">--device [cpu/gpu]</span><br></pre></td></tr></table></figure><p>为什么要注意这一点呢?因为在alphapose里默认加载的是default/yolox_l.py，然后权重和模型架构不一致就会导致报错，所以如果在其他项目中比如人体关键点检测项目中发现了用到了yolox，那你训练了一个只输出一个类别的权重，那你需要修改default文件夹中的参数。</p><h4 id="通过tensorboard查看当前训练状态">通过tensorboard查看当前训练状态</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir YOLOX_outputs/yolox_s_pose1/tensorboard --port=0</span><br><span class="line"><span class="comment"># 输出如下，打开网页即可</span></span><br><span class="line">TensorFlow installation not found - running with reduced feature <span class="built_in">set</span>.</span><br><span class="line"></span><br><span class="line">NOTE: Using experimental fast data loading logic. To <span class="built_in">disable</span>, pass</span><br><span class="line">    <span class="string">&quot;--load_fast=false&quot;</span> and report issues on GitHub. More details:</span><br><span class="line">    https://github.com/tensorflow/tensorboard/issues/4784</span><br><span class="line"></span><br><span class="line">Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all</span><br><span class="line">TensorBoard 2.11.2 at http://localhost:35245/ (Press CTRL+C to quit)</span><br></pre></td></tr></table></figure><p><img src="yolo%E8%AE%AD%E7%BB%83%E6%97%A5%E8%AE%B0/image-20250517102809079.png" alt="image-20250517102809079"></p><p>后面发现训练的内存还会被占用，用htops杀掉</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">htop</span><br><span class="line">空格选择程序</span><br><span class="line">F9杀掉程序</span><br><span class="line">enter确认</span><br></pre></td></tr></table></figure><h2 id="做完一定要测试一下效果">做完一定要测试一下效果</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 输出在YOLOX_outputs中的yolox_s里面，是按照时间确定的视频，发现训练的依托</span><br><span class="line">python tools/demo.py video -f exps/example/custom/yolox_s.py -c YOLOX_outputs/yolox_s_pose1/yolos_ckpt_num80epoth12.pth --conf 0.5 --nms 0.4 --tsize 640 --save_result --device gpu --path 09040032_Miqus_5_28519.avi</span><br></pre></td></tr></table></figure><h1>主播的疑问，每次训练到某个epoth就不动了，比如训练yolox-s到epoth12就代码不动了</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;yolox微调训练日记&lt;/h1&gt;
&lt;p&gt;20250516&lt;/p&gt;
&lt;p&gt;今天在测试视频的时候发现在某些角度，Alphapose不能识别运动员的位姿，思来想去应该是yolo的问题，alphapose是一个先识别人体再使用hrnet或者resnet进行识别的一个网络，所以如果</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_%E6%88%91%E7%9A%84%E8%AE%BA%E6%96%87%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E6%97%A5%E8%AE%B0_%E6%88%91%E7%9A%84%E8%AE%BA%E6%96%87%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95/</id>
    <published>2025-12-03T05:57:28.044Z</published>
    <updated>2025-08-28T13:53:08.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>zotero下载pdf，我一般看arxiv的文章，然后保存pdf，webav到坚果云</li><li>有些需要用思维导图做笔记的，利用mindmaster，那个再说了</li><li>针对每一个文献，建立一个连接到obsidian的md文件，这里用一个插件betternote插件，可以自动建立文献同名的md文件并创建在obsidian的文件夹目录下<a href="https://www.zotero.org/support/plugins">plugins [Zotero Documentation]</a>，</li><li>obsidian使用一个attachment management来管理图片，像typora一样把截图保存在md同名的文件夹下</li><li>把文献的pdf建立一个github的仓库，是hexo的模板的仓库，然后定期进行推送到远端，并部署blog网页到个人主页，记得obsidian的图片的格式不是标准的markdown的，所以需要关闭wiki格式，自己查一下什么意思</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;zotero下载pdf，我一般看arxiv的文章，然后保存pdf，webav到坚果云&lt;/li&gt;
&lt;li&gt;有些需要用思维导图做笔记的，利用mindmaster，那个再说了&lt;/li&gt;
&lt;li&gt;针对每一个文献，建立一个连接到obsidian的md文件，这里用一个插件b</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0CUDA%E5%88%87%E6%8D%A2%E7%89%88%E6%9C%AC/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E5%AE%9E%E8%B7%B5%E7%AC%94%E8%AE%B0CUDA%E5%88%87%E6%8D%A2%E7%89%88%E6%9C%AC/</id>
    <published>2025-12-03T05:57:28.043Z</published>
    <updated>2025-09-05T03:01:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>如果本地没有cuda，torch安装的时候的cuda-toolkit就已经是cuda环境了，就不需要cuda了<br>但是如果需要cuda编译一些东西，比如AlphaPose需要cuda编译，就需要cuda安装到本地。<br>安装之后，<br><strong>系统变量</strong>中多了 ==CUDA_PATH_V9_2== 和 ==NVCUDASAMPLES9_2_ROOT==两个环境变量，然后CUDA_PATH也会变成新的。</p><ul><li><strong>NVCUDASAMPLES_ROOT</strong>：D:\CUDA Documentation\NVIDIA Corporation\CUDA Samples\v9.2（Samples 的路径）</li><li><strong>CUDA_PATH _V9_2</strong>：D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2（CUDA Documentation 和 CUDA Development 的路径）</li><li>CUDA_PATH:<strong>CUDA_PATH _V9_2</strong>的路径<br><strong>Path</strong>中多了两个bin和libvvp两个变量</li><li>D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin</li><li>D:\CUDA Documentation\NVIDIA GPU Computing Toolkit\CUDA\v9.2\libnvvp</li></ul><h3 id="切换版本">切换版本</h3><p>在系统变量的 <strong>Path</strong> 中，上移所需要切换的版本<br>修改 <strong>CUDA_PATH</strong> 的值<br> 修改 <strong>NVCUDASAMPLES_ROOT</strong> 的值</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;如果本地没有cuda，torch安装的时候的cuda-toolkit就已经是cuda环境了，就不需要cuda了&lt;br&gt;
但是如果需要cuda编译一些东西，比如AlphaPose需要cuda编译，就需要cuda安装到本地。&lt;br&gt;
安装之后，&lt;br&gt;
&lt;strong&gt;系统变量</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</id>
    <published>2025-12-03T05:57:28.042Z</published>
    <updated>2024-02-28T10:48:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1>深度学习代码学习笔记</h1><h2 id="setuptools">setuptools</h2><p><a href="https://blog.csdn.net/gongdiwudu/article/details/118726517">介绍是什么</a></p><p><a href="https://zhuanlan.zhihu.com/p/162842824">简单使用</a></p><p><a href="https://blog.csdn.net/zylooooooooong/article/details/115564782">_ <em>all</em> _是什么</a></p><p>当我们向文件导入某个模块时，导入的是该模块中那些<strong>名称不以下划线（单下划线“_”或者双下划线“__”）开头</strong>的变量、函数和类。因此，如果我们不想模块文件中的某个成员被引入到其它文件中使用，可以在其名称前添加下划线。</p><p>通过在模块文件中设置__all__变量，当其它文件以“<code>from 模块名 import *</code>”的形式导入该模块时，该文件中只能使用__all__ 列表中指定的成员。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># mytest.py</span><br><span class="line">__all__ = [&#x27;a&#x27;,&#x27;b&#x27;,&#x27;d&#x27;]  #这里不同</span><br><span class="line"></span><br><span class="line">def a():</span><br><span class="line">    print(&#x27;aaaaa&#x27;)</span><br><span class="line">def b():</span><br><span class="line">    print(&#x27;bbbbb&#x27;) </span><br><span class="line">def c():                  # 这里不同</span><br><span class="line">    print(&#x27;ccccc&#x27;)</span><br><span class="line">def _d():</span><br><span class="line">    print(&#x27;ccccc&#x27;)    </span><br><span class="line"># mytest2.py</span><br><span class="line">from mytest import * #只在以from 模块名 import *形式导入模块时起作用</span><br><span class="line">a()</span><br><span class="line">b()</span><br><span class="line">c()不成功</span><br><span class="line">d()不成功</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="self">self</h2><ul><li><p>self指的是实例Instance本身</p></li><li><p>在Python类中规定，函数的第一个参数是实例对象本身，并且约定俗成，把其名字写为self</p></li></ul><ul><li>self指的是实例本身，而不是类</li><li>self可以用this替代，但是不要这么去写</li><li>类的方法中的self不可以省略</li><li>类中的方法的第一个参数一定要是self，而且不能省略。</li></ul><h2 id="init-方法">__ init__ ()方法</h2><p>在python中创建类后，通常会创建一个\ __ init__ ()方法，这个方法会在创建类的实例的时候自动执行。 \ __ init__ ()方法必须包含一个self参数，而且要是第一个参数。</p><h2 id="super-net-self-init">super(Net, self).<strong>init</strong>()</h2><p>子类把父类的__init__()放到自己的__init__()当中，这样子类就有了父类的__init__()的那些东西。Net类继承nn.<a href="https://so.csdn.net/so/search?q=Module&amp;spm=1001.2101.3001.7020">Module</a>，super(Net, self).<strong>init</strong>()就是对继承自父类nn.Module的属性进行初始化。而且是用nn.Module的初始化方法来初始化继承的属性</p><h2 id="from-gru-import-gru中的-是什么？">from .gru import GRU中的.是什么？</h2><p><a href="http://xn--gru-y28d05bt0k1s1asa426f6og6o6cjbb.py">指当前文件目录下的gru.py</a></p><h2 id="字典类型">字典类型</h2><p><a href="https://zhuanlan.zhihu.com/p/183788519">https://zhuanlan.zhihu.com/p/183788519</a></p><h2 id="os模块">OS模块</h2><p><code>os</code>模块提供  Python 程序 与 操作系统进行交互的接口。</p><p>太多了，边用边学吧</p><h2 id="torch">torch</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1&gt;深度学习代码学习笔记&lt;/h1&gt;
&lt;h2 id=&quot;setuptools&quot;&gt;setuptools&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/gongdiwudu/article/details/118726517&quot;&gt;介绍是什么&lt;/a&gt;&lt;/p</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Zero-1-to-3/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Zero-1-to-3/</id>
    <published>2025-12-03T05:57:28.041Z</published>
    <updated>2025-08-30T03:49:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Zero-Shot 3D Content Generation from a Single Image</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">加州大学圣地亚哥分校、英伟达、多伦多大学和麻省理工学院</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2023</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>从指定相机视点合成图像进一步生成三维模型</td></tr><tr><td><strong>所属领域</strong></td><td>新视图合成和 3D 形状重建</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>Zero1to3的核心是一个<strong>条件扩散模型</strong>（Conditional Diffusion Model），但它不是直接生成3D网格或点云，而是生成<strong>多视角的二维图像</strong>。</li><li>利用了强大的<strong>几何先验</strong>：<ul><li><strong>Depth Estimation (深度估计):</strong> 模型首先使用一个预训练的单目深度估计模型（如MiDaS或DPT）从输入的单张2D图像中预测出一个粗略的深度图。这个深度图提供了关于物体表面距离的重要几何线索。</li><li><strong>Camera Pose (相机姿态):</strong> 模型假设了一组固定的、围绕物体的相机轨迹（例如，一个环绕物体的圆圈）。它知道从哪个视角生成下一张图像。</li><li><strong>Cross-View Attention (跨视图注意力):</strong> 这是Zero1to3的关键创新。在扩散模型的UNet架构中，引入了跨视图的注意力机制。当生成一个新视角的图像时，模型不仅关注当前视角的特征，还能“看到”并参考已经生成或正在生成的其他视角的特征。这强制了不同视角之间的几何和外观一致性，避免了物体形状或纹理的矛盾。</li></ul></li><li>工作流程<ol><li><strong>输入:</strong> 一张单视角的2D图像。</li><li><strong>深度预测:</strong> 使用预训练模型估计输入图像的深度图。</li><li><strong>多视角生成:</strong> 以输入图像及其深度图为条件，扩散模型沿着预设的相机轨迹，逐步去噪生成一系列新视角的RGB图像和对应的深度图。<ul><li>生成过程是迭代的，利用跨视图注意力确保一致性。</li></ul></li><li><strong>3D重建:</strong> 将生成的多视角RGB图像和深度图作为输入，使用成熟的3D重建技术（如Multi-View Stereo - MVS 或 NeRF）来生成最终的3D模型（如网格或NeRF场景）。</li></ol></li></ol><h3 id="两个挑战">两个挑战</h3><ol><li><p>虽然大规模生成模型在不同视点的大量对象上进行训练，表示并未显式编码视点之间的对应关系。<strong>训练数据没有相机角度编码</strong></p></li><li><p>生成模型继承了互联网上反映的观点偏见。如图 2 所示，Stable Diffusion 倾向于生成正统姿势的前向椅子的图像。这两个问题极大地阻碍了从大规模扩散模型中提取 3D 知识的能力。<strong>训练的数据太多重复视角比如椅子就应该正着拍摄，Dalle2和Stable Diffusion数据集里都是这样的</strong></p></li></ol><h2 id="方法细节">方法细节</h2><ol><li>Learning to Control Camera Viewpoint：教给模型一种机制来控制用于捕获照片的相机外部函数。我们微调了预先训练的扩散模型，以便在不破坏其余表示的情况下学习对相机参数的控制，我们使用带有编码器 E、降噪器 U-Net 和解码器 D 的潜在扩散架构。在扩散时间步长 t ∼ [1， 1000] 处，设 c（x， R， T ） 是输入视图和相对相机外函数的嵌入。最终模型可以为缺少 3D 资产且从未出现在微调集中的对象类合成新视图。<strong>就是把视角信息作为promt扔进Latent Diffusion Model重新训练让他能够学习视角信息，这里训练的是一个从不同视角生成图片的LDM模型</strong></li><li>View-Conditioned Diffusion：<ol><li>3D 重建需要低级感知（深度、阴影、纹理等）和高级理解（类型、功能、结构等）。因此，我们采用混合调节机制。</li><li>在一个stream上，输入图像的 CLIP嵌入与 （R， T ） 连接，以形成“姿势 CLIP”嵌入 c（x， R， T ）。我们应用交叉注意力来调节去噪 U-Net，它提供输入图像的高级语义信息。将输入图像和带姿势的 CLIP 嵌入随机设置为 null 向量，并在推理过程中缩放条件信息。</li><li>在另一个流上，输入图像与被去噪的图像进行通道连接，帮助模型保持被合成对象的标识和细节。</li></ol></li><li>3D Reconstruction：<br>4. 需要完整的 3D 重建来捕获对象的外观和几何形状。我们采用了一个最近的开源框架，即 Score Jacobian Chaining （SJC），来优化具有文本到图像扩散模型的先验的 3D 表示。然而，由于扩散模型的概率性质，梯度更新是高度随机的。受 DreamFusion 的启发，SJC 中使用的一种关键技术是将无分类器的指导值设置为明显高于平常。这种方法降低了每个样本的多样性，但提高了重建的保真度。<br>5. 与 SJC 类似，我们随机采样视点并执行体积渲染。然后，我们用高斯噪声 ∼ N （0， 1） 扰动生成的图像，并通过应用以输入图像 x 为条件的 U-Net、姿势 CLIP 嵌入 c（x， R， T ） 和时间步长 t 来对它们进行降噪。输出多个视角之利用nerf生成三维模型</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ViT/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_ViT/</id>
    <published>2025-12-03T05:57:28.040Z</published>
    <updated>2025-08-29T15:51:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">An Image is Worth 16x16 Words: Transformers for Image Classification at Scale</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"><strong>Kaiming He</strong>（何恺明）、Xiangyu Zhang、Shaoqing Ren、Jian Sun（微软亚洲研究院）</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left"><strong>Google Research</strong></td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2020</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>全称</strong></td><td style="text-align:left">Vision Transformer</td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>分类</td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><p>CNN 存在一些局限：</p><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td><strong>局部感受野</strong></td><td>卷积核只能看到局部区域，难以建模长距离依赖</td></tr><tr><td><strong>归纳偏置过强</strong></td><td>平移不变性、局部性等假设可能限制模型表达能力</td></tr><tr><td><strong>难以扩展</strong></td><td>模型变大时性能提升有限</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>首次成功地将 <strong>纯 Transformer 架构</strong> 直接应用于图像分类任务</li></ol><h3 id="网络架构">网络架构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">输入图像 (224x224x3) </span><br><span class="line">↓</span><br><span class="line">分割成 16x16 的小块 → 14x14 = 196 个 patch </span><br><span class="line">↓</span><br><span class="line">每个 patch 展平 → 196 个 768 维向量（&quot;视觉词元&quot;） </span><br><span class="line">↓</span><br><span class="line">加上位置编码（Position Embedding） </span><br><span class="line">↓</span><br><span class="line">添加一个可学习的分类 token[CLS]（类似 BERT，训练时候CLS是给的，预测时候CLS随机值）</span><br><span class="line">↓</span><br><span class="line">输入标准 Transformer Encoder </span><br><span class="line">↓</span><br><span class="line">取[CLS] token 输出用于分类</span><br><span class="line">↓</span><br><span class="line">通过一个 LayerNorm</span><br><span class="line">↓</span><br><span class="line">输入MLP输出类别概率（D → D/4 → num_classes）</span><br></pre></td></tr></table></figure><h3 id="vit-的局限">ViT 的局限</h3><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td>❌ <strong>小数据性能差</strong></td><td>在 ImageNet 上需大规模预训练才能超越 ResNet</td></tr><tr><td>❌ <strong>计算量大</strong></td><td>Self-Attention 复杂度为 O(N²)，对高分辨率图像不友好</td></tr><tr><td>❌ <strong>缺乏局部归纳偏置</strong></td><td>需要数据来学习“邻近 patch 更相关”这一常识</td></tr><tr><td>❌ <strong>位置编码敏感</strong></td><td>插值位置编码可能导致性能下降</td></tr></tbody></table><h3 id="后续发展：">后续发展：</h3><table><thead><tr><th>方法</th><th>解决的问题</th></tr></thead><tbody><tr><td>**DeiT **（Facebook）</td><td>小数据训练（引入蒸馏 token）</td></tr><tr><td>**Swin Transformer **（MSRA）最成功</td><td>高分辨率 + 局部注意力（滑动窗口）</td></tr><tr><td>**PVT **（清华）</td><td>多尺度 + 金字塔结构</td></tr><tr><td><strong>T2T-ViT</strong></td><td>更好的 tokenization（层层聚合）</td></tr><tr><td><strong>ConViT</strong></td><td>引入卷积先验（soft convolutional inductive bias）</td></tr><tr><td><strong>CvT</strong></td><td>卷积 + Transformer 混合</td></tr><tr><td><strong>MobileViT</strong></td><td>轻量化，适合移动端</td></tr></tbody></table>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VilT/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VilT/</id>
    <published>2025-12-03T05:57:27.996Z</published>
    <updated>2025-08-30T03:55:28.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Learning Visual Language Representation from Web-scale Weak-supervised Data</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">韩国科学技术院（KAIST）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2021</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td></td></tr><tr><td><strong>输出</strong></td><td></td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><h3 id="背景">背景</h3><p>在ViLT之前，主流的视觉-语言模型（如LXMERT, CLIP, ALIGN, UNITER等）通常采用以下架构：</p><ol><li><strong>独立的特征提取器：</strong><ul><li>使用一个预训练的<strong>视觉编码器</strong>（如ResNet或Faster R-CNN）将图像转换为一组区域特征（region features）或网格特征（grid features）。</li><li>使用一个预训练的<strong>文本编码器</strong>（如BERT）将文本转换为词向量。</li></ul></li><li><strong>模态融合模块：</strong> 将提取出的视觉特征和文本特征送入一个额外的Transformer网络进行跨模态交互和融合。</li></ol><h3 id="创新点">创新点</h3><ol><li><strong>摒弃了传统多模态模型中复杂的、针对特定任务设计的视觉和语言特征提取器</strong>，而是采用了一种<strong>极简主义（minimalist）</strong> 的设计，直接将原始图像块（image patches）和文本token输入到一个共享的Transformer主干网络中进行联合处理。ViLT使用一个单一的、共享的Transformer编码器来同时处理视觉和语言信息。</li><li>通过两种自监督任务在大规模图文对数据集（如Conceptual Captions, SBU Captions）上进行预训练：<ol><li>MLM：随机遮盖输入文本中的一些token（例如15%）。模型的任务是根据未被遮盖的文本和<strong>整个图像</strong>来预测被遮盖的token。</li><li>Image-Text Matching (ITM - 图文匹配)</li></ol></li></ol><h3 id="方法细节">方法细节</h3><ul><li><strong>直接输入原始数据：</strong><ul><li><strong>文本：</strong> 与BERT相同，将文本分词（Tokenization）后，加上特殊标记（[CLS], [SEP]）和位置编码。</li><li><strong>图像：</strong> 不再使用复杂的CNN或检测器。而是像ViT一样，将输入图像<strong>直接分割成固定大小的非重叠图像块（patches）</strong>（例如16x16像素），然后将每个图像块通过一个线性投影层（Linear Projection）转换为一个向量。这些向量与文本token向量在维度上对齐。</li></ul></li><li><strong>联合嵌入序列：</strong> 将图像块向量序列和文本token向量序列<strong>拼接（Concatenate）</strong> 起来，形成一个混合的输入序列。</li><li><strong>共享的位置编码：</strong> 为这个混合序列中的每个元素（无论是图像块还是文本token）分配一个位置编码（Positional Encoding），以保留它们的顺序信息。ViLT探索了不同类型的位置编码（2D空间位置用于图像，1D序列位置用于文本）。</li><li><strong>模态类型嵌入（Optional）：</strong> 可以添加一个可学习的模态嵌入（Modality Embedding），用来区分一个元素是来自图像还是文本。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-LONG/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-LONG/</id>
    <published>2025-12-03T05:57:16.693Z</published>
    <updated>2025-09-06T13:27:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">VGGT-Long: Chunk it, Loop it, Align it– Pushing VGGT’s Limits on Kilometer-scale Long RGB Sequences</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left">Kai Deng, Zexin Ti, Jiawei Xu, Jian Yang, Jin Xie</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">南开大学；南京大学</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2025</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>图片序列（SLAM定义可传入激光IMU等各种机器人传感</td></tr><tr><td>**输出</td><td>一个<strong>环境的几何/语义地图</strong>（Map）：点云，稀疏的点云ORBSLAM，稠密的点云LSDSLAM，高斯场景<br> 一条<strong>智能体的运动轨迹</strong>（Trajectory）图片位姿 片位姿 片位姿</td></tr><tr><td><strong>所属领域</strong></td><td>SLAM（Simultaneous Localization and Mapping，</td></tr></tbody></table><h2 id="背景">背景</h2><table><thead><tr><th>问题</th><th>说明</th></tr></thead><tbody><tr><td>面对室外长序列数据场景</td><td>CUT3R, Fast3R存在严重的漂移问题<br>MASt3R-SLAM, VGGT则无法完成整个长序列的处理<br>MASt3R/CUT3R/VGGT开启foundation model for 3D的新范式，但都面临memory与drift问题<br>MASt3R-SLAM，集成 MASt3R+图优化，但复杂且实时性差。 在中短序列精度高但会 track lost<br>VGGT-Long 在所有序列运行稳定，且重建精度高</td></tr></tbody></table><p>VGGT-Long能够成功完成公里级场景的重建，并保持了场景的准确性。</p><h3 id="创新点">创新点</h3><ol><li>VGGT-Long能够成功完成公里级场景的重建，并保持了场景的准确性。无需相机标定、无需深度监督、也无需重新训练基础模型，仅通过一套高效的后处理系统，就解决了现有模型的可扩展性瓶颈</li><li>核心思想可以概括为其标题中的三个动词：分块（Chunk it）、循环（Loop it）、对齐（Align it）。<ol><li><strong>分块处理 (Chunk it):</strong> 将长视频序列分割成多个有重叠的、固定长度的短视频块（chunks）。然后，它以滑动窗口的方式，将这些视频块依次送入预训练的VGGT模型进行处理，得到每个块的局部3D点图（pointmap）和相机轨迹。<img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-SLAM/IMG-20250906212654592.png" alt=""></li><li><strong>重叠对齐 (Align it):</strong> 将它们拼接成一个全局一致的场景。VGGT-Long利用相邻视频块之间的重叠部分进行对齐。值得一提的是提出了一种置信度感知对齐（Confidence-aware alignment）策略。VGGT模型会为每个预测的点生成一个置信度分数，该策略可以有效抑制场景中高速运动的物体（如车辆）对对齐过程的干扰。<strong>其实就是重叠部分进行对齐匹配但是删除动态物体的匹配吗</strong><ol><li><strong>如何对齐？</strong>：<img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-SLAM/IMG-20250906212657866.png" alt=""></li></ol></li><li><strong>回环优化 (Loop it):</strong>  全局尺度的漂移（例如，起点和终点无法闭合）。为了解决这个问题，VGGT-Long引入了轻量级的回环闭合优化（Loop Closure Optimization）。当车辆回到先前经过的位置时，系统会检测到回环，并建立约束。然后通过全局LM（Levenberg-Marquardt）优化，一次性校正整个轨迹的累积误差，确保全局地图的一致性。<ol><li><strong>基于什么优化？</strong>：<img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-SLAM/IMG-20250906212658260.png" alt=""></li><li><strong>如何检测回环？</strong><img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-SLAM/IMG-20250906212659119.png" alt=""><img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGGT-SLAM/IMG-20250906212659512.png" alt=""></li></ol></li></ol></li></ol><h3 id="网络架构">网络架构</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGG/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_VGG/</id>
    <published>2025-12-03T05:57:16.680Z</published>
    <updated>2025-09-07T03:51:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Very Deep Convolutional Networks for Large-Scale Image Recognition_</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left"></td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">牛津大学 Visual Geometry Group（VGG）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2014</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td>单张 RGB 图像</td></tr><tr><td><strong>输出</strong></td><td>分类、分割</td></tr><tr><td><strong>所属领域</strong></td><td>视觉 Transformer</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li>证明了<strong>网络深度</strong>对性能的重要性，提出了结构简洁、可复用的“小卷积核堆叠”设计范式。AlexNet（8 层）卷积核大（11×11, 5×5），难以扩展</li><li>所有卷积层都使用 <strong>3×3 卷积核</strong>，步长为 1，填充为 1</li><li>多个 3×3 卷积堆叠可以模拟大感受野，减少参数，增加非线性（更多 ReLU）：<ol><li>两个 3×3 卷积 ≈ 一个 5×5 卷积（感受野 5×5）</li><li>三个 3×3 卷积 ≈ 一个 7×7 卷积（感受野 7×7）</li></ol></li><li>VGG 探索了从 <strong>11 层到 19 层</strong>的多种网络结构。</li><li>使用 ReLU 和 Dropout，- 随机裁剪、水平翻转、色彩扰动。<br><img src="%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_MoGe%202/IMG-20250907115111288.png" alt=""></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Transformer_Encoder_Decoder/"/>
    <id>http://outbreak-sen.github.io/2025/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB_Transformer_Encoder_Decoder/</id>
    <published>2025-12-03T05:57:16.666Z</published>
    <updated>2025-08-29T14:36:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本信息">基本信息</h2><table><thead><tr><th style="text-align:left">项目</th><th style="text-align:left">内容</th></tr></thead><tbody><tr><td style="text-align:left"><strong>论文标题</strong></td><td style="text-align:left">Attention is all you need</td></tr><tr><td style="text-align:left"><strong>作者</strong></td><td style="text-align:left">很多共一</td></tr><tr><td style="text-align:left"><strong>作者单位</strong></td><td style="text-align:left">Google Brain 团队（Vaswani et al.）</td></tr><tr><td style="text-align:left"><strong>时间</strong></td><td style="text-align:left">2017</td></tr><tr><td style="text-align:left"><strong>发表会议/期刊</strong></td><td style="text-align:left"></td></tr></tbody></table><h2 id="方法概览">方法概览</h2><table><thead><tr><th>特点</th><th>文章性质</th></tr></thead><tbody><tr><td><strong>输入</strong></td><td></td></tr><tr><td><strong>输出</strong></td><td></td></tr><tr><td><strong>所属领域</strong></td><td></td></tr></tbody></table><table><thead><tr><th><strong>EncoderLayer</strong></th><th>“自己看自己”：用 Self-Attention 编码输入，输出上下文表示。</th></tr></thead><tbody><tr><td><strong>DecoderLayer</strong></td><td>“边看自己，边看别人”：先自注意力（带掩码），再关注 Encoder 的输出。</td></tr></tbody></table><h3 id="创新点">创新点</h3><ol><li><strong>完全抛弃 RNN 和 CNN</strong>，仅依靠 <strong>自注意力机制（Self-Attention）</strong> 实现序列建模。是 GPT、BERT、ChatGPT 等模型的基石。</li><li></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;基本信息&quot;&gt;基本信息&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;项目&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;内容&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
</summary>
      
    
    
    
    
  </entry>
  
</feed>
